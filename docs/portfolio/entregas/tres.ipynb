{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ac68fe60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup completo para análisis multi-fuentes!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "\n",
    "# Configurar visualizaciones\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"Setup completo para análisis multi-fuentes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c33039ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos oficiales de NYC Taxi (dataset completo)...\n",
      "   Viajes cargados: 3,066,766 filas, 19 columnas\n",
      "   Columnas: ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'airport_fee']\n",
      "   Período: 2008-12-31 23:01:42 a 2023-02-01 00:56:53\n",
      "   Tamaño en memoria: 588.5 MB\n",
      "\n",
      "Cargando datos oficiales de zonas NYC...\n",
      "   Zonas cargadas: 265 filas, 4 columnas\n",
      "   Columnas: ['LocationID', 'Borough', 'Zone', 'service_zone']\n",
      "   Boroughs únicos: ['EWR' 'Queens' 'Bronx' 'Manhattan' 'Staten Island' 'Brooklyn' 'Unknown'\n",
      " nan]\n",
      "\n",
      "Cargando datos de calendario de eventos...\n",
      "   Eventos calendario: 3 filas\n",
      "   Columnas: ['date', 'name', 'special']\n",
      "\n",
      "VISTA PREVIA DE DATOS:\n",
      "\n",
      "--- TRIPS ---\n",
      "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
      "0         2  2023-01-01 00:32:10   2023-01-01 00:40:36              1.0   \n",
      "1         2  2023-01-01 00:55:08   2023-01-01 01:01:27              1.0   \n",
      "2         2  2023-01-01 00:25:04   2023-01-01 00:37:49              1.0   \n",
      "3         1  2023-01-01 00:03:48   2023-01-01 00:13:25              0.0   \n",
      "4         2  2023-01-01 00:10:29   2023-01-01 00:21:19              1.0   \n",
      "\n",
      "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
      "0           0.97         1.0                  N           161           141   \n",
      "1           1.10         1.0                  N            43           237   \n",
      "2           2.51         1.0                  N            48           238   \n",
      "3           1.90         1.0                  N           138             7   \n",
      "4           1.43         1.0                  N           107            79   \n",
      "\n",
      "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
      "0             2          9.3   1.00      0.5        0.00           0.0   \n",
      "1             1          7.9   1.00      0.5        4.00           0.0   \n",
      "2             1         14.9   1.00      0.5       15.00           0.0   \n",
      "3             1         12.1   7.25      0.5        0.00           0.0   \n",
      "4             1         11.4   1.00      0.5        3.28           0.0   \n",
      "\n",
      "   improvement_surcharge  total_amount  congestion_surcharge  airport_fee  \n",
      "0                    1.0         14.30                   2.5         0.00  \n",
      "1                    1.0         16.90                   2.5         0.00  \n",
      "2                    1.0         34.90                   2.5         0.00  \n",
      "3                    1.0         20.85                   0.0         1.25  \n",
      "4                    1.0         19.68                   2.5         0.00  \n",
      "\n",
      "--- ZONES ---\n",
      "   LocationID        Borough                     Zone service_zone\n",
      "0           1            EWR           Newark Airport          EWR\n",
      "1           2         Queens              Jamaica Bay    Boro Zone\n",
      "2           3          Bronx  Allerton/Pelham Gardens    Boro Zone\n",
      "3           4      Manhattan            Alphabet City  Yellow Zone\n",
      "4           5  Staten Island            Arden Heights    Boro Zone\n",
      "\n",
      "--- CALENDAR ---\n",
      "         date       name  special\n",
      "0  2022-01-01   New Year     True\n",
      "1  2022-01-03  Event Day     True\n",
      "2  2022-01-05  Promo Day     True\n"
     ]
    }
   ],
   "source": [
    "# === CARGAR DATOS DE MÚLTIPLES FUENTES ===\n",
    "\n",
    "# 1. Cargar datos de viajes desde Parquet (Dataset oficial completo NYC)\n",
    "print(\"Cargando datos oficiales de NYC Taxi (dataset completo)...\")\n",
    "trips_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\"\n",
    "\n",
    "# Cargar dataset oficial (~3M registros de enero 2023) # función para leer archivos .parquet (más eficiente que CSV)\n",
    "trips = pd.read_parquet(trips_url, engine=\"fastparquet\")\n",
    "\n",
    "print(f\"   Viajes cargados: {trips.shape[0]:,} filas, {trips.shape[1]} columnas\")\n",
    "print(f\"   Columnas: {list(trips.columns)}\")\n",
    "print(f\"   Período: {trips['tpep_pickup_datetime'].min()} a {trips['tpep_pickup_datetime'].max()}\")\n",
    "print(f\"   Tamaño en memoria: {trips.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# 2. Cargar datos de zonas desde CSV (Dataset oficial completo)\n",
    "print(\"\\nCargando datos oficiales de zonas NYC...\")\n",
    "zones_url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv\"\n",
    "zones = pd.read_csv(zones_url)  # función estándar para archivos CSV\n",
    "\n",
    "print(f\"   Zonas cargadas: {zones.shape[0]} filas, {zones.shape[1]} columnas\")\n",
    "print(f\"   Columnas: {list(zones.columns)}\")\n",
    "print(f\"   Boroughs únicos: {zones['Borough'].unique()}\")\n",
    "\n",
    "# 3. Cargar calendario de eventos desde JSON \n",
    "print(\"\\nCargando datos de calendario de eventos...\")\n",
    "calendar_url = \"https://juanfkurucz.com/ucu-id/ut1/data/calendar.json\"\n",
    "calendar = pd.read_json(calendar_url)  # función para archivos JSON\n",
    "calendar['date'] = pd.to_datetime(calendar['date']).dt.date  # convertir strings a fechas, luego extraer solo la fecha\n",
    "\n",
    "print(f\"   Eventos calendario: {calendar.shape[0]} filas\")\n",
    "print(f\"   Columnas: {list(calendar.columns)}\")\n",
    "\n",
    "# 4. Mostrar primeras filas de cada dataset\n",
    "print(\"\\nVISTA PREVIA DE DATOS:\")\n",
    "print(\"\\n--- TRIPS ---\")\n",
    "print(trips.head())  # método para mostrar primeras filas de un DataFrame\n",
    "print(\"\\n--- ZONES ---\")\n",
    "print(zones.head())  # mismo método para ver estructura de datos\n",
    "print(\"\\n--- CALENDAR ---\")\n",
    "print(calendar.head())  # revisar formato de los eventos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4263d845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando nombres de columnas...\n",
      "   Trips columnas: ['vendorid', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'ratecodeid', 'store_and_fwd_flag', 'pulocationid', 'dolocationid', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'airport_fee']\n",
      "   Zones columnas: ['locationid', 'borough', 'zone', 'service_zone']\n",
      "   Columna pickup_date creada\n",
      "   Rango de fechas: 2008-12-31 a 2023-02-01\n",
      "\n",
      "VERIFICACIÓN DE TIPOS PARA JOINS:\n",
      "   trips['pulocationid'] tipo: int64\n",
      "   zones['locationid'] tipo: int64\n",
      "   trips['pickup_date'] tipo: <class 'datetime.date'>\n",
      "   calendar['date'] tipo: <class 'datetime.date'>\n",
      "\n",
      "OPTIMIZACIÓN PARA DATASETS GRANDES:\n",
      "   Memoria inicial: 705.5 MB\n",
      "   Optimizando tipos de datos para 3M+ registros...\n",
      "   Limpiando valores nulos antes de optimización...\n",
      "   Registros después de limpieza: 3,066,766\n",
      "   Memoria optimizada: 649.9 MB\n",
      "   Ahorro de memoria: 7.9%\n",
      "\n",
      "DATOS FALTANTES ANTES DE JOINS:\n",
      "Trips (top 5 columnas con más nulos):\n",
      "airport_fee             71743\n",
      "congestion_surcharge    71743\n",
      "ratecodeid              71743\n",
      "store_and_fwd_flag      71743\n",
      "vendorid                    0\n",
      "dtype: int64\n",
      "\n",
      "Zones:\n",
      "locationid      0\n",
      "borough         1\n",
      "zone            1\n",
      "service_zone    2\n",
      "dtype: int64\n",
      "\n",
      "Calendar:\n",
      "date       0\n",
      "name       0\n",
      "special    0\n",
      "dtype: int64\n",
      "\n",
      "ANÁLISIS DE CALIDAD:\n",
      "   Total de viajes: 3,066,766\n",
      "   Viajes sin pickup location: 0\n",
      "   Viajes sin dropoff location: 0\n",
      "   Viajes sin passenger_count: 0\n",
      "\n",
      "ESTRATEGIAS DE LIMPIEZA:\n",
      "   Ubicaciones nulas: Eliminar (crítico para joins)\n",
      "   Passenger_count nulos: Rellenar con valor típico (1)\n",
      "   Tarifas nulas: Revisar caso por caso\n"
     ]
    }
   ],
   "source": [
    "# === NORMALIZAR Y PREPARAR DATOS PARA JOINS ===\n",
    "\n",
    "# 1. Estandarizar nombres de columnas\n",
    "print(\"Normalizando nombres de columnas...\")\n",
    "trips.columns = trips.columns.str.lower()  # convertir todas las columnas a minúsculas\n",
    "zones.columns = zones.columns.str.lower()  # misma transformación para consistencia\n",
    "\n",
    "print(f\"   Trips columnas: {list(trips.columns)}\")\n",
    "print(f\"   Zones columnas: {list(zones.columns)}\")\n",
    "\n",
    "# 2. Crear columna de fecha para el join con calendario\n",
    "trips['pickup_date'] = trips['tpep_pickup_datetime'].dt.date  # extraer solo la fecha (sin hora) de la columna datetime\n",
    "\n",
    "print(f\"   Columna pickup_date creada\")\n",
    "print(f\"   Rango de fechas: {trips['pickup_date'].min()} a {trips['pickup_date'].max()}\")\n",
    "\n",
    "# 3. Verificar tipos de datos para joins\n",
    "print(\"\\nVERIFICACIÓN DE TIPOS PARA JOINS:\")\n",
    "print(f\"   trips['pulocationid'] tipo: {trips['pulocationid'].dtype}\")\n",
    "print(f\"   zones['locationid'] tipo: {zones['locationid'].dtype}\")\n",
    "print(f\"   trips['pickup_date'] tipo: {type(trips['pickup_date'].iloc[0])}\")\n",
    "print(f\"   calendar['date'] tipo: {type(calendar['date'].iloc[0])}\")\n",
    "\n",
    "# 4. Optimización para datasets grandes (~3M registros)\n",
    "print(\"\\nOPTIMIZACIÓN PARA DATASETS GRANDES:\")\n",
    "initial_memory = trips.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"   Memoria inicial: {initial_memory:.1f} MB\")\n",
    "\n",
    "# Optimizar tipos de datos para 3+ millones de registros\n",
    "print(\"   Optimizando tipos de datos para 3M+ registros...\")\n",
    "\n",
    "# Limpiar valores nulos antes de convertir tipos\n",
    "print(\"   Limpiando valores nulos antes de optimización...\")\n",
    "trips['passenger_count'] = trips['passenger_count'].fillna(1)  # método para rellenar valores nulos con un valor específico\n",
    "trips = trips.dropna(subset=['pulocationid', 'dolocationid'])  # eliminar filas críticas sin ubicación (necesarias para joins)\n",
    "\n",
    "# Convertir tipos después de limpiar\n",
    "trips['pulocationid'] = trips['pulocationid'].astype('int16')\n",
    "trips['dolocationid'] = trips['dolocationid'].astype('int16') \n",
    "trips['passenger_count'] = trips['passenger_count'].astype('int8')\n",
    "zones['locationid'] = zones['locationid'].astype('int16')\n",
    "\n",
    "print(f\"   Registros después de limpieza: {len(trips):,}\")\n",
    "\n",
    "optimized_memory = trips.memory_usage(deep=True).sum() / 1024**2\n",
    "savings = ((initial_memory - optimized_memory) / initial_memory * 100)\n",
    "\n",
    "print(f\"   Memoria optimizada: {optimized_memory:.1f} MB\")\n",
    "print(f\"   Ahorro de memoria: {savings:.1f}%\")\n",
    "\n",
    "# 5. Revisar datos faltantes antes de joins\n",
    "print(\"\\nDATOS FALTANTES ANTES DE JOINS:\")\n",
    "print(\"Trips (top 5 columnas con más nulos):\")\n",
    "trips_nulls = trips.isna().sum().sort_values(ascending=False).head()  # método para detectar valores nulos, sumar y ordenar\n",
    "print(trips_nulls)\n",
    "\n",
    "print(\"\\nZones:\")\n",
    "zones_nulls = zones.isna().sum()  # revisar si hay valores faltantes en lookup table\n",
    "print(zones_nulls)\n",
    "\n",
    "print(\"\\nCalendar:\")\n",
    "calendar_nulls = calendar.isna().sum()  # verificar integridad del calendario de eventos\n",
    "print(calendar_nulls)\n",
    "\n",
    "# Análisis de calidad de datos\n",
    "print(\"\\nANÁLISIS DE CALIDAD:\")\n",
    "total_trips = len(trips)\n",
    "print(f\"   Total de viajes: {total_trips:,}\")\n",
    "print(f\"   Viajes sin pickup location: {trips['pulocationid'].isna().sum():,}\")\n",
    "print(f\"   Viajes sin dropoff location: {trips['dolocationid'].isna().sum():,}\")\n",
    "print(f\"   Viajes sin passenger_count: {trips['passenger_count'].isna().sum():,}\")\n",
    "\n",
    "# Estrategias de limpieza recomendadas\n",
    "print(\"\\nESTRATEGIAS DE LIMPIEZA:\")\n",
    "print(\"   Ubicaciones nulas: Eliminar (crítico para joins)\")\n",
    "print(\"   Passenger_count nulos: Rellenar con valor típico (1)\")\n",
    "print(\"   Tarifas nulas: Revisar caso por caso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8eed1071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros en trips: 2995023\n"
     ]
    }
   ],
   "source": [
    "#Eliminar los datos nulos (71000 de 3000000 es razonable)\n",
    "trips = trips.dropna()\n",
    "print(f\"Registros en trips: {len(trips)}\")\n",
    "\n",
    "# Rellenar valores nulos en las columnas de zones con 'Desconocido'\n",
    "zones['borough'] = zones['borough'].fillna('Desconocido')\n",
    "zones['zone'] = zones['zone'].fillna('Desconocido')\n",
    "zones['service_zone'] = zones['service_zone'].fillna('Desconocido')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f79fcf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Realizando join: trips + zones...\n",
      "   Registros antes del join: 2995023\n",
      "   Registros después del join: 2995023\n",
      "   Nuevas columnas añadidas: ['locationid', 'borough', 'zone', 'service_zone']\n",
      "\n",
      "VERIFICACIÓN DEL JOIN:\n",
      "Conteo por Borough:\n",
      "borough\n",
      "Manhattan        2648320\n",
      "Queens            285126\n",
      "Unknown            39788\n",
      "Brooklyn           15478\n",
      "Bronx               3931\n",
      "Desconocido         1632\n",
      "EWR                  409\n",
      "Staten Island        339\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Viajes sin borough asignado: 0\n",
      "\n",
      "MUESTRA DEL DATASET INTEGRADO:\n",
      "   pulocationid    borough               zone  trip_distance  total_amount\n",
      "0           161  Manhattan     Midtown Center           0.97         14.30\n",
      "1            43  Manhattan       Central Park           1.10         16.90\n",
      "2            48  Manhattan       Clinton East           2.51         34.90\n",
      "3           138     Queens  LaGuardia Airport           1.90         20.85\n",
      "4           107  Manhattan           Gramercy           1.43         19.68\n"
     ]
    }
   ],
   "source": [
    "# === PRIMER JOIN: TRIPS + ZONES ===\n",
    "\n",
    "# 1. Hacer join de trips con zones para obtener información geográfica\n",
    "print(\"Realizando join: trips + zones...\")\n",
    "trips_with_zones = trips.merge(zones,   # método principal para unir DataFrames\n",
    "                                left_on='pulocationid',   # columna de trips que contiene ID de zona de pickup\n",
    "                                right_on='locationid',  # columna de zones que contiene ID correspondiente\n",
    "                                how='left')       # tipo de join que mantiene todos los trips\n",
    "\n",
    "print(f\"   Registros antes del join: {len(trips)}\")\n",
    "print(f\"   Registros después del join: {len(trips_with_zones)}\")\n",
    "print(f\"   Nuevas columnas añadidas: {[col for col in trips_with_zones.columns if col not in trips.columns]}\")\n",
    "\n",
    "# 2. Verificar el resultado del join\n",
    "print(\"\\nVERIFICACIÓN DEL JOIN:\")\n",
    "print(\"Conteo por Borough:\")\n",
    "print(trips_with_zones['borough'].value_counts())\n",
    "\n",
    "# 3. Verificar si hay valores nulos después del join\n",
    "null_after_join = trips_with_zones['borough'].isna().sum()  # contar nulos en columna borough\n",
    "print(f\"\\nViajes sin borough asignado: {null_after_join}\")\n",
    "\n",
    "if null_after_join > 0:\n",
    "    print(\"   Algunos viajes no encontraron su zona correspondiente\")\n",
    "    print(\"   LocationIDs problemáticos:\")\n",
    "    problematic_ids = trips_with_zones[trips_with_zones['borough'].isna()]['pulocationid'].unique()  # filtrar filas con nulos\n",
    "    print(f\"   {problematic_ids}\")\n",
    "\n",
    "# 4. Mostrar muestra del resultado\n",
    "print(\"\\nMUESTRA DEL DATASET INTEGRADO:\")\n",
    "print(trips_with_zones[['pulocationid', 'borough', 'zone', 'trip_distance', 'total_amount']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2b1e5453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Realizando join: trips_zones + calendar...\n",
      "   Registros antes del join: 2995023\n",
      "   Registros después del join: 2995023\n",
      "\n",
      "DISTRIBUCIÓN DE DÍAS ESPECIALES:\n",
      "is_special_day\n",
      "False    2995023\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Ejemplos de eventos especiales:\n",
      "   No hay eventos especiales en este período\n",
      "\n",
      "DATASET FINAL INTEGRADO:\n",
      "   Total registros: 2995023\n",
      "   Total columnas: 28\n",
      "   Columnas principales: ['borough', 'zone', 'is_special_day', 'trip_distance', 'total_amount']\n",
      "\n",
      "VERIFICACIÓN FINAL:\n",
      "Datos faltantes por columna clave:\n",
      "   borough: 0 nulos\n",
      "   zone: 0 nulos\n",
      "   trip_distance: 0 nulos\n",
      "   total_amount: 0 nulos\n",
      "   is_special_day: 0 nulos\n"
     ]
    }
   ],
   "source": [
    "# === SEGUNDO JOIN: TRIPS_ZONES + CALENDAR ===\n",
    "\n",
    "# 1. Hacer join con datos de calendario\n",
    "print(\"Realizando join: trips_zones + calendar...\")\n",
    "trips_complete = trips_with_zones.merge(calendar,   # mismo método de join que antes\n",
    "                                         left_on='pickup_date',   # columna de fecha que creamos en trips\n",
    "                                         right_on='date',  # columna de fecha en calendar\n",
    "                                         how='left')       # tipo que mantiene todos los trips aunque no haya evento especial\n",
    "\n",
    "print(f\"   Registros antes del join: {len(trips_with_zones)}\")\n",
    "print(f\"   Registros después del join: {len(trips_complete)}\")\n",
    "\n",
    "# 2. Crear flag de evento especial\n",
    "trips_complete['is_special_day'] = trips_complete['special'].fillna('False')  # método para rellenar nulos con valor por defecto\n",
    "\n",
    "print(\"\\nDISTRIBUCIÓN DE DÍAS ESPECIALES:\")\n",
    "print(trips_complete['is_special_day'].value_counts())\n",
    "print(\"\\nEjemplos de eventos especiales:\")\n",
    "special_days = trips_complete[trips_complete['is_special_day'] == True]\n",
    "if len(special_days) > 0:\n",
    "    print(special_days[['pickup_date', 'special', 'borough']].drop_duplicates())\n",
    "else:\n",
    "    print(\"   No hay eventos especiales en este período\")\n",
    "\n",
    "# 3. Mostrar dataset final integrado\n",
    "print(\"\\nDATASET FINAL INTEGRADO:\")\n",
    "print(f\"   Total registros: {len(trips_complete)}\")\n",
    "print(f\"   Total columnas: {len(trips_complete.columns)}\")\n",
    "print(f\"   Columnas principales: {['borough', 'zone', 'is_special_day', 'trip_distance', 'total_amount']}\")\n",
    "\n",
    "# 4. Verificar integridad de los datos finales\n",
    "print(\"\\nVERIFICACIÓN FINAL:\")\n",
    "print(\"Datos faltantes por columna clave:\")\n",
    "key_columns = ['borough', 'zone', 'trip_distance', 'total_amount', 'is_special_day']\n",
    "for col in key_columns:\n",
    "    missing = trips_complete[col].isna().sum()  # verificar nulos en cada columna clave final\n",
    "    print(f\"   {col}: {missing} nulos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "59a798f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis por Borough (procesando datos grandes)...\n",
      "\n",
      "ANÁLISIS COMPLETO POR BOROUGH:\n",
      "               num_trips  avg_distance  std_distance  median_distance  \\\n",
      "borough                                                                 \n",
      "Manhattan        2648320          2.41         40.73             1.61   \n",
      "Queens            285126         12.32         12.31            11.27   \n",
      "Unknown            39788          7.59        145.56             2.63   \n",
      "Brooklyn           15478          4.48          4.94             3.03   \n",
      "Bronx               3931          5.19          6.40             2.90   \n",
      "Desconocido         1632          2.34          7.56             0.00   \n",
      "EWR                  409          1.60          5.68             0.00   \n",
      "Staten Island        339         11.34         10.24            14.80   \n",
      "\n",
      "               avg_total  std_total  median_total  avg_fare  avg_tip  \\\n",
      "borough                                                                \n",
      "Manhattan          22.35      14.47         19.00     14.64     2.86   \n",
      "Queens             67.35      33.65         70.45     50.04     7.85   \n",
      "Unknown            38.12      30.48         25.30     26.47     4.82   \n",
      "Brooklyn           32.02      23.17         27.00     26.20     2.62   \n",
      "Bronx              34.15      33.83         29.00     30.01     0.65   \n",
      "Desconocido       108.20      96.90         91.20     95.08     9.93   \n",
      "EWR               104.34      62.82        118.50     87.99    12.43   \n",
      "Staten Island      62.44      45.04         67.80     48.68     1.29   \n",
      "\n",
      "               median_tip  avg_passengers  \n",
      "borough                                    \n",
      "Manhattan            2.66            1.36  \n",
      "Queens               8.18            1.39  \n",
      "Unknown              3.14            1.35  \n",
      "Brooklyn             0.00            1.26  \n",
      "Bronx                0.00            1.10  \n",
      "Desconocido          0.01            1.43  \n",
      "EWR                 10.00            1.58  \n",
      "Staten Island        0.00            1.13  \n",
      "\n",
      "ANÁLISIS CON MÉTRICAS EMPRESARIALES:\n",
      "               num_trips  market_share  revenue_per_km  tip_rate\n",
      "borough                                                         \n",
      "Manhattan        2648320          88.4            9.27      19.5\n",
      "Queens            285126           9.5            5.47      15.7\n",
      "Unknown            39788           1.3            5.02      18.2\n",
      "Brooklyn           15478           0.5            7.15      10.0\n",
      "Bronx               3931           0.1            6.58       2.2\n",
      "Desconocido         1632           0.1           46.24      10.4\n",
      "EWR                  409           0.0           65.21      14.1\n",
      "Staten Island        339           0.0            5.51       2.6\n",
      "\n",
      "INSIGHTS PRINCIPALES:\n",
      "   Borough con más viajes: Manhattan\n",
      "   Borough con viajes más largos: Queens\n",
      "   Borough con tarifas más altas: ['Desconocido', 'EWR', 'Queens']\n",
      "   Mejor revenue por km: EWR\n"
     ]
    }
   ],
   "source": [
    "# === ANÁLISIS AGREGADO POR BOROUGH ===\n",
    "\n",
    "# 1. Análisis básico por borough (con dataset grande)\n",
    "print(\"Análisis por Borough (procesando datos grandes)...\")\n",
    "borough_analysis = trips_complete.groupby(by='borough').agg({   # método para agrupar datos, por qué columna geográfica?\n",
    "    'pulocationid': 'count',  # función para contar número de registros/viajes\n",
    "    'trip_distance': ['mean', 'std', 'median'],  # función para promedio + desviación + mediana\n",
    "    'total_amount': ['mean', 'std', 'median'],   # mismas estadísticas para tarifas\n",
    "    'fare_amount': 'mean',     # solo promedio de tarifa base\n",
    "    'tip_amount': ['mean', 'median'],  # estadísticas de propinas\n",
    "    'passenger_count': 'mean'  # función para promedio de pasajeros\n",
    "}).round(2)\n",
    "\n",
    "# Aplanar columnas multi-nivel\n",
    "borough_analysis.columns = ['num_trips', 'avg_distance', 'std_distance', 'median_distance',\n",
    "                           'avg_total', 'std_total', 'median_total', 'avg_fare', \n",
    "                           'avg_tip', 'median_tip', 'avg_passengers']\n",
    "\n",
    "# Ordenar por número de viajes\n",
    "borough_analysis = borough_analysis.sort_values(by='num_trips', ascending=False)  # método para ordenar DataFrame por una columna específica\n",
    "\n",
    "print(\"\\nANÁLISIS COMPLETO POR BOROUGH:\")\n",
    "print(borough_analysis)\n",
    "\n",
    "# 2. Calcular métricas adicionales empresariales\n",
    "borough_analysis['revenue_per_km'] = (borough_analysis['avg_total'] / \n",
    "                                     borough_analysis['avg_distance']).round(2)\n",
    "borough_analysis['tip_rate'] = (borough_analysis['avg_tip'] / \n",
    "                               borough_analysis['avg_fare'] * 100).round(1)\n",
    "borough_analysis['market_share'] = (borough_analysis['num_trips'] / \n",
    "                                  borough_analysis['num_trips'].sum() * 100).round(1)\n",
    "\n",
    "print(\"\\nANÁLISIS CON MÉTRICAS EMPRESARIALES:\")\n",
    "print(borough_analysis[['num_trips', 'market_share', 'revenue_per_km', 'tip_rate']])\n",
    "\n",
    "# 3. Encontrar insights\n",
    "print(\"\\nINSIGHTS PRINCIPALES:\")\n",
    "print(f\"   Borough con más viajes: {borough_analysis.index[0]}\")\n",
    "print(f\"   Borough con viajes más largos: {borough_analysis['avg_distance'].idxmax()}\")\n",
    "print(\"   Borough con tarifas más altas:\", borough_analysis.sort_values(by='avg_total', ascending=False).head(3).index.tolist())\n",
    "print(f\"   Mejor revenue por km: {borough_analysis['revenue_per_km'].idxmax()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c0a41b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   vendorid tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
      "0         2  2023-01-01 00:32:10   2023-01-01 00:40:36                1   \n",
      "1         2  2023-01-01 00:55:08   2023-01-01 01:01:27                1   \n",
      "2         2  2023-01-01 00:25:04   2023-01-01 00:37:49                1   \n",
      "3         1  2023-01-01 00:03:48   2023-01-01 00:13:25                0   \n",
      "4         2  2023-01-01 00:10:29   2023-01-01 00:21:19                1   \n",
      "\n",
      "   trip_distance  ratecodeid store_and_fwd_flag  pulocationid  dolocationid  \\\n",
      "0           0.97         1.0                  N           161           141   \n",
      "1           1.10         1.0                  N            43           237   \n",
      "2           2.51         1.0                  N            48           238   \n",
      "3           1.90         1.0                  N           138             7   \n",
      "4           1.43         1.0                  N           107            79   \n",
      "\n",
      "   payment_type  ...  airport_fee  pickup_date  locationid    borough  \\\n",
      "0             2  ...         0.00   2023-01-01         161  Manhattan   \n",
      "1             1  ...         0.00   2023-01-01          43  Manhattan   \n",
      "2             1  ...         0.00   2023-01-01          48  Manhattan   \n",
      "3             1  ...         1.25   2023-01-01         138     Queens   \n",
      "4             1  ...         0.00   2023-01-01         107  Manhattan   \n",
      "\n",
      "                zone  service_zone  date  name  special is_special_day  \n",
      "0     Midtown Center   Yellow Zone   NaN   NaN      NaN          False  \n",
      "1       Central Park   Yellow Zone   NaN   NaN      NaN          False  \n",
      "2       Clinton East   Yellow Zone   NaN   NaN      NaN          False  \n",
      "3  LaGuardia Airport      Airports   NaN   NaN      NaN          False  \n",
      "4           Gramercy   Yellow Zone   NaN   NaN      NaN          False  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "Análisis: Borough + Día Especial...\n",
      "\n",
      "ANÁLISIS BOROUGH + DÍA ESPECIAL:\n",
      "                              num_trips  avg_distance  avg_total\n",
      "borough       is_special_day                                    \n",
      "Bronx         False                3931          5.19      34.15\n",
      "Brooklyn      False               15478          4.48      32.02\n",
      "Desconocido   False                1632          2.34     108.20\n",
      "EWR           False                 409          1.60     104.34\n",
      "Manhattan     False             2648320          2.41      22.35\n",
      "Queens        False              285126         12.32      67.35\n",
      "Staten Island False                 339         11.34      62.44\n",
      "Unknown       False               39788          7.59      38.12\n",
      "\n",
      "COMPARACIÓN DÍAS NORMALES VS ESPECIALES:\n",
      "            Avg Distance  Avg Amount  Num Trips\n",
      "Día Normal          3.44       26.97    2995023\n",
      "\n",
      "SOLO HAY Día Normal:\n",
      "   Viajes: 2,995,023.0\n",
      "   Distancia promedio: 3.44 millas\n",
      "   Tarifa promedio: $26.97\n",
      "   No hay datos de días especiales para comparar en este período\n"
     ]
    }
   ],
   "source": [
    "# === ANÁLISIS COMPARATIVO: DÍAS NORMALES VS ESPECIALES ===\n",
    "print(trips_complete.head())\n",
    "# 1. Análisis por borough y tipo de día\n",
    "print(\"Análisis: Borough + Día Especial...\")\n",
    "borough_day_analysis = trips_complete.groupby(by=['borough', 'is_special_day']).agg({  # agrupar por DOS columnas: geografía y tipo de día\n",
    "    'pulocationid': 'count',  # función para contar viajes\n",
    "    'trip_distance': 'mean',  # función para promedio de distancia\n",
    "    'total_amount': 'mean'    # función para promedio de tarifa\n",
    "}).round(2)\n",
    "\n",
    "borough_day_analysis.columns = ['num_trips', 'avg_distance', 'avg_total']\n",
    "\n",
    "print(\"\\nANÁLISIS BOROUGH + DÍA ESPECIAL:\")\n",
    "print(borough_day_analysis)\n",
    "\n",
    "# 2. Comparar días normales vs especiales\n",
    "print(\"\\nCOMPARACIÓN DÍAS NORMALES VS ESPECIALES:\")\n",
    "\n",
    "# Pivotear para comparar fácilmente\n",
    "comparison = trips_complete.groupby(by='is_special_day').agg({  # agrupar solo por tipo de día para comparación general\n",
    "    'trip_distance': 'mean',    # promedio de distancia por tipo de día\n",
    "    'total_amount': 'mean',     # promedio de tarifa por tipo de día\n",
    "    'pulocationid': 'count'     # conteo de viajes por tipo de día\n",
    "}).round(2)\n",
    "\n",
    "# Renombrar índices según los valores únicos encontrados\n",
    "unique_day_types = comparison.index.tolist()\n",
    "if len(unique_day_types) == 2:\n",
    "    comparison.index = ['Día Normal', 'Día Especial']\n",
    "elif len(unique_day_types) == 1:\n",
    "    if unique_day_types[0] in ['False', False]:\n",
    "        comparison.index = ['Día Normal']\n",
    "    else:\n",
    "        comparison.index = ['Día Especial']\n",
    "\n",
    "comparison.columns = ['Avg Distance', 'Avg Amount', 'Num Trips']\n",
    "\n",
    "print(comparison)\n",
    "\n",
    "# 3. Calcular diferencias porcentuales\n",
    "if len(comparison) > 1:\n",
    "    # Hay tanto días normales como especiales\n",
    "    if 'Día Normal' in comparison.index and 'Día Especial' in comparison.index:\n",
    "        normal_day = comparison.loc['Día Normal']\n",
    "        special_day = comparison.loc['Día Especial']\n",
    "\n",
    "        print(\"\\nIMPACTO DE DÍAS ESPECIALES:\")\n",
    "        distance_change = ((special_day['Avg Distance'] - normal_day['Avg Distance']) / normal_day['Avg Distance'] * 100)\n",
    "        amount_change = ((special_day['Avg Amount'] - normal_day['Avg Amount']) / normal_day['Avg Amount'] * 100)\n",
    "\n",
    "        print(f\"   Cambio en distancia promedio: {distance_change:+.1f}%\")\n",
    "        print(f\"   Cambio en tarifa promedio: {amount_change:+.1f}%\")\n",
    "    else:\n",
    "        print(\"\\nINFORMACIÓN DE DÍAS:\")\n",
    "        for idx, row in comparison.iterrows():\n",
    "            print(f\"   {idx}: {row['Num Trips']:,} viajes, ${row['Avg Amount']:.2f} promedio\")\n",
    "else:\n",
    "    print(f\"\\nSOLO HAY {comparison.index[0]}:\")\n",
    "    print(f\"   Viajes: {comparison.iloc[0]['Num Trips']:,}\")\n",
    "    print(f\"   Distancia promedio: {comparison.iloc[0]['Avg Distance']:.2f} millas\")\n",
    "    print(f\"   Tarifa promedio: ${comparison.iloc[0]['Avg Amount']:.2f}\")\n",
    "    print(\"   No hay datos de días especiales para comparar en este período\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dbde4fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ Aplicando técnicas para datasets grandes...\n",
      "Dataset grande detectado: 2,995,023 registros\n",
      "Creando muestra estratificada para visualizaciones...\n",
      "Muestra creada: 10,000 registros (0.3%)\n",
      "\n",
      "ANÁLISIS DE PERFORMANCE:\n",
      "   total_trips: 2,995,023\n",
      "   matched_zones: 2,995,023\n",
      "   match_rate: 100.0%\n",
      "   unique_zones_used: 255\n",
      "   total_zones_available: 265\n",
      "   zone_coverage: 96.2%\n",
      "\n",
      "ANÁLISIS TEMPORAL AVANZADO:\n",
      "Horas pico por número de viajes:\n",
      "      18:00 - 210,761.0 viajes\n",
      "      17:00 - 204,808.0 viajes\n",
      "      15:00 - 193,114.0 viajes\n"
     ]
    }
   ],
   "source": [
    "# === TÉCNICAS PARA TRABAJAR CON DATASETS GRANDES ===\n",
    "\n",
    "# 1. Sampling estratégico para visualizaciones\n",
    "print(\"⚡ Aplicando técnicas para datasets grandes...\")\n",
    "\n",
    "# Si el dataset es muy grande, usar muestra para visualizaciones\n",
    "if len(trips_complete) > 50000:\n",
    "    print(f\"Dataset grande detectado: {len(trips_complete):,} registros\")\n",
    "    print(\"Creando muestra estratificada para visualizaciones...\")\n",
    "\n",
    "    # Muestra proporcional por borough\n",
    "    sample_size = min(10000, len(trips_complete) // 10)\n",
    "    trips_sample = trips_complete.sample(n=sample_size, random_state=42)  # método para tomar muestra aleatoria de n registros\n",
    "\n",
    "    print(f\"Muestra creada: {len(trips_sample):,} registros ({len(trips_sample)/len(trips_complete)*100:.1f}%)\")\n",
    "else:\n",
    "    trips_sample = trips_complete\n",
    "    print(\"Dataset pequeño, usando datos completos para visualización\")\n",
    "\n",
    "# 2. Análisis de performance de joins\n",
    "print(\"\\nANÁLISIS DE PERFORMANCE:\")\n",
    "join_stats = {\n",
    "    'total_trips': len(trips),\n",
    "    'matched_zones': (trips_complete['borough'].notna()).sum(),\n",
    "    'match_rate': (trips_complete['borough'].notna().sum() / len(trips) * 100),\n",
    "    'unique_zones_used': trips_complete['zone'].nunique(),\n",
    "    'total_zones_available': len(zones),\n",
    "    'zone_coverage': (trips_complete['zone'].nunique() / len(zones) * 100)\n",
    "}\n",
    "\n",
    "for key, value in join_stats.items():\n",
    "    if 'rate' in key or 'coverage' in key:\n",
    "        print(f\"   {key}: {value:.1f}%\")\n",
    "    else:\n",
    "        print(f\"   {key}: {value:,}\")\n",
    "\n",
    "# 3. Análisis temporal avanzado (solo si hay suficientes datos)\n",
    "if len(trips_complete) > 1000:\n",
    "    print(\"\\nANÁLISIS TEMPORAL AVANZADO:\")\n",
    "\n",
    "    # Análisis por hora del día\n",
    "    trips_complete['pickup_hour'] = trips_complete['tpep_pickup_datetime'].dt.hour  # extraer hora de la fecha/hora\n",
    "    hourly_analysis = trips_complete.groupby(by='pickup_hour').agg({  # agrupar por hora del día\n",
    "        'pulocationid': 'count',     # contar viajes por hora\n",
    "        'total_amount': 'mean',      # tarifa promedio por hora\n",
    "        'trip_distance': 'mean'      # distancia promedio por hora\n",
    "    }).round(2)\n",
    "\n",
    "    hourly_analysis.columns = ['trips_count', 'avg_amount', 'avg_distance']\n",
    "\n",
    "    print(\"Horas pico por número de viajes:\")\n",
    "    peak_hours = hourly_analysis.sort_values(by='trips_count', ascending=False).head(3)  # ordenar por más viajes, tomar top 3\n",
    "    for hour, stats in peak_hours.iterrows():\n",
    "        print(f\"      {hour:02d}:00 - {stats['trips_count']:,} viajes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "53f2cd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando correlaciones entre variables numéricas...\n",
      "\n",
      "Matriz de Correlación:\n",
      "               trip_distance  total_amount  fare_amount  tip_amount\n",
      "trip_distance          1.000         0.094        0.094       0.061\n",
      "total_amount           0.094         1.000        0.980       0.708\n",
      "fare_amount            0.094         0.980        1.000       0.588\n",
      "tip_amount             0.061         0.708        0.588       1.000\n",
      "\n",
      "Correlaciones más fuertes:\n",
      "   total_amount vs fare_amount: 0.980\n",
      "   total_amount vs tip_amount: 0.708\n",
      "   fare_amount vs tip_amount: 0.588\n",
      "\n",
      "INTERPRETACIÓN DE CORRELACIONES:\n",
      "   > 0.7: Correlación fuerte positiva\n",
      "   0.3-0.7: Correlación moderada positiva\n",
      "   -0.3-0.3: Correlación débil\n",
      "   < -0.7: Correlación fuerte negativa\n"
     ]
    }
   ],
   "source": [
    "# === ANÁLISIS DE CORRELACIONES NUMÉRICAS ===\n",
    "\n",
    "# Calcular correlaciones entre variables numéricas\n",
    "print(\"Calculando correlaciones entre variables numéricas...\")\n",
    "numeric_cols = ['trip_distance', 'total_amount', 'fare_amount', 'tip_amount']\n",
    "corr_matrix = trips_complete[numeric_cols].corr()  # método para calcular matriz de correlación\n",
    "\n",
    "print(\"\\nMatriz de Correlación:\")\n",
    "print(corr_matrix.round(3))\n",
    "\n",
    "print(\"\\nCorrelaciones más fuertes:\")\n",
    "corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "\n",
    "corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "for var1, var2, corr in corr_pairs[:3]:\n",
    "    print(f\"   {var1} vs {var2}: {corr:.3f}\")\n",
    "\n",
    "print(\"\\nINTERPRETACIÓN DE CORRELACIONES:\")\n",
    "print(\"   > 0.7: Correlación fuerte positiva\")\n",
    "print(\"   0.3-0.7: Correlación moderada positiva\") \n",
    "print(\"   -0.3-0.3: Correlación débil\")\n",
    "print(\"   < -0.7: Correlación fuerte negativa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4928312",
   "metadata": {},
   "source": [
    "## PREGUNTAS FINALES\n",
    "1. ¿Qué diferencia hay entre un LEFT JOIN y un INNER JOIN? PISTA: Guía visual de joins\n",
    "\n",
    "Usando el left join te quedas con los datos de la tabla, es la que se mantiene, mientras que de la derecha son los valores que se agregan. \n",
    "Usando inner join se hace interseccion de las dos tablas. \n",
    "\n",
    "2. ¿Por qué usamos LEFT JOIN en lugar de INNER JOIN para trips+zones? PISTA: ¿Qué pasaría si algunos viajes no tienen zona asignada?\n",
    "\n",
    "Porque al hacer left te aseguras que vas a mantener toda la informacion de los viajes agregando las zonas correspondientes a los mismos. Si hicieramos inner vamos a perder la informacion de los trips que no tienen zona asignada. \n",
    "\n",
    "3. ¿Qué problemas pueden surgir al hacer joins con datos de fechas? PISTA: Tipos de datos, formatos, zonas horarias\n",
    "\n",
    "• Diferencias en el tipo de dato, por ejemplo: string o datetime.\n",
    "• Formatos de fecha distintos, por ejemplo: YYYY-MM-DD o DD/MM/YYYY.\n",
    "• Valores nulos o fechas faltantes que pueden impedir el join.\n",
    "\n",
    "4. ¿Cuál es la ventaja de integrar múltiples fuentes de datos? PISTA: Análisis más rico, contexto completo, insights cruzados\n",
    "\n",
    "Nos permite realizar un análisis más completo y contextualizado. Además, se pueden cruzar variables de diferentes bases para descubrir patrones que no serían visibles en un solo dataset; esto enriquece la información y habilita conclusiones más profundos.\n",
    "\n",
    "5. ¿Qué insights de negocio obtuviste del análisis integrado? PISTA: Patrones por zona, impacto de eventos especiales, oportunidades\n",
    "\n",
    "Manhattan concentra la mayoría de los viajes, los viajes en Queens son más largos y costosos en promedio y el mejor revenue por km son de EWR. \n",
    "Hay diferencias claras en el revenue por kilómetro y en la tasa de propinas entre boroughs y los días especiales pueden tener impacto en la distancia y tarifa promedio.\n",
    "\n",
    "## BONUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ddc262b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefect instalado y configurado\n",
      "   Versión: 3.4.14\n"
     ]
    }
   ],
   "source": [
    "import prefect\n",
    "from prefect import task, flow, get_run_logger\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "os.environ[\"PREFECT_LOGGING_SERVER_ENABLED\"] = \"false\"\n",
    "\n",
    "print(\"Prefect instalado y configurado\")\n",
    "print(f\"   Versión: {prefect.__version__}\")\n",
    "\n",
    "# === TASKS SIMPLES PARA APRENDER PREFECT ===\n",
    "\n",
    "@task(retries=3, retry_delay_seconds=10, name=\"Cargar Datos\")\n",
    "def cargar_datos(url: str, tipo: str) -> pd.DataFrame:\n",
    "    logger = get_run_logger()\n",
    "    logger.info(f\"Cargando {tipo} desde: {url}\")\n",
    "\n",
    "    tipo = tipo.lower().strip()\n",
    "    if tipo == \"parquet\":\n",
    "        # Requiere pyarrow instalado\n",
    "        return pd.read_parquet(url, engine=\"pyarrow\")\n",
    "    elif tipo == \"csv\":\n",
    "        # Maneja compresión (gzip) automáticamente\n",
    "        return pd.read_csv(url, encoding=\"utf-8\", low_memory=False, compression=\"infer\")\n",
    "    else:\n",
    "        raise ValueError(f\"Tipo no soportado: {tipo} (usa 'csv' o 'parquet')\")\n",
    "\n",
    "\n",
    "@task(name=\"Hacer Join Simple\")\n",
    "def hacer_join_simple(trips: pd.DataFrame, zones: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Task para hacer join básico de trips + zones\"\"\"\n",
    "    logger = get_run_logger()\n",
    "    logger.info(\"Haciendo join simple...\")\n",
    "\n",
    "    # Normalizar columnas\n",
    "    trips.columns = trips.columns.str.lower()  # convertir a minúsculas\n",
    "    zones.columns = zones.columns.str.lower()  # misma transformación\n",
    "\n",
    "    # Join básico\n",
    "    resultado = trips.merge(zones,   # método para unir DataFrames\n",
    "                             left_on='pickup_date',   # columna de pickup location en trips\n",
    "                             right_on='locationid',  # columna de location en zones\n",
    "                             how='left')       # tipo de join que mantiene todos los trips\n",
    "\n",
    "    logger.info(f\"Join completado: {len(resultado)} registros\")\n",
    "    return resultado\n",
    "\n",
    "@task(name=\"Análisis Rápido\")\n",
    "\n",
    "def analisis_rapido(data: pd.DataFrame) -> dict:\n",
    "    \"\"\"Task para análisis básico\"\"\"\n",
    "    logger = get_run_logger()\n",
    "    logger.info(\"Haciendo análisis básico...\")\n",
    "\n",
    "    # Stats simples\n",
    "    stats = {\n",
    "        'total_registros': len(data),\n",
    "        'boroughs': data['borough'].count().head(3).to_dict(),  # método para contar valores\n",
    "        'distancia_promedio': round(data['trip_distance'].mean(), 2),  # método para promedio\n",
    "        'tarifa_promedio': round(data['total_amount'].mean(), 2)  # método para promedio\n",
    "    }\n",
    "\n",
    "    logger.info(f\"Análisis completado: {stats['total_registros']} registros\")\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c58f71a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FLOW PRINCIPAL (EL PIPELINE COMPLETO) ===\n",
    "\n",
    "@flow(name=\"Pipeline Simple NYC Taxi\")\n",
    "def pipeline_taxi_simple():\n",
    "    \"\"\"\n",
    "    Flow simple que conecta todos los tasks\n",
    "    \"\"\"\n",
    "    logger = get_run_logger()\n",
    "    logger.info(\"Iniciando pipeline simple...\")\n",
    "\n",
    "    # URLs de datos\n",
    "    trips_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\"\n",
    "    zones_url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv\"\n",
    "\n",
    "    # PASO 1: Cargar datos (con retry automático si falla)\n",
    "    logger.info(\"Paso 1: Cargando datos...\")\n",
    "    trips = cargar_datos(trips_url, \"parquet\")  # tipo de datos trips\n",
    "    zones = cargar_datos(zones_url, \"csv\")  # tipo de datos zones\n",
    "\n",
    "    # PASO 2: Hacer join\n",
    "    logger.info(\"Paso 2: Haciendo join...\")\n",
    "    data_unida = hacer_join_simple(trips, zones)\n",
    "\n",
    "    # PASO 3: Análisis básico\n",
    "    logger.info(\"Paso 3: Analizando...\")\n",
    "    resultados = analisis_rapido(data_unida)\n",
    "\n",
    "    # PASO 4: Mostrar resultados\n",
    "    logger.info(\"Pipeline completado!\")\n",
    "    logger.info(f\"Resultados: {resultados}\")\n",
    "\n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d72e7c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif __name__ == \"__main__\":\\n    print(\"Ejecutando pipeline simple...\")\\n\\n    # Ejecutar el flow\\n    resultado = pipeline_taxi_simple()  # nombre de la función del flow\\n\\n    print(\"\\nRESULTADOS FINALES:\")\\n    print(f\"   Total registros: {resultado[\\'total_registros\\']:,}\")\\n    print(f\"   Distancia promedio: {resultado[\\'distancia_promedio\\']} millas\")\\n    print(f\"   Tarifa promedio: ${resultado[\\'tarifa_promedio\\']}\")\\n    print(\"\\nTop 3 Boroughs:\")\\n    for borough, count in resultado[\\'top_boroughs\\'].items():  # clave del diccionario que contiene boroughs\\n        print(f\"   {borough}: {count:,} viajes\")'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === EJECUTAR EL PIPELINE ===\n",
    "\"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Ejecutando pipeline simple...\")\n",
    "\n",
    "    # Ejecutar el flow\n",
    "    resultado = pipeline_taxi_simple()  # nombre de la función del flow\n",
    "\n",
    "    print(\"\\nRESULTADOS FINALES:\")\n",
    "    print(f\"   Total registros: {resultado['total_registros']:,}\")\n",
    "    print(f\"   Distancia promedio: {resultado['distancia_promedio']} millas\")\n",
    "    print(f\"   Tarifa promedio: ${resultado['tarifa_promedio']}\")\n",
    "    print(\"\\nTop 3 Boroughs:\")\n",
    "    for borough, count in resultado['top_boroughs'].items():  # clave del diccionario que contiene boroughs\n",
    "        print(f\"   {borough}: {count:,} viajes\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31bdbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANTES - Código normal:\n",
    "def cargar_datos(url):\n",
    "    return pd.read_csv(url)  # Si falla, todo se rompe\n",
    "\n",
    "# DESPUÉS - Con Prefect:\n",
    "@task(retries=2)\n",
    "def cargar_datos(url):\n",
    "    return pd.read_csv(url)  # Si falla, lo intenta 2 veces más"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5e2306",
   "metadata": {},
   "source": [
    "# PREGUNTAS BONUS\n",
    "\n",
    "1. ¿Qué ventaja tiene usar @task en lugar de una función normal? PISTA: ¿Qué pasa si la carga de datos falla temporalmente?\n",
    "\n",
    "    Una función normal en Python se ejecuta sin ningún control extra.\n",
    "\n",
    "    Cuando usas @task:\n",
    "\n",
    "    - Podés definir reintentos automáticos si falla por algo temporal (ej: red caída al bajar datos).\n",
    "\n",
    "    - Podés definir cortes por tiempo (si un paso tarda demasiado, Prefect lo corta).\n",
    "\n",
    "    - Tenés logs integrados en la interfaz de Prefect.\n",
    "\n",
    "    - Cada task queda orquestado y monitoreado: podés ver qué falló y reintentar sólo ese paso.\n",
    "\n",
    "2. ¿Para qué sirve el @flow decorator? PISTA: ¿Cómo conecta y organiza los tasks?\n",
    "    El @flow define un pipeline completo, que organiza y conecta varios @task.\n",
    "\n",
    "    Le dice a Prefect: “Esto no es sólo un script de Python, es un flujo de trabajo con dependencias y monitoreo”.\n",
    "\n",
    "    Permite:\n",
    "\n",
    "    - Ejecutar tasks en orden y pasar resultados de uno a otro.\n",
    "\n",
    "    - Monitorear el flow run entero.\n",
    "\n",
    "3. ¿En qué casos reales usarías esto? PISTA: Reportes diarios, análisis automáticos, pipelines de ML\n",
    "\n",
    "    Reportes diarios: Automatizar que cada mañana se bajen datos de ventas, se limpien y se envíen reportes a un dashboard.\n",
    "\n",
    "    Análisis automáticos: Procesar logs de usuarios, detectar anomalías o generar alertas de fraude sin intervención manual.\n",
    "\n",
    "    Pipelines de ML: cargar dataset - limpiar/preprocesar - entrenar modelo - guardar métricas "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Portafolio \u2014 Inicio","text":"<p>Bienvenido al portafolio del curso de Ingenier\u00eda de datos de Mercedes Barrutia. Aqu\u00ed documentar\u00e9 mi progreso, evidencias y reflexiones a lo largo del semestre.</p>"},{"location":"acerca/","title":"Acerca de m\u00ed","text":"<p>Mi nombre es Mercedes Barrutia, tengo 19 a\u00f1os y estoy estudiando Ingenier\u00eda en Inteligencia Artificial y Ciencia de Datos en la Universidad Cat\u00f3lica del Uruguay. Soy una persona curiosa, organizada y proactiva, con una gran motivaci\u00f3n por seguir aprendiendo y enfrentar desaf\u00edos. </p>"},{"location":"acerca/#habilidades","title":"Habilidades","text":"<ul> <li>Idiomas: Ingl\u00e9s nivel C2</li> <li>Trabajo en equipo: disfruto colaborar en equipos y proyectos multidisciplinarios, aportando ideas y aprendiendo de los dem\u00e1s.</li> <li>Organizaci\u00f3n y proactividad: me destaco por planificar mis tareas de forma eficiente y cumplir objetivos en tiempo y forma.</li> <li>Comunicaci\u00f3n: tengo facilidad para transmitir ideas de manera clara en presentaciones.</li> <li>Intereses: matem\u00e1tica, programaci\u00f3n en Python, an\u00e1lisis de datos, algoritmos de inteligencia artificial y desarrollo de proyectos innovadores.</li> </ul>"},{"location":"acerca/#contacto","title":"Contacto","text":"<ul> <li>mercedes.barrutia@correo.ucu.edu.uy</li> <li>mercedesbarrutiabaldoni@gmail.com</li> <li>093577169</li> </ul>"},{"location":"recursos/","title":"Recursos \u00fatiles","text":"<ul> <li>Gu\u00eda del curso (enlace relativo si aplica)</li> <li>Documentaci\u00f3n de MkDocs Material: <code>https://squidfunk.github.io/mkdocs-material/</code></li> <li>Est\u00e1ndares de portafolio del curso (enlace relativo si aplica)</li> </ul>"},{"location":"portfolio/","title":"Portafolio","text":"<p>! Bienvenido a las entradas del portafolio. </p> <p></p> <ul> <li>Primera entrada: 01-primera-entrada.md</li> <li>Segunda entrada: 02-segunda-entrada.md</li> <li>Tercera entrada: 03-tercera-entrada.md</li> </ul>"},{"location":"portfolio/plantilla/","title":"Plantilla de entrada de portafolio","text":""},{"location":"portfolio/plantilla/#contexto","title":"Contexto","text":"<p>Breve descripci\u00f3n del tema/actividad.</p>"},{"location":"portfolio/plantilla/#objetivos","title":"Objetivos","text":"<ul> <li>Objetivo espec\u00edfico y medible</li> </ul>"},{"location":"portfolio/plantilla/#actividades-con-tiempos-estimados","title":"Actividades (con tiempos estimados)","text":"<ul> <li>Tarea X \u2014 45 min</li> </ul>"},{"location":"portfolio/plantilla/#desarrollo","title":"Desarrollo","text":"<p>Resumen de lo realizado, decisiones y resultados intermedios.</p>"},{"location":"portfolio/plantilla/#evidencias","title":"Evidencias","text":"<ul> <li>Capturas, enlaces a notebooks/repos, resultados, gr\u00e1ficos</li> </ul>"},{"location":"portfolio/plantilla/#reflexion","title":"Reflexi\u00f3n","text":"<ul> <li>Qu\u00e9 aprendiste, qu\u00e9 mejorar\u00edas, pr\u00f3ximos pasos</li> </ul>"},{"location":"portfolio/plantilla/#referencias","title":"Referencias","text":"<ul> <li>Fuentes consultadas con enlaces relativos cuando corresponda</li> </ul>"},{"location":"portfolio/plantilla/#guia-de-formato-y-ejemplos-mkdocs-material","title":"Gu\u00eda de formato y ejemplos (MkDocs Material)","text":"<p>Us\u00e1 estos ejemplos para enriquecer tus entradas. Todos funcionan con la configuraci\u00f3n del template.</p>"},{"location":"portfolio/plantilla/#admoniciones","title":"Admoniciones","text":"<p>Nota</p> <p>Este es un bloque informativo.</p> <p>Sugerencia</p> <p>Consider\u00e1 alternativas y justifica decisiones.</p> <p>Atenci\u00f3n</p> <p>Riesgos, limitaciones o supuestos relevantes.</p>"},{"location":"portfolio/plantilla/#detalles-colapsables","title":"Detalles colapsables","text":"Ver desarrollo paso a paso <ul> <li>Paso 1: preparar datos</li> <li>Paso 2: entrenar modelo</li> <li>Paso 3: evaluar m\u00e9tricas</li> </ul>"},{"location":"portfolio/plantilla/#codigo-con-resaltado-y-lineas-numeradas","title":"C\u00f3digo con resaltado y l\u00edneas numeradas","text":"<pre><code>def train(\n    data_path: str,\n    epochs: int = 10,\n    learning_rate: float = 1e-3,\n) -&gt; None:\n    print(\"Entrenando...\")\n    # TODO: implementar\n</code></pre>"},{"location":"portfolio/plantilla/#listas-de-tareas-checklist","title":"Listas de tareas (checklist)","text":"<ul> <li> Preparar datos</li> <li> Explorar dataset</li> <li> Entrenar baseline</li> </ul>"},{"location":"portfolio/plantilla/#tabla-de-actividades-con-tiempos","title":"Tabla de actividades con tiempos","text":"Actividad Tiempo Resultado esperado Revisi\u00f3n bibliogr\u00e1fica 45m Lista de fuentes priorizadas Implementaci\u00f3n 90m Script ejecutable/documentado Evaluaci\u00f3n 60m M\u00e9tricas y an\u00e1lisis de errores"},{"location":"portfolio/plantilla/#imagenes-con-glightbox-y-atributos","title":"Im\u00e1genes con glightbox y atributos","text":"<p>Imagen directa (abre en lightbox):</p> <p></p> <p>Click para ampliar (lightbox):</p> <p></p>"},{"location":"portfolio/plantilla/#enlaces-internos-y-relativos","title":"Enlaces internos y relativos","text":"<p>Consult\u00e1 tambi\u00e9n: Acerca de m\u00ed y Recursos.</p>"},{"location":"portfolio/plantilla/#notas-al-pie-y-citas","title":"Notas al pie y citas","text":"<p>Texto con una afirmaci\u00f3n que requiere aclaraci\u00f3n<sup>1</sup>.</p>"},{"location":"portfolio/plantilla/#emojis-y-enfasis","title":"Emojis y \u00e9nfasis","text":"<p>Resultados destacados   y conceptos <code>clave</code>.</p> <ol> <li> <p>Esta es una nota al pie con detalles adicionales y referencias.\u00a0\u21a9</p> </li> </ol>"},{"location":"portfolio/entregas/01-primera-entrada/","title":"Entrada 01 \u2014 Mi primera experiencia","text":""},{"location":"portfolio/entregas/01-primera-entrada/#contexto","title":"Contexto","text":"<p>Actividad inicial del curso en la cual trabaj\u00e9 con el dataset Iris. El objetivo fue practicar la carga de datos desde diferentes fuentes (GitHub, seaborn, sklearn, archivo local) y verificar la consistencia entre datasets y comenzar a responder preguntas simples con las especies de flores. </p>"},{"location":"portfolio/entregas/01-primera-entrada/#objetivos","title":"Objetivos","text":"<ul> <li>Familiarizarme con el trabajo del portafolio.</li> <li>Practicar carga y exploraci\u00f3n de datos en Python.</li> <li>Plantear y responder preguntas b\u00e1sicas a partir de un dataset.</li> </ul>"},{"location":"portfolio/entregas/01-primera-entrada/#actividades-con-tiempos-estimados","title":"Actividades (con tiempos estimados)","text":"<ul> <li>Configurar entorno y repositorio \u2014 30 min  </li> <li>Cargar dataset Iris desde distintas fuentes \u2014 15 min  </li> <li>Formular y responder preguntas de negocio iniciales \u2014 30 min  </li> <li>Documentar la experiencia en el portafolio \u2014 30 min  </li> </ul>"},{"location":"portfolio/entregas/01-primera-entrada/#desarrollo","title":"Desarrollo","text":"<ol> <li>Entorno: se instalaron librer\u00edas y se configur\u00f3 la estructura para guardar resultados.  </li> <li>Carga de datos: se import\u00f3 el dataset Iris desde cuatro fuentes distintas:</li> <li>CSV en GitHub.</li> <li>Dataset de seaborn.</li> <li>Dataset de sklearn (<code>load_iris</code>).</li> <li>Archivo subido manualmente.  </li> <li>Comparaci\u00f3n de datasets: se verific\u00f3 que las distintas fuentes entregan la misma informaci\u00f3n.  </li> <li>Preguntas iniciales:</li> <li>\u00bfCu\u00e1l es la especie con p\u00e9talo m\u00e1s largo?  </li> <li>\u00bfExiste relaci\u00f3n entre el largo del s\u00e9palo y del p\u00e9talo?  </li> <li>\u00bfCu\u00e1l es la especie con promedio de s\u00e9palo m\u00e1s ancho?  </li> </ol>"},{"location":"portfolio/entregas/01-primera-entrada/#evidencias","title":"Evidencias","text":"<ul> <li>Notebook del an\u00e1lisis: entrega_uno.ipynb</li> </ul>"},{"location":"portfolio/entregas/01-primera-entrada/#reflexion","title":"Reflexi\u00f3n","text":"<p>Lo m\u00e1s desafiante: cargar los datos desde distintas fuentes y formas que no conoc\u00eda. Pr\u00f3ximos pasos: avanzar hacia an\u00e1lisis m\u00e1s profundo (estad\u00edsticas comparativas por especie) y visualizaciones m\u00e1s avanzadas.</p>"},{"location":"portfolio/entregas/01-primera-entrada/#conclusion","title":"Conclusi\u00f3n","text":"<p>Esta primera pr\u00e1ctica permiti\u00f3 reforzar conceptos b\u00e1sicos de manejo de datos en Python. El dataset Iris fue ideal para practicar carga, exploraci\u00f3n y validaci\u00f3n de datos.</p>"},{"location":"portfolio/entregas/02-segunda-entrega/","title":"An\u00e1lisis Exploratorio de Netflix","text":""},{"location":"portfolio/entregas/02-segunda-entrega/#contexto","title":"Contexto","text":"<p>Se realiz\u00f3 un an\u00e1lisis exploratorio del dataset p\u00fablico de Netflix, que contiene informaci\u00f3n sobre pel\u00edculas y series. El objetivo fue conocer la estructura de los datos, identificar valores faltantes y obtener visualizaciones descriptivas.</p>"},{"location":"portfolio/entregas/02-segunda-entrega/#objetivos","title":"Objetivos","text":"<ul> <li>Cargar y explorar el dataset de Netflix desde una fuente online.</li> <li>Identificar y cuantificar datos faltantes.</li> <li>Generar visualizaciones para entender la distribuci\u00f3n de variables.</li> </ul>"},{"location":"portfolio/entregas/02-segunda-entrega/#actividades-con-tiempos-estimados","title":"Actividades (con tiempos estimados)","text":"<ul> <li>Carga y exploraci\u00f3n inicial del dataset \u2014 15 min  </li> <li>An\u00e1lisis de valores faltantes \u2014 15 min  </li> <li>Visualizaciones exploratorias \u2014 30 min  </li> <li>S\u00edntesis de hallazgos \u2014 30 min  </li> </ul>"},{"location":"portfolio/entregas/02-segunda-entrega/#desarrollo","title":"Desarrollo","text":"<ol> <li>Carga de datos: se utiliz\u00f3 <code>pandas</code> para leer el CSV.  </li> <li>Exploraci\u00f3n inicial: con <code>shape</code>, <code>head</code>, <code>info</code> y <code>describe</code> se obtuvo un panorama general de las variables.  </li> <li>Detecci\u00f3n de valores faltantes: se consulto si hba\u00eda datos faltantes por columna.  </li> <li>Visualizaciones: con <code>matplotlib</code> y <code>seaborn</code> se grafic\u00f3 y se exploraron distribuciones de variables.  </li> <li>Hallazgos preliminares: se detectaron columnas con muchos nulos y se confirm\u00f3 que el dataset tiene m\u00e1s pel\u00edculas que series y que la mayor parte del contenido se concentra en los \u00faltimos 20 a\u00f1os.</li> </ol>"},{"location":"portfolio/entregas/02-segunda-entrega/#evidencias","title":"Evidencias","text":"<ul> <li>Notebook del an\u00e1lisis: entrega_dos.ipynb </li> </ul>"},{"location":"portfolio/entregas/02-segunda-entrega/#reflexion","title":"Reflexi\u00f3n","text":"<p>Aprendizaje: record\u00e9 funciones de pandas, matplotlib y seaborn y aprend\u00ed caracteristicas de EDPA. Pr\u00f3ximos pasos: an\u00e1lisis con mayor profundidad, con datos actualizados y mayor cantidad de variables. </p>"},{"location":"portfolio/entregas/02-segunda-entrega/#conclusion","title":"Conclusi\u00f3n","text":"<p>El an\u00e1lisis exploratorio me permiti\u00f3 validar el dataset de Netflix, con datos que nos permitieron trabajr y practicar (valores faltantes y variables confusas). Aun con esto, se  identificaron tendencias como el crecimiento sostenido de producciones en las \u00faltimas dos d\u00e9cadas y el predominio de pel\u00edculas sobre series. Esta base abre la posibilidad a an\u00e1lisis m\u00e1s avanzados de contenido y estrategias de producci\u00f3n.</p>"},{"location":"portfolio/entregas/02-segunda-entrega/#referencias","title":"Referencias","text":"<p>Dataset: https://www.kaggle.com/shivamb/netflix-shows https://pandas.pydata.org/docs/ https://seaborn.pydata.org/</p>"},{"location":"portfolio/entregas/03-tercera-entrega/","title":"03 tercera entrega","text":"<p>title: \"An\u00e1lisis Exploratorio \u2014 Archivo Tres (NYC Yellow Taxi)\" date: 2025-08-27</p>"},{"location":"portfolio/entregas/03-tercera-entrega/#analisis-exploratorio-archivo-tres-nyc-yellow-taxi","title":"An\u00e1lisis Exploratorio \u2014 Archivo tres (NYC Yellow Taxi)","text":""},{"location":"portfolio/entregas/03-tercera-entrega/#contexto","title":"Contexto","text":"<p>Se realiz\u00f3 un an\u00e1lisis sobre el dataset Yellow Taxi de NYC, integr\u00e1ndolo con el mapa de zonas de taxis y un calendario de eventos. El foco estuvo en validar la calidad de los datos, preparar datos para joins y obtener vistas descriptivas \u00fatiles para luego contestar preguntas.</p>"},{"location":"portfolio/entregas/03-tercera-entrega/#objetivos","title":"Objetivos","text":"<ul> <li>Cargar el dataset principal en formato Parquet y auxiliares (zonas, calendario).</li> <li>Normalizar nombres/tipos y verificar datos para joins.</li> <li>Identificar y cuantificar valores faltantes en columnas clave.</li> <li>Generar m\u00e9tricas descriptivas (horarios, correlaciones, cobertura de zonas).</li> </ul>"},{"location":"portfolio/entregas/03-tercera-entrega/#actividades","title":"Actividades","text":"<ul> <li>Carga y exploraci\u00f3n inicial \u2014 20 min</li> <li>Limpieza, normalizaci\u00f3n y verificaci\u00f3n de joins \u2014 20 min</li> <li>An\u00e1lisis de valores faltantes \u2014 10 min</li> <li>Visualizaciones y m\u00e9tricas \u2014 25 min</li> <li>Conclusiones \u2014 20 min</li> </ul>"},{"location":"portfolio/entregas/03-tercera-entrega/#desarrollo","title":"Desarrollo","text":"<ol> <li> <p>Carga de datos</p> </li> <li> <p>Yellow Taxi: 3,066,766 filas, 19 columnas.</p> </li> <li>Zonas: 265 filas, 4 columnas.</li> <li> <p>Per\u00edodo observado en trips: 2008\u201112\u201131 23:01:42 a 2023\u201102\u201101 00:56:53.</p> </li> <li> <p>Normalizaci\u00f3n y preparaci\u00f3n</p> </li> <li> <p>Estandarizaci\u00f3n de variables en nombres de columnas (ej.: <code>PULocationID</code> \u2192 <code>pulocationid</code>).</p> </li> <li>Creaci\u00f3n de <code>pickup_date</code> y verificaci\u00f3n de tipos para joins.</li> <li> <p>Limpieza b\u00e1sica previa a joins.</p> </li> <li> <p>Integraci\u00f3n (JOINS)</p> </li> <li> <p>trips + zones</p> </li> <li> <p>trips + zones + calendar</p> </li> <li> <p>Verificamos los valores faltantes</p> </li> </ol>"},{"location":"portfolio/entregas/03-tercera-entrega/#evidencias","title":"Evidencias","text":"<ul> <li>Notebook del an\u00e1lisis: 03-tercera-entrega.ipynb</li> </ul>"},{"location":"portfolio/entregas/03-tercera-entrega/#reflexion","title":"Reflexi\u00f3n","text":"<ul> <li> <p>Aprendizajes t\u00e9cnicos: lectura directa de Parquet (mejor que CSV en tiempos y memoria), verificaci\u00f3n de datos para evitar errores al hacer join y la utilidad del LEFT JOIN para conservar todos los viajes aunque falte la zona. </p> </li> <li> <p>Siguientes pasos:</p> </li> <li> <p>Incorporar dropoff zones y an\u00e1lisis origen\u2011destino.</p> </li> <li>Modelar demanda por franja horaria y borough. </li> <li>Posible incorporaci\u00f3n de **tarifas din\u00e1micas.</li> </ul>"},{"location":"portfolio/entregas/03-tercera-entrega/#conclusion","title":"Conclusi\u00f3n","text":"<p>El dataset est\u00e1 completo para las variables que analizamos y fue \u00fatil para aprender sobre parquet, tema desconocido hasta el momento. Ahora la base queda lista para profundizar en patrones origen\u2011destino o segmentaci\u00f3n por zonas.</p>"},{"location":"portfolio/entregas/03-tercera-entrega/#referencias","title":"Referencias","text":"<ul> <li>pandas (documentaci\u00f3n): https://pandas.pydata.org/docs/</li> <li>pyarrow / fastparquet </li> </ul>"},{"location":"portfolio/entregas/dos/","title":"3. Pregunta de negocio","text":"In\u00a0[\u00a0]: Copied! <pre>!pip -q install seaborn\nimport seaborn as sns\n\n\nimport sys, pandas as pd, seaborn as sns, matplotlib\nprint(sys.version)\nprint(pd.__version__, sns.__version__, matplotlib.__version__)\n\nfrom pathlib import Path\ntry:\n    from google.colab import drive\n    drive.mount('/content/drive')\n    ROOT = Path('/content/drive/MyDrive/ID-UT1-iris')\nexcept Exception:\n    ROOT = Path.cwd() / 'ID-UT1-iris'\n\nPLOTS_DIR = ROOT / 'results' / 'visualizaciones'\nPERF_DIR = ROOT / 'results' / 'perfiles'\nREPORT_DIR = ROOT / 'results' / 'reportes'\nfor d in (PLOTS_DIR, PERF_DIR, REPORT_DIR):\n    d.mkdir(parents=True, exist_ok=True)\nprint('Outputs \u2192', ROOT)\n</pre> !pip -q install seaborn import seaborn as sns   import sys, pandas as pd, seaborn as sns, matplotlib print(sys.version) print(pd.__version__, sns.__version__, matplotlib.__version__)  from pathlib import Path try:     from google.colab import drive     drive.mount('/content/drive')     ROOT = Path('/content/drive/MyDrive/ID-UT1-iris') except Exception:     ROOT = Path.cwd() / 'ID-UT1-iris'  PLOTS_DIR = ROOT / 'results' / 'visualizaciones' PERF_DIR = ROOT / 'results' / 'perfiles' REPORT_DIR = ROOT / 'results' / 'reportes' for d in (PLOTS_DIR, PERF_DIR, REPORT_DIR):     d.mkdir(parents=True, exist_ok=True) print('Outputs \u2192', ROOT) <pre>3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n2.2.2 0.13.2 3.10.0\nMounted at /content/drive\nOutputs \u2192 /content/drive/MyDrive/ID-UT1-iris\n</pre> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nurl = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv'\ndf1 = pd.read_csv(url)\ndf1.head()\n</pre> import pandas as pd url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv' df1 = pd.read_csv(url) df1.head() Out[\u00a0]: sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa In\u00a0[\u00a0]: Copied! <pre>import seaborn as sns\ndf2 = sns.load_dataset('iris')\ndf2.head()\n</pre> import seaborn as sns df2 = sns.load_dataset('iris') df2.head() Out[\u00a0]: sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa In\u00a0[\u00a0]: Copied! <pre>from sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris(as_frame=True)\ndf3 = data.frame\ndf3.rename(columns={'target': 'species'}, inplace=True)\ndf3.head()\n</pre> from sklearn.datasets import load_iris import pandas as pd data = load_iris(as_frame=True) df3 = data.frame df3.rename(columns={'target': 'species'}, inplace=True) df3.head() Out[\u00a0]: sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) species 0 5.1 3.5 1.4 0.2 0 1 4.9 3.0 1.4 0.2 0 2 4.7 3.2 1.3 0.2 0 3 4.6 3.1 1.5 0.2 0 4 5.0 3.6 1.4 0.2 0 In\u00a0[\u00a0]: Copied! <pre>from google.colab import files\nimport io, pandas as pd\nuploaded = files.upload()\ndf4 = pd.read_csv(io.BytesIO(uploaded[list(uploaded.keys())[0]]))\ndf4.head()\n</pre> from google.colab import files import io, pandas as pd uploaded = files.upload() df4 = pd.read_csv(io.BytesIO(uploaded[list(uploaded.keys())[0]])) df4.head()        Upload widget is only available when the cell has been executed in the       current browser session. Please rerun this cell to enable.        <pre>Saving Iris.csv to Iris.csv\n</pre> Out[\u00a0]: Id SepalLengthCm SepalWidthCm PetalLengthCm PetalWidthCm Species 0 1 5.1 3.5 1.4 0.2 Iris-setosa 1 2 4.9 3.0 1.4 0.2 Iris-setosa 2 3 4.7 3.2 1.3 0.2 Iris-setosa 3 4 4.6 3.1 1.5 0.2 Iris-setosa 4 5 5.0 3.6 1.4 0.2 Iris-setosa In\u00a0[\u00a0]: Copied! <pre>if df1.equals(df2):\n  print(\"Las bases 1 y 2 son iguales\")\nelse:\n  print(\"Las bases 1 y 2 son distintas\")\n\nif df3.equals(df2):\n  print(\"Las bases 2 y 3 son iguales\")\nelse:\n  print(\"Las bases 2 y 3 son distintas\")\n\nif df3.equals(df4):\n  print(\"Las bases 4 y 3 son iguales\")\nelse:\n  print(\"Las bases 4 y 3 son distintas\")\n\nif df1.equals(df4):\n  print(\"Las bases 1 y 4 son iguales\")\nelse:\n  print(\"Las bases 1 y 4 son distintas\")\n</pre> if df1.equals(df2):   print(\"Las bases 1 y 2 son iguales\") else:   print(\"Las bases 1 y 2 son distintas\")  if df3.equals(df2):   print(\"Las bases 2 y 3 son iguales\") else:   print(\"Las bases 2 y 3 son distintas\")  if df3.equals(df4):   print(\"Las bases 4 y 3 son iguales\") else:   print(\"Las bases 4 y 3 son distintas\")  if df1.equals(df4):   print(\"Las bases 1 y 4 son iguales\") else:   print(\"Las bases 1 y 4 son distintas\")    <pre>Las bases 1 y 2 son iguales\nLas bases 2 y 3 son distintas\nLas bases 4 y 3 son distintas\nLas bases 1 y 4 son distintas\n</pre> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\ndf1.shape, df1.dtypes, df1.isna().sum()\ndisplay(df1.describe(include='all').T)\n</pre> import pandas as pd df1.shape, df1.dtypes, df1.isna().sum() display(df1.describe(include='all').T) count unique top freq mean std min 25% 50% 75% max sepal_length 150.0 NaN NaN NaN 5.843333 0.828066 4.3 5.1 5.8 6.4 7.9 sepal_width 150.0 NaN NaN NaN 3.057333 0.435866 2.0 2.8 3.0 3.3 4.4 petal_length 150.0 NaN NaN NaN 3.758 1.765298 1.0 1.6 4.35 5.1 6.9 petal_width 150.0 NaN NaN NaN 1.199333 0.762238 0.1 0.3 1.3 1.8 2.5 species 150 3 setosa 50 NaN NaN NaN NaN NaN NaN NaN In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\ndf2.shape, df2.dtypes, df2.isna().sum()\ndisplay(df2.describe(include='all').T)\n</pre> import pandas as pd df2.shape, df2.dtypes, df2.isna().sum() display(df2.describe(include='all').T) count unique top freq mean std min 25% 50% 75% max sepal_length 150.0 NaN NaN NaN 5.843333 0.828066 4.3 5.1 5.8 6.4 7.9 sepal_width 150.0 NaN NaN NaN 3.057333 0.435866 2.0 2.8 3.0 3.3 4.4 petal_length 150.0 NaN NaN NaN 3.758 1.765298 1.0 1.6 4.35 5.1 6.9 petal_width 150.0 NaN NaN NaN 1.199333 0.762238 0.1 0.3 1.3 1.8 2.5 species 150 3 setosa 50 NaN NaN NaN NaN NaN NaN NaN In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\ndf3.shape, df3.dtypes, df3.isna().sum()\ndisplay(df3.describe(include='all').T)\n</pre> import pandas as pd df3.shape, df3.dtypes, df3.isna().sum() display(df3.describe(include='all').T) count mean std min 25% 50% 75% max sepal length (cm) 150.0 5.843333 0.828066 4.3 5.1 5.80 6.4 7.9 sepal width (cm) 150.0 3.057333 0.435866 2.0 2.8 3.00 3.3 4.4 petal length (cm) 150.0 3.758000 1.765298 1.0 1.6 4.35 5.1 6.9 petal width (cm) 150.0 1.199333 0.762238 0.1 0.3 1.30 1.8 2.5 species 150.0 1.000000 0.819232 0.0 0.0 1.00 2.0 2.0 In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\ndf4.shape, df4.dtypes, df4.isna().sum()\ndisplay(df4.describe(include='all').T)\n</pre> import pandas as pd df4.shape, df4.dtypes, df4.isna().sum() display(df4.describe(include='all').T) count unique top freq mean std min 25% 50% 75% max Id 150.0 NaN NaN NaN 75.5 43.445368 1.0 38.25 75.5 112.75 150.0 SepalLengthCm 150.0 NaN NaN NaN 5.843333 0.828066 4.3 5.1 5.8 6.4 7.9 SepalWidthCm 150.0 NaN NaN NaN 3.054 0.433594 2.0 2.8 3.0 3.3 4.4 PetalLengthCm 150.0 NaN NaN NaN 3.758667 1.76442 1.0 1.6 4.35 5.1 6.9 PetalWidthCm 150.0 NaN NaN NaN 1.198667 0.763161 0.1 0.3 1.3 1.8 2.5 Species 150 3 Iris-setosa 50 NaN NaN NaN NaN NaN NaN NaN In\u00a0[44]: Copied! <pre>from pandas.api.types import is_string_dtype\n\nif is_string_dtype(df1['species']):\n    df1['species'] = df1['species'].astype('category')\n\n\ndesc = df1.describe()\ncorr = df1.select_dtypes('number').corr()\nprint(desc); print(corr)\n</pre> from pandas.api.types import is_string_dtype  if is_string_dtype(df1['species']):     df1['species'] = df1['species'].astype('category')   desc = df1.describe() corr = df1.select_dtypes('number').corr() print(desc); print(corr)  <pre>       sepal_length  sepal_width  petal_length  petal_width\ncount    150.000000   150.000000    150.000000   150.000000\nmean       5.843333     3.057333      3.758000     1.199333\nstd        0.828066     0.435866      1.765298     0.762238\nmin        4.300000     2.000000      1.000000     0.100000\n25%        5.100000     2.800000      1.600000     0.300000\n50%        5.800000     3.000000      4.350000     1.300000\n75%        6.400000     3.300000      5.100000     1.800000\nmax        7.900000     4.400000      6.900000     2.500000\n              sepal_length  sepal_width  petal_length  petal_width\nsepal_length      1.000000    -0.117570      0.871754     0.817941\nsepal_width      -0.117570     1.000000     -0.428440    -0.366126\npetal_length      0.871754    -0.428440      1.000000     0.962865\npetal_width       0.817941    -0.366126      0.962865     1.000000\n</pre>"},{"location":"portfolio/entregas/dos/#3-pregunta-de-negocio","title":"3. Pregunta de negocio\u00b6","text":"<ol> <li>\u00bfCu\u00e1l es la especie con p\u00e9talo m\u00e1s largo?</li> <li>Hay alguna relacion con el largo del sepal y el del petalo</li> <li>\u00bfCu\u00e1l es la especia con promedio de sepal m\u00e1s ancho?</li> </ol>"},{"location":"portfolio/entregas/tres/","title":"PREGUNTAS BONUS","text":"In\u00a0[52]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sqlite3\nfrom pathlib import Path\n\n# Configurar visualizaciones\nplt.style.use('default')\nsns.set_palette('husl')\nplt.rcParams['figure.figsize'] = (10, 6)\n\nprint(\"Setup completo para an\u00e1lisis multi-fuentes!\")\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import sqlite3 from pathlib import Path  # Configurar visualizaciones plt.style.use('default') sns.set_palette('husl') plt.rcParams['figure.figsize'] = (10, 6)  print(\"Setup completo para an\u00e1lisis multi-fuentes!\") <pre>Setup completo para an\u00e1lisis multi-fuentes!\n</pre> In\u00a0[53]: Copied! <pre># === CARGAR DATOS DE M\u00daLTIPLES FUENTES ===\n\n# 1. Cargar datos de viajes desde Parquet (Dataset oficial completo NYC)\nprint(\"Cargando datos oficiales de NYC Taxi (dataset completo)...\")\ntrips_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\"\n\n# Cargar dataset oficial (~3M registros de enero 2023) # funci\u00f3n para leer archivos .parquet (m\u00e1s eficiente que CSV)\ntrips = pd.read_parquet(trips_url, engine=\"fastparquet\")\n\nprint(f\"   Viajes cargados: {trips.shape[0]:,} filas, {trips.shape[1]} columnas\")\nprint(f\"   Columnas: {list(trips.columns)}\")\nprint(f\"   Per\u00edodo: {trips['tpep_pickup_datetime'].min()} a {trips['tpep_pickup_datetime'].max()}\")\nprint(f\"   Tama\u00f1o en memoria: {trips.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n\n# 2. Cargar datos de zonas desde CSV (Dataset oficial completo)\nprint(\"\\nCargando datos oficiales de zonas NYC...\")\nzones_url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv\"\nzones = pd.read_csv(zones_url)  # funci\u00f3n est\u00e1ndar para archivos CSV\n\nprint(f\"   Zonas cargadas: {zones.shape[0]} filas, {zones.shape[1]} columnas\")\nprint(f\"   Columnas: {list(zones.columns)}\")\nprint(f\"   Boroughs \u00fanicos: {zones['Borough'].unique()}\")\n\n# 3. Cargar calendario de eventos desde JSON \nprint(\"\\nCargando datos de calendario de eventos...\")\ncalendar_url = \"https://juanfkurucz.com/ucu-id/ut1/data/calendar.json\"\ncalendar = pd.read_json(calendar_url)  # funci\u00f3n para archivos JSON\ncalendar['date'] = pd.to_datetime(calendar['date']).dt.date  # convertir strings a fechas, luego extraer solo la fecha\n\nprint(f\"   Eventos calendario: {calendar.shape[0]} filas\")\nprint(f\"   Columnas: {list(calendar.columns)}\")\n\n# 4. Mostrar primeras filas de cada dataset\nprint(\"\\nVISTA PREVIA DE DATOS:\")\nprint(\"\\n--- TRIPS ---\")\nprint(trips.head())  # m\u00e9todo para mostrar primeras filas de un DataFrame\nprint(\"\\n--- ZONES ---\")\nprint(zones.head())  # mismo m\u00e9todo para ver estructura de datos\nprint(\"\\n--- CALENDAR ---\")\nprint(calendar.head())  # revisar formato de los eventos\n</pre> # === CARGAR DATOS DE M\u00daLTIPLES FUENTES ===  # 1. Cargar datos de viajes desde Parquet (Dataset oficial completo NYC) print(\"Cargando datos oficiales de NYC Taxi (dataset completo)...\") trips_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\"  # Cargar dataset oficial (~3M registros de enero 2023) # funci\u00f3n para leer archivos .parquet (m\u00e1s eficiente que CSV) trips = pd.read_parquet(trips_url, engine=\"fastparquet\")  print(f\"   Viajes cargados: {trips.shape[0]:,} filas, {trips.shape[1]} columnas\") print(f\"   Columnas: {list(trips.columns)}\") print(f\"   Per\u00edodo: {trips['tpep_pickup_datetime'].min()} a {trips['tpep_pickup_datetime'].max()}\") print(f\"   Tama\u00f1o en memoria: {trips.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")  # 2. Cargar datos de zonas desde CSV (Dataset oficial completo) print(\"\\nCargando datos oficiales de zonas NYC...\") zones_url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv\" zones = pd.read_csv(zones_url)  # funci\u00f3n est\u00e1ndar para archivos CSV  print(f\"   Zonas cargadas: {zones.shape[0]} filas, {zones.shape[1]} columnas\") print(f\"   Columnas: {list(zones.columns)}\") print(f\"   Boroughs \u00fanicos: {zones['Borough'].unique()}\")  # 3. Cargar calendario de eventos desde JSON  print(\"\\nCargando datos de calendario de eventos...\") calendar_url = \"https://juanfkurucz.com/ucu-id/ut1/data/calendar.json\" calendar = pd.read_json(calendar_url)  # funci\u00f3n para archivos JSON calendar['date'] = pd.to_datetime(calendar['date']).dt.date  # convertir strings a fechas, luego extraer solo la fecha  print(f\"   Eventos calendario: {calendar.shape[0]} filas\") print(f\"   Columnas: {list(calendar.columns)}\")  # 4. Mostrar primeras filas de cada dataset print(\"\\nVISTA PREVIA DE DATOS:\") print(\"\\n--- TRIPS ---\") print(trips.head())  # m\u00e9todo para mostrar primeras filas de un DataFrame print(\"\\n--- ZONES ---\") print(zones.head())  # mismo m\u00e9todo para ver estructura de datos print(\"\\n--- CALENDAR ---\") print(calendar.head())  # revisar formato de los eventos <pre>Cargando datos oficiales de NYC Taxi (dataset completo)...\n   Viajes cargados: 3,066,766 filas, 19 columnas\n   Columnas: ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'airport_fee']\n   Per\u00edodo: 2008-12-31 23:01:42 a 2023-02-01 00:56:53\n   Tama\u00f1o en memoria: 588.5 MB\n\nCargando datos oficiales de zonas NYC...\n   Zonas cargadas: 265 filas, 4 columnas\n   Columnas: ['LocationID', 'Borough', 'Zone', 'service_zone']\n   Boroughs \u00fanicos: ['EWR' 'Queens' 'Bronx' 'Manhattan' 'Staten Island' 'Brooklyn' 'Unknown'\n nan]\n\nCargando datos de calendario de eventos...\n   Eventos calendario: 3 filas\n   Columnas: ['date', 'name', 'special']\n\nVISTA PREVIA DE DATOS:\n\n--- TRIPS ---\n   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n0         2  2023-01-01 00:32:10   2023-01-01 00:40:36              1.0   \n1         2  2023-01-01 00:55:08   2023-01-01 01:01:27              1.0   \n2         2  2023-01-01 00:25:04   2023-01-01 00:37:49              1.0   \n3         1  2023-01-01 00:03:48   2023-01-01 00:13:25              0.0   \n4         2  2023-01-01 00:10:29   2023-01-01 00:21:19              1.0   \n\n   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n0           0.97         1.0                  N           161           141   \n1           1.10         1.0                  N            43           237   \n2           2.51         1.0                  N            48           238   \n3           1.90         1.0                  N           138             7   \n4           1.43         1.0                  N           107            79   \n\n   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n0             2          9.3   1.00      0.5        0.00           0.0   \n1             1          7.9   1.00      0.5        4.00           0.0   \n2             1         14.9   1.00      0.5       15.00           0.0   \n3             1         12.1   7.25      0.5        0.00           0.0   \n4             1         11.4   1.00      0.5        3.28           0.0   \n\n   improvement_surcharge  total_amount  congestion_surcharge  airport_fee  \n0                    1.0         14.30                   2.5         0.00  \n1                    1.0         16.90                   2.5         0.00  \n2                    1.0         34.90                   2.5         0.00  \n3                    1.0         20.85                   0.0         1.25  \n4                    1.0         19.68                   2.5         0.00  \n\n--- ZONES ---\n   LocationID        Borough                     Zone service_zone\n0           1            EWR           Newark Airport          EWR\n1           2         Queens              Jamaica Bay    Boro Zone\n2           3          Bronx  Allerton/Pelham Gardens    Boro Zone\n3           4      Manhattan            Alphabet City  Yellow Zone\n4           5  Staten Island            Arden Heights    Boro Zone\n\n--- CALENDAR ---\n         date       name  special\n0  2022-01-01   New Year     True\n1  2022-01-03  Event Day     True\n2  2022-01-05  Promo Day     True\n</pre> In\u00a0[54]: Copied! <pre># === NORMALIZAR Y PREPARAR DATOS PARA JOINS ===\n\n# 1. Estandarizar nombres de columnas\nprint(\"Normalizando nombres de columnas...\")\ntrips.columns = trips.columns.str.lower()  # convertir todas las columnas a min\u00fasculas\nzones.columns = zones.columns.str.lower()  # misma transformaci\u00f3n para consistencia\n\nprint(f\"   Trips columnas: {list(trips.columns)}\")\nprint(f\"   Zones columnas: {list(zones.columns)}\")\n\n# 2. Crear columna de fecha para el join con calendario\ntrips['pickup_date'] = trips['tpep_pickup_datetime'].dt.date  # extraer solo la fecha (sin hora) de la columna datetime\n\nprint(f\"   Columna pickup_date creada\")\nprint(f\"   Rango de fechas: {trips['pickup_date'].min()} a {trips['pickup_date'].max()}\")\n\n# 3. Verificar tipos de datos para joins\nprint(\"\\nVERIFICACI\u00d3N DE TIPOS PARA JOINS:\")\nprint(f\"   trips['pulocationid'] tipo: {trips['pulocationid'].dtype}\")\nprint(f\"   zones['locationid'] tipo: {zones['locationid'].dtype}\")\nprint(f\"   trips['pickup_date'] tipo: {type(trips['pickup_date'].iloc[0])}\")\nprint(f\"   calendar['date'] tipo: {type(calendar['date'].iloc[0])}\")\n\n# 4. Optimizaci\u00f3n para datasets grandes (~3M registros)\nprint(\"\\nOPTIMIZACI\u00d3N PARA DATASETS GRANDES:\")\ninitial_memory = trips.memory_usage(deep=True).sum() / 1024**2\nprint(f\"   Memoria inicial: {initial_memory:.1f} MB\")\n\n# Optimizar tipos de datos para 3+ millones de registros\nprint(\"   Optimizando tipos de datos para 3M+ registros...\")\n\n# Limpiar valores nulos antes de convertir tipos\nprint(\"   Limpiando valores nulos antes de optimizaci\u00f3n...\")\ntrips['passenger_count'] = trips['passenger_count'].fillna(1)  # m\u00e9todo para rellenar valores nulos con un valor espec\u00edfico\ntrips = trips.dropna(subset=['pulocationid', 'dolocationid'])  # eliminar filas cr\u00edticas sin ubicaci\u00f3n (necesarias para joins)\n\n# Convertir tipos despu\u00e9s de limpiar\ntrips['pulocationid'] = trips['pulocationid'].astype('int16')\ntrips['dolocationid'] = trips['dolocationid'].astype('int16') \ntrips['passenger_count'] = trips['passenger_count'].astype('int8')\nzones['locationid'] = zones['locationid'].astype('int16')\n\nprint(f\"   Registros despu\u00e9s de limpieza: {len(trips):,}\")\n\noptimized_memory = trips.memory_usage(deep=True).sum() / 1024**2\nsavings = ((initial_memory - optimized_memory) / initial_memory * 100)\n\nprint(f\"   Memoria optimizada: {optimized_memory:.1f} MB\")\nprint(f\"   Ahorro de memoria: {savings:.1f}%\")\n\n# 5. Revisar datos faltantes antes de joins\nprint(\"\\nDATOS FALTANTES ANTES DE JOINS:\")\nprint(\"Trips (top 5 columnas con m\u00e1s nulos):\")\ntrips_nulls = trips.isna().sum().sort_values(ascending=False).head()  # m\u00e9todo para detectar valores nulos, sumar y ordenar\nprint(trips_nulls)\n\nprint(\"\\nZones:\")\nzones_nulls = zones.isna().sum()  # revisar si hay valores faltantes en lookup table\nprint(zones_nulls)\n\nprint(\"\\nCalendar:\")\ncalendar_nulls = calendar.isna().sum()  # verificar integridad del calendario de eventos\nprint(calendar_nulls)\n\n# An\u00e1lisis de calidad de datos\nprint(\"\\nAN\u00c1LISIS DE CALIDAD:\")\ntotal_trips = len(trips)\nprint(f\"   Total de viajes: {total_trips:,}\")\nprint(f\"   Viajes sin pickup location: {trips['pulocationid'].isna().sum():,}\")\nprint(f\"   Viajes sin dropoff location: {trips['dolocationid'].isna().sum():,}\")\nprint(f\"   Viajes sin passenger_count: {trips['passenger_count'].isna().sum():,}\")\n\n# Estrategias de limpieza recomendadas\nprint(\"\\nESTRATEGIAS DE LIMPIEZA:\")\nprint(\"   Ubicaciones nulas: Eliminar (cr\u00edtico para joins)\")\nprint(\"   Passenger_count nulos: Rellenar con valor t\u00edpico (1)\")\nprint(\"   Tarifas nulas: Revisar caso por caso\")\n</pre> # === NORMALIZAR Y PREPARAR DATOS PARA JOINS ===  # 1. Estandarizar nombres de columnas print(\"Normalizando nombres de columnas...\") trips.columns = trips.columns.str.lower()  # convertir todas las columnas a min\u00fasculas zones.columns = zones.columns.str.lower()  # misma transformaci\u00f3n para consistencia  print(f\"   Trips columnas: {list(trips.columns)}\") print(f\"   Zones columnas: {list(zones.columns)}\")  # 2. Crear columna de fecha para el join con calendario trips['pickup_date'] = trips['tpep_pickup_datetime'].dt.date  # extraer solo la fecha (sin hora) de la columna datetime  print(f\"   Columna pickup_date creada\") print(f\"   Rango de fechas: {trips['pickup_date'].min()} a {trips['pickup_date'].max()}\")  # 3. Verificar tipos de datos para joins print(\"\\nVERIFICACI\u00d3N DE TIPOS PARA JOINS:\") print(f\"   trips['pulocationid'] tipo: {trips['pulocationid'].dtype}\") print(f\"   zones['locationid'] tipo: {zones['locationid'].dtype}\") print(f\"   trips['pickup_date'] tipo: {type(trips['pickup_date'].iloc[0])}\") print(f\"   calendar['date'] tipo: {type(calendar['date'].iloc[0])}\")  # 4. Optimizaci\u00f3n para datasets grandes (~3M registros) print(\"\\nOPTIMIZACI\u00d3N PARA DATASETS GRANDES:\") initial_memory = trips.memory_usage(deep=True).sum() / 1024**2 print(f\"   Memoria inicial: {initial_memory:.1f} MB\")  # Optimizar tipos de datos para 3+ millones de registros print(\"   Optimizando tipos de datos para 3M+ registros...\")  # Limpiar valores nulos antes de convertir tipos print(\"   Limpiando valores nulos antes de optimizaci\u00f3n...\") trips['passenger_count'] = trips['passenger_count'].fillna(1)  # m\u00e9todo para rellenar valores nulos con un valor espec\u00edfico trips = trips.dropna(subset=['pulocationid', 'dolocationid'])  # eliminar filas cr\u00edticas sin ubicaci\u00f3n (necesarias para joins)  # Convertir tipos despu\u00e9s de limpiar trips['pulocationid'] = trips['pulocationid'].astype('int16') trips['dolocationid'] = trips['dolocationid'].astype('int16')  trips['passenger_count'] = trips['passenger_count'].astype('int8') zones['locationid'] = zones['locationid'].astype('int16')  print(f\"   Registros despu\u00e9s de limpieza: {len(trips):,}\")  optimized_memory = trips.memory_usage(deep=True).sum() / 1024**2 savings = ((initial_memory - optimized_memory) / initial_memory * 100)  print(f\"   Memoria optimizada: {optimized_memory:.1f} MB\") print(f\"   Ahorro de memoria: {savings:.1f}%\")  # 5. Revisar datos faltantes antes de joins print(\"\\nDATOS FALTANTES ANTES DE JOINS:\") print(\"Trips (top 5 columnas con m\u00e1s nulos):\") trips_nulls = trips.isna().sum().sort_values(ascending=False).head()  # m\u00e9todo para detectar valores nulos, sumar y ordenar print(trips_nulls)  print(\"\\nZones:\") zones_nulls = zones.isna().sum()  # revisar si hay valores faltantes en lookup table print(zones_nulls)  print(\"\\nCalendar:\") calendar_nulls = calendar.isna().sum()  # verificar integridad del calendario de eventos print(calendar_nulls)  # An\u00e1lisis de calidad de datos print(\"\\nAN\u00c1LISIS DE CALIDAD:\") total_trips = len(trips) print(f\"   Total de viajes: {total_trips:,}\") print(f\"   Viajes sin pickup location: {trips['pulocationid'].isna().sum():,}\") print(f\"   Viajes sin dropoff location: {trips['dolocationid'].isna().sum():,}\") print(f\"   Viajes sin passenger_count: {trips['passenger_count'].isna().sum():,}\")  # Estrategias de limpieza recomendadas print(\"\\nESTRATEGIAS DE LIMPIEZA:\") print(\"   Ubicaciones nulas: Eliminar (cr\u00edtico para joins)\") print(\"   Passenger_count nulos: Rellenar con valor t\u00edpico (1)\") print(\"   Tarifas nulas: Revisar caso por caso\") <pre>Normalizando nombres de columnas...\n   Trips columnas: ['vendorid', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'ratecodeid', 'store_and_fwd_flag', 'pulocationid', 'dolocationid', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'airport_fee']\n   Zones columnas: ['locationid', 'borough', 'zone', 'service_zone']\n   Columna pickup_date creada\n   Rango de fechas: 2008-12-31 a 2023-02-01\n\nVERIFICACI\u00d3N DE TIPOS PARA JOINS:\n   trips['pulocationid'] tipo: int64\n   zones['locationid'] tipo: int64\n   trips['pickup_date'] tipo: &lt;class 'datetime.date'&gt;\n   calendar['date'] tipo: &lt;class 'datetime.date'&gt;\n\nOPTIMIZACI\u00d3N PARA DATASETS GRANDES:\n   Memoria inicial: 705.5 MB\n   Optimizando tipos de datos para 3M+ registros...\n   Limpiando valores nulos antes de optimizaci\u00f3n...\n   Registros despu\u00e9s de limpieza: 3,066,766\n   Memoria optimizada: 649.9 MB\n   Ahorro de memoria: 7.9%\n\nDATOS FALTANTES ANTES DE JOINS:\nTrips (top 5 columnas con m\u00e1s nulos):\nairport_fee             71743\ncongestion_surcharge    71743\nratecodeid              71743\nstore_and_fwd_flag      71743\nvendorid                    0\ndtype: int64\n\nZones:\nlocationid      0\nborough         1\nzone            1\nservice_zone    2\ndtype: int64\n\nCalendar:\ndate       0\nname       0\nspecial    0\ndtype: int64\n\nAN\u00c1LISIS DE CALIDAD:\n   Total de viajes: 3,066,766\n   Viajes sin pickup location: 0\n   Viajes sin dropoff location: 0\n   Viajes sin passenger_count: 0\n\nESTRATEGIAS DE LIMPIEZA:\n   Ubicaciones nulas: Eliminar (cr\u00edtico para joins)\n   Passenger_count nulos: Rellenar con valor t\u00edpico (1)\n   Tarifas nulas: Revisar caso por caso\n</pre> In\u00a0[55]: Copied! <pre>#Eliminar los datos nulos (71000 de 3000000 es razonable)\ntrips = trips.dropna()\nprint(f\"Registros en trips: {len(trips)}\")\n\n# Rellenar valores nulos en las columnas de zones con 'Desconocido'\nzones['borough'] = zones['borough'].fillna('Desconocido')\nzones['zone'] = zones['zone'].fillna('Desconocido')\nzones['service_zone'] = zones['service_zone'].fillna('Desconocido')\n</pre> #Eliminar los datos nulos (71000 de 3000000 es razonable) trips = trips.dropna() print(f\"Registros en trips: {len(trips)}\")  # Rellenar valores nulos en las columnas de zones con 'Desconocido' zones['borough'] = zones['borough'].fillna('Desconocido') zones['zone'] = zones['zone'].fillna('Desconocido') zones['service_zone'] = zones['service_zone'].fillna('Desconocido') <pre>Registros en trips: 2995023\n</pre> In\u00a0[56]: Copied! <pre># === PRIMER JOIN: TRIPS + ZONES ===\n\n# 1. Hacer join de trips con zones para obtener informaci\u00f3n geogr\u00e1fica\nprint(\"Realizando join: trips + zones...\")\ntrips_with_zones = trips.merge(zones,   # m\u00e9todo principal para unir DataFrames\n                                left_on='pulocationid',   # columna de trips que contiene ID de zona de pickup\n                                right_on='locationid',  # columna de zones que contiene ID correspondiente\n                                how='left')       # tipo de join que mantiene todos los trips\n\nprint(f\"   Registros antes del join: {len(trips)}\")\nprint(f\"   Registros despu\u00e9s del join: {len(trips_with_zones)}\")\nprint(f\"   Nuevas columnas a\u00f1adidas: {[col for col in trips_with_zones.columns if col not in trips.columns]}\")\n\n# 2. Verificar el resultado del join\nprint(\"\\nVERIFICACI\u00d3N DEL JOIN:\")\nprint(\"Conteo por Borough:\")\nprint(trips_with_zones['borough'].value_counts())\n\n# 3. Verificar si hay valores nulos despu\u00e9s del join\nnull_after_join = trips_with_zones['borough'].isna().sum()  # contar nulos en columna borough\nprint(f\"\\nViajes sin borough asignado: {null_after_join}\")\n\nif null_after_join &gt; 0:\n    print(\"   Algunos viajes no encontraron su zona correspondiente\")\n    print(\"   LocationIDs problem\u00e1ticos:\")\n    problematic_ids = trips_with_zones[trips_with_zones['borough'].isna()]['pulocationid'].unique()  # filtrar filas con nulos\n    print(f\"   {problematic_ids}\")\n\n# 4. Mostrar muestra del resultado\nprint(\"\\nMUESTRA DEL DATASET INTEGRADO:\")\nprint(trips_with_zones[['pulocationid', 'borough', 'zone', 'trip_distance', 'total_amount']].head())\n</pre> # === PRIMER JOIN: TRIPS + ZONES ===  # 1. Hacer join de trips con zones para obtener informaci\u00f3n geogr\u00e1fica print(\"Realizando join: trips + zones...\") trips_with_zones = trips.merge(zones,   # m\u00e9todo principal para unir DataFrames                                 left_on='pulocationid',   # columna de trips que contiene ID de zona de pickup                                 right_on='locationid',  # columna de zones que contiene ID correspondiente                                 how='left')       # tipo de join que mantiene todos los trips  print(f\"   Registros antes del join: {len(trips)}\") print(f\"   Registros despu\u00e9s del join: {len(trips_with_zones)}\") print(f\"   Nuevas columnas a\u00f1adidas: {[col for col in trips_with_zones.columns if col not in trips.columns]}\")  # 2. Verificar el resultado del join print(\"\\nVERIFICACI\u00d3N DEL JOIN:\") print(\"Conteo por Borough:\") print(trips_with_zones['borough'].value_counts())  # 3. Verificar si hay valores nulos despu\u00e9s del join null_after_join = trips_with_zones['borough'].isna().sum()  # contar nulos en columna borough print(f\"\\nViajes sin borough asignado: {null_after_join}\")  if null_after_join &gt; 0:     print(\"   Algunos viajes no encontraron su zona correspondiente\")     print(\"   LocationIDs problem\u00e1ticos:\")     problematic_ids = trips_with_zones[trips_with_zones['borough'].isna()]['pulocationid'].unique()  # filtrar filas con nulos     print(f\"   {problematic_ids}\")  # 4. Mostrar muestra del resultado print(\"\\nMUESTRA DEL DATASET INTEGRADO:\") print(trips_with_zones[['pulocationid', 'borough', 'zone', 'trip_distance', 'total_amount']].head()) <pre>Realizando join: trips + zones...\n   Registros antes del join: 2995023\n   Registros despu\u00e9s del join: 2995023\n   Nuevas columnas a\u00f1adidas: ['locationid', 'borough', 'zone', 'service_zone']\n\nVERIFICACI\u00d3N DEL JOIN:\nConteo por Borough:\nborough\nManhattan        2648320\nQueens            285126\nUnknown            39788\nBrooklyn           15478\nBronx               3931\nDesconocido         1632\nEWR                  409\nStaten Island        339\nName: count, dtype: int64\n\nViajes sin borough asignado: 0\n\nMUESTRA DEL DATASET INTEGRADO:\n   pulocationid    borough               zone  trip_distance  total_amount\n0           161  Manhattan     Midtown Center           0.97         14.30\n1            43  Manhattan       Central Park           1.10         16.90\n2            48  Manhattan       Clinton East           2.51         34.90\n3           138     Queens  LaGuardia Airport           1.90         20.85\n4           107  Manhattan           Gramercy           1.43         19.68\n</pre> In\u00a0[57]: Copied! <pre># === SEGUNDO JOIN: TRIPS_ZONES + CALENDAR ===\n\n# 1. Hacer join con datos de calendario\nprint(\"Realizando join: trips_zones + calendar...\")\ntrips_complete = trips_with_zones.merge(calendar,   # mismo m\u00e9todo de join que antes\n                                         left_on='pickup_date',   # columna de fecha que creamos en trips\n                                         right_on='date',  # columna de fecha en calendar\n                                         how='left')       # tipo que mantiene todos los trips aunque no haya evento especial\n\nprint(f\"   Registros antes del join: {len(trips_with_zones)}\")\nprint(f\"   Registros despu\u00e9s del join: {len(trips_complete)}\")\n\n# 2. Crear flag de evento especial\ntrips_complete['is_special_day'] = trips_complete['special'].fillna('False')  # m\u00e9todo para rellenar nulos con valor por defecto\n\nprint(\"\\nDISTRIBUCI\u00d3N DE D\u00cdAS ESPECIALES:\")\nprint(trips_complete['is_special_day'].value_counts())\nprint(\"\\nEjemplos de eventos especiales:\")\nspecial_days = trips_complete[trips_complete['is_special_day'] == True]\nif len(special_days) &gt; 0:\n    print(special_days[['pickup_date', 'special', 'borough']].drop_duplicates())\nelse:\n    print(\"   No hay eventos especiales en este per\u00edodo\")\n\n# 3. Mostrar dataset final integrado\nprint(\"\\nDATASET FINAL INTEGRADO:\")\nprint(f\"   Total registros: {len(trips_complete)}\")\nprint(f\"   Total columnas: {len(trips_complete.columns)}\")\nprint(f\"   Columnas principales: {['borough', 'zone', 'is_special_day', 'trip_distance', 'total_amount']}\")\n\n# 4. Verificar integridad de los datos finales\nprint(\"\\nVERIFICACI\u00d3N FINAL:\")\nprint(\"Datos faltantes por columna clave:\")\nkey_columns = ['borough', 'zone', 'trip_distance', 'total_amount', 'is_special_day']\nfor col in key_columns:\n    missing = trips_complete[col].isna().sum()  # verificar nulos en cada columna clave final\n    print(f\"   {col}: {missing} nulos\")\n</pre> # === SEGUNDO JOIN: TRIPS_ZONES + CALENDAR ===  # 1. Hacer join con datos de calendario print(\"Realizando join: trips_zones + calendar...\") trips_complete = trips_with_zones.merge(calendar,   # mismo m\u00e9todo de join que antes                                          left_on='pickup_date',   # columna de fecha que creamos en trips                                          right_on='date',  # columna de fecha en calendar                                          how='left')       # tipo que mantiene todos los trips aunque no haya evento especial  print(f\"   Registros antes del join: {len(trips_with_zones)}\") print(f\"   Registros despu\u00e9s del join: {len(trips_complete)}\")  # 2. Crear flag de evento especial trips_complete['is_special_day'] = trips_complete['special'].fillna('False')  # m\u00e9todo para rellenar nulos con valor por defecto  print(\"\\nDISTRIBUCI\u00d3N DE D\u00cdAS ESPECIALES:\") print(trips_complete['is_special_day'].value_counts()) print(\"\\nEjemplos de eventos especiales:\") special_days = trips_complete[trips_complete['is_special_day'] == True] if len(special_days) &gt; 0:     print(special_days[['pickup_date', 'special', 'borough']].drop_duplicates()) else:     print(\"   No hay eventos especiales en este per\u00edodo\")  # 3. Mostrar dataset final integrado print(\"\\nDATASET FINAL INTEGRADO:\") print(f\"   Total registros: {len(trips_complete)}\") print(f\"   Total columnas: {len(trips_complete.columns)}\") print(f\"   Columnas principales: {['borough', 'zone', 'is_special_day', 'trip_distance', 'total_amount']}\")  # 4. Verificar integridad de los datos finales print(\"\\nVERIFICACI\u00d3N FINAL:\") print(\"Datos faltantes por columna clave:\") key_columns = ['borough', 'zone', 'trip_distance', 'total_amount', 'is_special_day'] for col in key_columns:     missing = trips_complete[col].isna().sum()  # verificar nulos en cada columna clave final     print(f\"   {col}: {missing} nulos\") <pre>Realizando join: trips_zones + calendar...\n   Registros antes del join: 2995023\n   Registros despu\u00e9s del join: 2995023\n\nDISTRIBUCI\u00d3N DE D\u00cdAS ESPECIALES:\nis_special_day\nFalse    2995023\nName: count, dtype: int64\n\nEjemplos de eventos especiales:\n   No hay eventos especiales en este per\u00edodo\n\nDATASET FINAL INTEGRADO:\n   Total registros: 2995023\n   Total columnas: 28\n   Columnas principales: ['borough', 'zone', 'is_special_day', 'trip_distance', 'total_amount']\n\nVERIFICACI\u00d3N FINAL:\nDatos faltantes por columna clave:\n   borough: 0 nulos\n   zone: 0 nulos\n   trip_distance: 0 nulos\n   total_amount: 0 nulos\n   is_special_day: 0 nulos\n</pre> In\u00a0[58]: Copied! <pre># === AN\u00c1LISIS AGREGADO POR BOROUGH ===\n\n# 1. An\u00e1lisis b\u00e1sico por borough (con dataset grande)\nprint(\"An\u00e1lisis por Borough (procesando datos grandes)...\")\nborough_analysis = trips_complete.groupby(by='borough').agg({   # m\u00e9todo para agrupar datos, por qu\u00e9 columna geogr\u00e1fica?\n    'pulocationid': 'count',  # funci\u00f3n para contar n\u00famero de registros/viajes\n    'trip_distance': ['mean', 'std', 'median'],  # funci\u00f3n para promedio + desviaci\u00f3n + mediana\n    'total_amount': ['mean', 'std', 'median'],   # mismas estad\u00edsticas para tarifas\n    'fare_amount': 'mean',     # solo promedio de tarifa base\n    'tip_amount': ['mean', 'median'],  # estad\u00edsticas de propinas\n    'passenger_count': 'mean'  # funci\u00f3n para promedio de pasajeros\n}).round(2)\n\n# Aplanar columnas multi-nivel\nborough_analysis.columns = ['num_trips', 'avg_distance', 'std_distance', 'median_distance',\n                           'avg_total', 'std_total', 'median_total', 'avg_fare', \n                           'avg_tip', 'median_tip', 'avg_passengers']\n\n# Ordenar por n\u00famero de viajes\nborough_analysis = borough_analysis.sort_values(by='num_trips', ascending=False)  # m\u00e9todo para ordenar DataFrame por una columna espec\u00edfica\n\nprint(\"\\nAN\u00c1LISIS COMPLETO POR BOROUGH:\")\nprint(borough_analysis)\n\n# 2. Calcular m\u00e9tricas adicionales empresariales\nborough_analysis['revenue_per_km'] = (borough_analysis['avg_total'] / \n                                     borough_analysis['avg_distance']).round(2)\nborough_analysis['tip_rate'] = (borough_analysis['avg_tip'] / \n                               borough_analysis['avg_fare'] * 100).round(1)\nborough_analysis['market_share'] = (borough_analysis['num_trips'] / \n                                  borough_analysis['num_trips'].sum() * 100).round(1)\n\nprint(\"\\nAN\u00c1LISIS CON M\u00c9TRICAS EMPRESARIALES:\")\nprint(borough_analysis[['num_trips', 'market_share', 'revenue_per_km', 'tip_rate']])\n\n# 3. Encontrar insights\nprint(\"\\nINSIGHTS PRINCIPALES:\")\nprint(f\"   Borough con m\u00e1s viajes: {borough_analysis.index[0]}\")\nprint(f\"   Borough con viajes m\u00e1s largos: {borough_analysis['avg_distance'].idxmax()}\")\nprint(\"   Borough con tarifas m\u00e1s altas:\", borough_analysis.sort_values(by='avg_total', ascending=False).head(3).index.tolist())\nprint(f\"   Mejor revenue por km: {borough_analysis['revenue_per_km'].idxmax()}\")\n</pre> # === AN\u00c1LISIS AGREGADO POR BOROUGH ===  # 1. An\u00e1lisis b\u00e1sico por borough (con dataset grande) print(\"An\u00e1lisis por Borough (procesando datos grandes)...\") borough_analysis = trips_complete.groupby(by='borough').agg({   # m\u00e9todo para agrupar datos, por qu\u00e9 columna geogr\u00e1fica?     'pulocationid': 'count',  # funci\u00f3n para contar n\u00famero de registros/viajes     'trip_distance': ['mean', 'std', 'median'],  # funci\u00f3n para promedio + desviaci\u00f3n + mediana     'total_amount': ['mean', 'std', 'median'],   # mismas estad\u00edsticas para tarifas     'fare_amount': 'mean',     # solo promedio de tarifa base     'tip_amount': ['mean', 'median'],  # estad\u00edsticas de propinas     'passenger_count': 'mean'  # funci\u00f3n para promedio de pasajeros }).round(2)  # Aplanar columnas multi-nivel borough_analysis.columns = ['num_trips', 'avg_distance', 'std_distance', 'median_distance',                            'avg_total', 'std_total', 'median_total', 'avg_fare',                             'avg_tip', 'median_tip', 'avg_passengers']  # Ordenar por n\u00famero de viajes borough_analysis = borough_analysis.sort_values(by='num_trips', ascending=False)  # m\u00e9todo para ordenar DataFrame por una columna espec\u00edfica  print(\"\\nAN\u00c1LISIS COMPLETO POR BOROUGH:\") print(borough_analysis)  # 2. Calcular m\u00e9tricas adicionales empresariales borough_analysis['revenue_per_km'] = (borough_analysis['avg_total'] /                                       borough_analysis['avg_distance']).round(2) borough_analysis['tip_rate'] = (borough_analysis['avg_tip'] /                                 borough_analysis['avg_fare'] * 100).round(1) borough_analysis['market_share'] = (borough_analysis['num_trips'] /                                    borough_analysis['num_trips'].sum() * 100).round(1)  print(\"\\nAN\u00c1LISIS CON M\u00c9TRICAS EMPRESARIALES:\") print(borough_analysis[['num_trips', 'market_share', 'revenue_per_km', 'tip_rate']])  # 3. Encontrar insights print(\"\\nINSIGHTS PRINCIPALES:\") print(f\"   Borough con m\u00e1s viajes: {borough_analysis.index[0]}\") print(f\"   Borough con viajes m\u00e1s largos: {borough_analysis['avg_distance'].idxmax()}\") print(\"   Borough con tarifas m\u00e1s altas:\", borough_analysis.sort_values(by='avg_total', ascending=False).head(3).index.tolist()) print(f\"   Mejor revenue por km: {borough_analysis['revenue_per_km'].idxmax()}\") <pre>An\u00e1lisis por Borough (procesando datos grandes)...\n\nAN\u00c1LISIS COMPLETO POR BOROUGH:\n               num_trips  avg_distance  std_distance  median_distance  \\\nborough                                                                 \nManhattan        2648320          2.41         40.73             1.61   \nQueens            285126         12.32         12.31            11.27   \nUnknown            39788          7.59        145.56             2.63   \nBrooklyn           15478          4.48          4.94             3.03   \nBronx               3931          5.19          6.40             2.90   \nDesconocido         1632          2.34          7.56             0.00   \nEWR                  409          1.60          5.68             0.00   \nStaten Island        339         11.34         10.24            14.80   \n\n               avg_total  std_total  median_total  avg_fare  avg_tip  \\\nborough                                                                \nManhattan          22.35      14.47         19.00     14.64     2.86   \nQueens             67.35      33.65         70.45     50.04     7.85   \nUnknown            38.12      30.48         25.30     26.47     4.82   \nBrooklyn           32.02      23.17         27.00     26.20     2.62   \nBronx              34.15      33.83         29.00     30.01     0.65   \nDesconocido       108.20      96.90         91.20     95.08     9.93   \nEWR               104.34      62.82        118.50     87.99    12.43   \nStaten Island      62.44      45.04         67.80     48.68     1.29   \n\n               median_tip  avg_passengers  \nborough                                    \nManhattan            2.66            1.36  \nQueens               8.18            1.39  \nUnknown              3.14            1.35  \nBrooklyn             0.00            1.26  \nBronx                0.00            1.10  \nDesconocido          0.01            1.43  \nEWR                 10.00            1.58  \nStaten Island        0.00            1.13  \n\nAN\u00c1LISIS CON M\u00c9TRICAS EMPRESARIALES:\n               num_trips  market_share  revenue_per_km  tip_rate\nborough                                                         \nManhattan        2648320          88.4            9.27      19.5\nQueens            285126           9.5            5.47      15.7\nUnknown            39788           1.3            5.02      18.2\nBrooklyn           15478           0.5            7.15      10.0\nBronx               3931           0.1            6.58       2.2\nDesconocido         1632           0.1           46.24      10.4\nEWR                  409           0.0           65.21      14.1\nStaten Island        339           0.0            5.51       2.6\n\nINSIGHTS PRINCIPALES:\n   Borough con m\u00e1s viajes: Manhattan\n   Borough con viajes m\u00e1s largos: Queens\n   Borough con tarifas m\u00e1s altas: ['Desconocido', 'EWR', 'Queens']\n   Mejor revenue por km: EWR\n</pre> In\u00a0[59]: Copied! <pre># === AN\u00c1LISIS COMPARATIVO: D\u00cdAS NORMALES VS ESPECIALES ===\nprint(trips_complete.head())\n# 1. An\u00e1lisis por borough y tipo de d\u00eda\nprint(\"An\u00e1lisis: Borough + D\u00eda Especial...\")\nborough_day_analysis = trips_complete.groupby(by=['borough', 'is_special_day']).agg({  # agrupar por DOS columnas: geograf\u00eda y tipo de d\u00eda\n    'pulocationid': 'count',  # funci\u00f3n para contar viajes\n    'trip_distance': 'mean',  # funci\u00f3n para promedio de distancia\n    'total_amount': 'mean'    # funci\u00f3n para promedio de tarifa\n}).round(2)\n\nborough_day_analysis.columns = ['num_trips', 'avg_distance', 'avg_total']\n\nprint(\"\\nAN\u00c1LISIS BOROUGH + D\u00cdA ESPECIAL:\")\nprint(borough_day_analysis)\n\n# 2. Comparar d\u00edas normales vs especiales\nprint(\"\\nCOMPARACI\u00d3N D\u00cdAS NORMALES VS ESPECIALES:\")\n\n# Pivotear para comparar f\u00e1cilmente\ncomparison = trips_complete.groupby(by='is_special_day').agg({  # agrupar solo por tipo de d\u00eda para comparaci\u00f3n general\n    'trip_distance': 'mean',    # promedio de distancia por tipo de d\u00eda\n    'total_amount': 'mean',     # promedio de tarifa por tipo de d\u00eda\n    'pulocationid': 'count'     # conteo de viajes por tipo de d\u00eda\n}).round(2)\n\n# Renombrar \u00edndices seg\u00fan los valores \u00fanicos encontrados\nunique_day_types = comparison.index.tolist()\nif len(unique_day_types) == 2:\n    comparison.index = ['D\u00eda Normal', 'D\u00eda Especial']\nelif len(unique_day_types) == 1:\n    if unique_day_types[0] in ['False', False]:\n        comparison.index = ['D\u00eda Normal']\n    else:\n        comparison.index = ['D\u00eda Especial']\n\ncomparison.columns = ['Avg Distance', 'Avg Amount', 'Num Trips']\n\nprint(comparison)\n\n# 3. Calcular diferencias porcentuales\nif len(comparison) &gt; 1:\n    # Hay tanto d\u00edas normales como especiales\n    if 'D\u00eda Normal' in comparison.index and 'D\u00eda Especial' in comparison.index:\n        normal_day = comparison.loc['D\u00eda Normal']\n        special_day = comparison.loc['D\u00eda Especial']\n\n        print(\"\\nIMPACTO DE D\u00cdAS ESPECIALES:\")\n        distance_change = ((special_day['Avg Distance'] - normal_day['Avg Distance']) / normal_day['Avg Distance'] * 100)\n        amount_change = ((special_day['Avg Amount'] - normal_day['Avg Amount']) / normal_day['Avg Amount'] * 100)\n\n        print(f\"   Cambio en distancia promedio: {distance_change:+.1f}%\")\n        print(f\"   Cambio en tarifa promedio: {amount_change:+.1f}%\")\n    else:\n        print(\"\\nINFORMACI\u00d3N DE D\u00cdAS:\")\n        for idx, row in comparison.iterrows():\n            print(f\"   {idx}: {row['Num Trips']:,} viajes, ${row['Avg Amount']:.2f} promedio\")\nelse:\n    print(f\"\\nSOLO HAY {comparison.index[0]}:\")\n    print(f\"   Viajes: {comparison.iloc[0]['Num Trips']:,}\")\n    print(f\"   Distancia promedio: {comparison.iloc[0]['Avg Distance']:.2f} millas\")\n    print(f\"   Tarifa promedio: ${comparison.iloc[0]['Avg Amount']:.2f}\")\n    print(\"   No hay datos de d\u00edas especiales para comparar en este per\u00edodo\")\n</pre> # === AN\u00c1LISIS COMPARATIVO: D\u00cdAS NORMALES VS ESPECIALES === print(trips_complete.head()) # 1. An\u00e1lisis por borough y tipo de d\u00eda print(\"An\u00e1lisis: Borough + D\u00eda Especial...\") borough_day_analysis = trips_complete.groupby(by=['borough', 'is_special_day']).agg({  # agrupar por DOS columnas: geograf\u00eda y tipo de d\u00eda     'pulocationid': 'count',  # funci\u00f3n para contar viajes     'trip_distance': 'mean',  # funci\u00f3n para promedio de distancia     'total_amount': 'mean'    # funci\u00f3n para promedio de tarifa }).round(2)  borough_day_analysis.columns = ['num_trips', 'avg_distance', 'avg_total']  print(\"\\nAN\u00c1LISIS BOROUGH + D\u00cdA ESPECIAL:\") print(borough_day_analysis)  # 2. Comparar d\u00edas normales vs especiales print(\"\\nCOMPARACI\u00d3N D\u00cdAS NORMALES VS ESPECIALES:\")  # Pivotear para comparar f\u00e1cilmente comparison = trips_complete.groupby(by='is_special_day').agg({  # agrupar solo por tipo de d\u00eda para comparaci\u00f3n general     'trip_distance': 'mean',    # promedio de distancia por tipo de d\u00eda     'total_amount': 'mean',     # promedio de tarifa por tipo de d\u00eda     'pulocationid': 'count'     # conteo de viajes por tipo de d\u00eda }).round(2)  # Renombrar \u00edndices seg\u00fan los valores \u00fanicos encontrados unique_day_types = comparison.index.tolist() if len(unique_day_types) == 2:     comparison.index = ['D\u00eda Normal', 'D\u00eda Especial'] elif len(unique_day_types) == 1:     if unique_day_types[0] in ['False', False]:         comparison.index = ['D\u00eda Normal']     else:         comparison.index = ['D\u00eda Especial']  comparison.columns = ['Avg Distance', 'Avg Amount', 'Num Trips']  print(comparison)  # 3. Calcular diferencias porcentuales if len(comparison) &gt; 1:     # Hay tanto d\u00edas normales como especiales     if 'D\u00eda Normal' in comparison.index and 'D\u00eda Especial' in comparison.index:         normal_day = comparison.loc['D\u00eda Normal']         special_day = comparison.loc['D\u00eda Especial']          print(\"\\nIMPACTO DE D\u00cdAS ESPECIALES:\")         distance_change = ((special_day['Avg Distance'] - normal_day['Avg Distance']) / normal_day['Avg Distance'] * 100)         amount_change = ((special_day['Avg Amount'] - normal_day['Avg Amount']) / normal_day['Avg Amount'] * 100)          print(f\"   Cambio en distancia promedio: {distance_change:+.1f}%\")         print(f\"   Cambio en tarifa promedio: {amount_change:+.1f}%\")     else:         print(\"\\nINFORMACI\u00d3N DE D\u00cdAS:\")         for idx, row in comparison.iterrows():             print(f\"   {idx}: {row['Num Trips']:,} viajes, ${row['Avg Amount']:.2f} promedio\") else:     print(f\"\\nSOLO HAY {comparison.index[0]}:\")     print(f\"   Viajes: {comparison.iloc[0]['Num Trips']:,}\")     print(f\"   Distancia promedio: {comparison.iloc[0]['Avg Distance']:.2f} millas\")     print(f\"   Tarifa promedio: ${comparison.iloc[0]['Avg Amount']:.2f}\")     print(\"   No hay datos de d\u00edas especiales para comparar en este per\u00edodo\") <pre>   vendorid tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n0         2  2023-01-01 00:32:10   2023-01-01 00:40:36                1   \n1         2  2023-01-01 00:55:08   2023-01-01 01:01:27                1   \n2         2  2023-01-01 00:25:04   2023-01-01 00:37:49                1   \n3         1  2023-01-01 00:03:48   2023-01-01 00:13:25                0   \n4         2  2023-01-01 00:10:29   2023-01-01 00:21:19                1   \n\n   trip_distance  ratecodeid store_and_fwd_flag  pulocationid  dolocationid  \\\n0           0.97         1.0                  N           161           141   \n1           1.10         1.0                  N            43           237   \n2           2.51         1.0                  N            48           238   \n3           1.90         1.0                  N           138             7   \n4           1.43         1.0                  N           107            79   \n\n   payment_type  ...  airport_fee  pickup_date  locationid    borough  \\\n0             2  ...         0.00   2023-01-01         161  Manhattan   \n1             1  ...         0.00   2023-01-01          43  Manhattan   \n2             1  ...         0.00   2023-01-01          48  Manhattan   \n3             1  ...         1.25   2023-01-01         138     Queens   \n4             1  ...         0.00   2023-01-01         107  Manhattan   \n\n                zone  service_zone  date  name  special is_special_day  \n0     Midtown Center   Yellow Zone   NaN   NaN      NaN          False  \n1       Central Park   Yellow Zone   NaN   NaN      NaN          False  \n2       Clinton East   Yellow Zone   NaN   NaN      NaN          False  \n3  LaGuardia Airport      Airports   NaN   NaN      NaN          False  \n4           Gramercy   Yellow Zone   NaN   NaN      NaN          False  \n\n[5 rows x 28 columns]\nAn\u00e1lisis: Borough + D\u00eda Especial...\n\nAN\u00c1LISIS BOROUGH + D\u00cdA ESPECIAL:\n                              num_trips  avg_distance  avg_total\nborough       is_special_day                                    \nBronx         False                3931          5.19      34.15\nBrooklyn      False               15478          4.48      32.02\nDesconocido   False                1632          2.34     108.20\nEWR           False                 409          1.60     104.34\nManhattan     False             2648320          2.41      22.35\nQueens        False              285126         12.32      67.35\nStaten Island False                 339         11.34      62.44\nUnknown       False               39788          7.59      38.12\n\nCOMPARACI\u00d3N D\u00cdAS NORMALES VS ESPECIALES:\n            Avg Distance  Avg Amount  Num Trips\nD\u00eda Normal          3.44       26.97    2995023\n\nSOLO HAY D\u00eda Normal:\n   Viajes: 2,995,023.0\n   Distancia promedio: 3.44 millas\n   Tarifa promedio: $26.97\n   No hay datos de d\u00edas especiales para comparar en este per\u00edodo\n</pre> In\u00a0[60]: Copied! <pre># === T\u00c9CNICAS PARA TRABAJAR CON DATASETS GRANDES ===\n\n# 1. Sampling estrat\u00e9gico para visualizaciones\nprint(\"\u26a1 Aplicando t\u00e9cnicas para datasets grandes...\")\n\n# Si el dataset es muy grande, usar muestra para visualizaciones\nif len(trips_complete) &gt; 50000:\n    print(f\"Dataset grande detectado: {len(trips_complete):,} registros\")\n    print(\"Creando muestra estratificada para visualizaciones...\")\n\n    # Muestra proporcional por borough\n    sample_size = min(10000, len(trips_complete) // 10)\n    trips_sample = trips_complete.sample(n=sample_size, random_state=42)  # m\u00e9todo para tomar muestra aleatoria de n registros\n\n    print(f\"Muestra creada: {len(trips_sample):,} registros ({len(trips_sample)/len(trips_complete)*100:.1f}%)\")\nelse:\n    trips_sample = trips_complete\n    print(\"Dataset peque\u00f1o, usando datos completos para visualizaci\u00f3n\")\n\n# 2. An\u00e1lisis de performance de joins\nprint(\"\\nAN\u00c1LISIS DE PERFORMANCE:\")\njoin_stats = {\n    'total_trips': len(trips),\n    'matched_zones': (trips_complete['borough'].notna()).sum(),\n    'match_rate': (trips_complete['borough'].notna().sum() / len(trips) * 100),\n    'unique_zones_used': trips_complete['zone'].nunique(),\n    'total_zones_available': len(zones),\n    'zone_coverage': (trips_complete['zone'].nunique() / len(zones) * 100)\n}\n\nfor key, value in join_stats.items():\n    if 'rate' in key or 'coverage' in key:\n        print(f\"   {key}: {value:.1f}%\")\n    else:\n        print(f\"   {key}: {value:,}\")\n\n# 3. An\u00e1lisis temporal avanzado (solo si hay suficientes datos)\nif len(trips_complete) &gt; 1000:\n    print(\"\\nAN\u00c1LISIS TEMPORAL AVANZADO:\")\n\n    # An\u00e1lisis por hora del d\u00eda\n    trips_complete['pickup_hour'] = trips_complete['tpep_pickup_datetime'].dt.hour  # extraer hora de la fecha/hora\n    hourly_analysis = trips_complete.groupby(by='pickup_hour').agg({  # agrupar por hora del d\u00eda\n        'pulocationid': 'count',     # contar viajes por hora\n        'total_amount': 'mean',      # tarifa promedio por hora\n        'trip_distance': 'mean'      # distancia promedio por hora\n    }).round(2)\n\n    hourly_analysis.columns = ['trips_count', 'avg_amount', 'avg_distance']\n\n    print(\"Horas pico por n\u00famero de viajes:\")\n    peak_hours = hourly_analysis.sort_values(by='trips_count', ascending=False).head(3)  # ordenar por m\u00e1s viajes, tomar top 3\n    for hour, stats in peak_hours.iterrows():\n        print(f\"      {hour:02d}:00 - {stats['trips_count']:,} viajes\")\n</pre> # === T\u00c9CNICAS PARA TRABAJAR CON DATASETS GRANDES ===  # 1. Sampling estrat\u00e9gico para visualizaciones print(\"\u26a1 Aplicando t\u00e9cnicas para datasets grandes...\")  # Si el dataset es muy grande, usar muestra para visualizaciones if len(trips_complete) &gt; 50000:     print(f\"Dataset grande detectado: {len(trips_complete):,} registros\")     print(\"Creando muestra estratificada para visualizaciones...\")      # Muestra proporcional por borough     sample_size = min(10000, len(trips_complete) // 10)     trips_sample = trips_complete.sample(n=sample_size, random_state=42)  # m\u00e9todo para tomar muestra aleatoria de n registros      print(f\"Muestra creada: {len(trips_sample):,} registros ({len(trips_sample)/len(trips_complete)*100:.1f}%)\") else:     trips_sample = trips_complete     print(\"Dataset peque\u00f1o, usando datos completos para visualizaci\u00f3n\")  # 2. An\u00e1lisis de performance de joins print(\"\\nAN\u00c1LISIS DE PERFORMANCE:\") join_stats = {     'total_trips': len(trips),     'matched_zones': (trips_complete['borough'].notna()).sum(),     'match_rate': (trips_complete['borough'].notna().sum() / len(trips) * 100),     'unique_zones_used': trips_complete['zone'].nunique(),     'total_zones_available': len(zones),     'zone_coverage': (trips_complete['zone'].nunique() / len(zones) * 100) }  for key, value in join_stats.items():     if 'rate' in key or 'coverage' in key:         print(f\"   {key}: {value:.1f}%\")     else:         print(f\"   {key}: {value:,}\")  # 3. An\u00e1lisis temporal avanzado (solo si hay suficientes datos) if len(trips_complete) &gt; 1000:     print(\"\\nAN\u00c1LISIS TEMPORAL AVANZADO:\")      # An\u00e1lisis por hora del d\u00eda     trips_complete['pickup_hour'] = trips_complete['tpep_pickup_datetime'].dt.hour  # extraer hora de la fecha/hora     hourly_analysis = trips_complete.groupby(by='pickup_hour').agg({  # agrupar por hora del d\u00eda         'pulocationid': 'count',     # contar viajes por hora         'total_amount': 'mean',      # tarifa promedio por hora         'trip_distance': 'mean'      # distancia promedio por hora     }).round(2)      hourly_analysis.columns = ['trips_count', 'avg_amount', 'avg_distance']      print(\"Horas pico por n\u00famero de viajes:\")     peak_hours = hourly_analysis.sort_values(by='trips_count', ascending=False).head(3)  # ordenar por m\u00e1s viajes, tomar top 3     for hour, stats in peak_hours.iterrows():         print(f\"      {hour:02d}:00 - {stats['trips_count']:,} viajes\") <pre>\u26a1 Aplicando t\u00e9cnicas para datasets grandes...\nDataset grande detectado: 2,995,023 registros\nCreando muestra estratificada para visualizaciones...\nMuestra creada: 10,000 registros (0.3%)\n\nAN\u00c1LISIS DE PERFORMANCE:\n   total_trips: 2,995,023\n   matched_zones: 2,995,023\n   match_rate: 100.0%\n   unique_zones_used: 255\n   total_zones_available: 265\n   zone_coverage: 96.2%\n\nAN\u00c1LISIS TEMPORAL AVANZADO:\nHoras pico por n\u00famero de viajes:\n      18:00 - 210,761.0 viajes\n      17:00 - 204,808.0 viajes\n      15:00 - 193,114.0 viajes\n</pre> In\u00a0[61]: Copied! <pre># === AN\u00c1LISIS DE CORRELACIONES NUM\u00c9RICAS ===\n\n# Calcular correlaciones entre variables num\u00e9ricas\nprint(\"Calculando correlaciones entre variables num\u00e9ricas...\")\nnumeric_cols = ['trip_distance', 'total_amount', 'fare_amount', 'tip_amount']\ncorr_matrix = trips_complete[numeric_cols].corr()  # m\u00e9todo para calcular matriz de correlaci\u00f3n\n\nprint(\"\\nMatriz de Correlaci\u00f3n:\")\nprint(corr_matrix.round(3))\n\nprint(\"\\nCorrelaciones m\u00e1s fuertes:\")\ncorr_pairs = []\nfor i in range(len(corr_matrix.columns)):\n    for j in range(i+1, len(corr_matrix.columns)):\n        corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n\ncorr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\nfor var1, var2, corr in corr_pairs[:3]:\n    print(f\"   {var1} vs {var2}: {corr:.3f}\")\n\nprint(\"\\nINTERPRETACI\u00d3N DE CORRELACIONES:\")\nprint(\"   &gt; 0.7: Correlaci\u00f3n fuerte positiva\")\nprint(\"   0.3-0.7: Correlaci\u00f3n moderada positiva\") \nprint(\"   -0.3-0.3: Correlaci\u00f3n d\u00e9bil\")\nprint(\"   &lt; -0.7: Correlaci\u00f3n fuerte negativa\")\n</pre> # === AN\u00c1LISIS DE CORRELACIONES NUM\u00c9RICAS ===  # Calcular correlaciones entre variables num\u00e9ricas print(\"Calculando correlaciones entre variables num\u00e9ricas...\") numeric_cols = ['trip_distance', 'total_amount', 'fare_amount', 'tip_amount'] corr_matrix = trips_complete[numeric_cols].corr()  # m\u00e9todo para calcular matriz de correlaci\u00f3n  print(\"\\nMatriz de Correlaci\u00f3n:\") print(corr_matrix.round(3))  print(\"\\nCorrelaciones m\u00e1s fuertes:\") corr_pairs = [] for i in range(len(corr_matrix.columns)):     for j in range(i+1, len(corr_matrix.columns)):         corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))  corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True) for var1, var2, corr in corr_pairs[:3]:     print(f\"   {var1} vs {var2}: {corr:.3f}\")  print(\"\\nINTERPRETACI\u00d3N DE CORRELACIONES:\") print(\"   &gt; 0.7: Correlaci\u00f3n fuerte positiva\") print(\"   0.3-0.7: Correlaci\u00f3n moderada positiva\")  print(\"   -0.3-0.3: Correlaci\u00f3n d\u00e9bil\") print(\"   &lt; -0.7: Correlaci\u00f3n fuerte negativa\") <pre>Calculando correlaciones entre variables num\u00e9ricas...\n\nMatriz de Correlaci\u00f3n:\n               trip_distance  total_amount  fare_amount  tip_amount\ntrip_distance          1.000         0.094        0.094       0.061\ntotal_amount           0.094         1.000        0.980       0.708\nfare_amount            0.094         0.980        1.000       0.588\ntip_amount             0.061         0.708        0.588       1.000\n\nCorrelaciones m\u00e1s fuertes:\n   total_amount vs fare_amount: 0.980\n   total_amount vs tip_amount: 0.708\n   fare_amount vs tip_amount: 0.588\n\nINTERPRETACI\u00d3N DE CORRELACIONES:\n   &gt; 0.7: Correlaci\u00f3n fuerte positiva\n   0.3-0.7: Correlaci\u00f3n moderada positiva\n   -0.3-0.3: Correlaci\u00f3n d\u00e9bil\n   &lt; -0.7: Correlaci\u00f3n fuerte negativa\n</pre> In\u00a0[62]: Copied! <pre>import prefect\nfrom prefect import task, flow, get_run_logger\nimport pandas as pd\n\nimport os\nos.environ[\"PREFECT_LOGGING_SERVER_ENABLED\"] = \"false\"\n\nprint(\"Prefect instalado y configurado\")\nprint(f\"   Versi\u00f3n: {prefect.__version__}\")\n\n# === TASKS SIMPLES PARA APRENDER PREFECT ===\n\n@task(retries=3, retry_delay_seconds=10, name=\"Cargar Datos\")\ndef cargar_datos(url: str, tipo: str) -&gt; pd.DataFrame:\n    logger = get_run_logger()\n    logger.info(f\"Cargando {tipo} desde: {url}\")\n\n    tipo = tipo.lower().strip()\n    if tipo == \"parquet\":\n        # Requiere pyarrow instalado\n        return pd.read_parquet(url, engine=\"pyarrow\")\n    elif tipo == \"csv\":\n        # Maneja compresi\u00f3n (gzip) autom\u00e1ticamente\n        return pd.read_csv(url, encoding=\"utf-8\", low_memory=False, compression=\"infer\")\n    else:\n        raise ValueError(f\"Tipo no soportado: {tipo} (usa 'csv' o 'parquet')\")\n\n\n@task(name=\"Hacer Join Simple\")\ndef hacer_join_simple(trips: pd.DataFrame, zones: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Task para hacer join b\u00e1sico de trips + zones\"\"\"\n    logger = get_run_logger()\n    logger.info(\"Haciendo join simple...\")\n\n    # Normalizar columnas\n    trips.columns = trips.columns.str.lower()  # convertir a min\u00fasculas\n    zones.columns = zones.columns.str.lower()  # misma transformaci\u00f3n\n\n    # Join b\u00e1sico\n    resultado = trips.merge(zones,   # m\u00e9todo para unir DataFrames\n                             left_on='pickup_date',   # columna de pickup location en trips\n                             right_on='locationid',  # columna de location en zones\n                             how='left')       # tipo de join que mantiene todos los trips\n\n    logger.info(f\"Join completado: {len(resultado)} registros\")\n    return resultado\n\n@task(name=\"An\u00e1lisis R\u00e1pido\")\n\ndef analisis_rapido(data: pd.DataFrame) -&gt; dict:\n    \"\"\"Task para an\u00e1lisis b\u00e1sico\"\"\"\n    logger = get_run_logger()\n    logger.info(\"Haciendo an\u00e1lisis b\u00e1sico...\")\n\n    # Stats simples\n    stats = {\n        'total_registros': len(data),\n        'boroughs': data['borough'].count().head(3).to_dict(),  # m\u00e9todo para contar valores\n        'distancia_promedio': round(data['trip_distance'].mean(), 2),  # m\u00e9todo para promedio\n        'tarifa_promedio': round(data['total_amount'].mean(), 2)  # m\u00e9todo para promedio\n    }\n\n    logger.info(f\"An\u00e1lisis completado: {stats['total_registros']} registros\")\n    return stats\n</pre> import prefect from prefect import task, flow, get_run_logger import pandas as pd  import os os.environ[\"PREFECT_LOGGING_SERVER_ENABLED\"] = \"false\"  print(\"Prefect instalado y configurado\") print(f\"   Versi\u00f3n: {prefect.__version__}\")  # === TASKS SIMPLES PARA APRENDER PREFECT ===  @task(retries=3, retry_delay_seconds=10, name=\"Cargar Datos\") def cargar_datos(url: str, tipo: str) -&gt; pd.DataFrame:     logger = get_run_logger()     logger.info(f\"Cargando {tipo} desde: {url}\")      tipo = tipo.lower().strip()     if tipo == \"parquet\":         # Requiere pyarrow instalado         return pd.read_parquet(url, engine=\"pyarrow\")     elif tipo == \"csv\":         # Maneja compresi\u00f3n (gzip) autom\u00e1ticamente         return pd.read_csv(url, encoding=\"utf-8\", low_memory=False, compression=\"infer\")     else:         raise ValueError(f\"Tipo no soportado: {tipo} (usa 'csv' o 'parquet')\")   @task(name=\"Hacer Join Simple\") def hacer_join_simple(trips: pd.DataFrame, zones: pd.DataFrame) -&gt; pd.DataFrame:     \"\"\"Task para hacer join b\u00e1sico de trips + zones\"\"\"     logger = get_run_logger()     logger.info(\"Haciendo join simple...\")      # Normalizar columnas     trips.columns = trips.columns.str.lower()  # convertir a min\u00fasculas     zones.columns = zones.columns.str.lower()  # misma transformaci\u00f3n      # Join b\u00e1sico     resultado = trips.merge(zones,   # m\u00e9todo para unir DataFrames                              left_on='pickup_date',   # columna de pickup location en trips                              right_on='locationid',  # columna de location en zones                              how='left')       # tipo de join que mantiene todos los trips      logger.info(f\"Join completado: {len(resultado)} registros\")     return resultado  @task(name=\"An\u00e1lisis R\u00e1pido\")  def analisis_rapido(data: pd.DataFrame) -&gt; dict:     \"\"\"Task para an\u00e1lisis b\u00e1sico\"\"\"     logger = get_run_logger()     logger.info(\"Haciendo an\u00e1lisis b\u00e1sico...\")      # Stats simples     stats = {         'total_registros': len(data),         'boroughs': data['borough'].count().head(3).to_dict(),  # m\u00e9todo para contar valores         'distancia_promedio': round(data['trip_distance'].mean(), 2),  # m\u00e9todo para promedio         'tarifa_promedio': round(data['total_amount'].mean(), 2)  # m\u00e9todo para promedio     }      logger.info(f\"An\u00e1lisis completado: {stats['total_registros']} registros\")     return stats <pre>Prefect instalado y configurado\n   Versi\u00f3n: 3.4.14\n</pre> In\u00a0[63]: Copied! <pre># === FLOW PRINCIPAL (EL PIPELINE COMPLETO) ===\n\n@flow(name=\"Pipeline Simple NYC Taxi\")\ndef pipeline_taxi_simple():\n    \"\"\"\n    Flow simple que conecta todos los tasks\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\"Iniciando pipeline simple...\")\n\n    # URLs de datos\n    trips_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\"\n    zones_url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv\"\n\n    # PASO 1: Cargar datos (con retry autom\u00e1tico si falla)\n    logger.info(\"Paso 1: Cargando datos...\")\n    trips = cargar_datos(trips_url, \"parquet\")  # tipo de datos trips\n    zones = cargar_datos(zones_url, \"csv\")  # tipo de datos zones\n\n    # PASO 2: Hacer join\n    logger.info(\"Paso 2: Haciendo join...\")\n    data_unida = hacer_join_simple(trips, zones)\n\n    # PASO 3: An\u00e1lisis b\u00e1sico\n    logger.info(\"Paso 3: Analizando...\")\n    resultados = analisis_rapido(data_unida)\n\n    # PASO 4: Mostrar resultados\n    logger.info(\"Pipeline completado!\")\n    logger.info(f\"Resultados: {resultados}\")\n\n    return resultados\n</pre> # === FLOW PRINCIPAL (EL PIPELINE COMPLETO) ===  @flow(name=\"Pipeline Simple NYC Taxi\") def pipeline_taxi_simple():     \"\"\"     Flow simple que conecta todos los tasks     \"\"\"     logger = get_run_logger()     logger.info(\"Iniciando pipeline simple...\")      # URLs de datos     trips_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\"     zones_url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv\"      # PASO 1: Cargar datos (con retry autom\u00e1tico si falla)     logger.info(\"Paso 1: Cargando datos...\")     trips = cargar_datos(trips_url, \"parquet\")  # tipo de datos trips     zones = cargar_datos(zones_url, \"csv\")  # tipo de datos zones      # PASO 2: Hacer join     logger.info(\"Paso 2: Haciendo join...\")     data_unida = hacer_join_simple(trips, zones)      # PASO 3: An\u00e1lisis b\u00e1sico     logger.info(\"Paso 3: Analizando...\")     resultados = analisis_rapido(data_unida)      # PASO 4: Mostrar resultados     logger.info(\"Pipeline completado!\")     logger.info(f\"Resultados: {resultados}\")      return resultados In\u00a0[64]: Copied! <pre># === EJECUTAR EL PIPELINE ===\n\nif __name__ == \"__main__\":\n    print(\"Ejecutando pipeline simple...\")\n\n    # Ejecutar el flow\n    resultado = pipeline_taxi_simple()  # nombre de la funci\u00f3n del flow\n\n    print(\"\\nRESULTADOS FINALES:\")\n    print(f\"   Total registros: {resultado['total_registros']:,}\")\n    print(f\"   Distancia promedio: {resultado['distancia_promedio']} millas\")\n    print(f\"   Tarifa promedio: ${resultado['tarifa_promedio']}\")\n    print(\"\\nTop 3 Boroughs:\")\n    for borough, count in resultado['top_boroughs'].items():  # clave del diccionario que contiene boroughs\n        print(f\"   {borough}: {count:,} viajes\")\n</pre> # === EJECUTAR EL PIPELINE ===  if __name__ == \"__main__\":     print(\"Ejecutando pipeline simple...\")      # Ejecutar el flow     resultado = pipeline_taxi_simple()  # nombre de la funci\u00f3n del flow      print(\"\\nRESULTADOS FINALES:\")     print(f\"   Total registros: {resultado['total_registros']:,}\")     print(f\"   Distancia promedio: {resultado['distancia_promedio']} millas\")     print(f\"   Tarifa promedio: ${resultado['tarifa_promedio']}\")     print(\"\\nTop 3 Boroughs:\")     for borough, count in resultado['top_boroughs'].items():  # clave del diccionario que contiene boroughs         print(f\"   {borough}: {count:,} viajes\") <pre>Ejecutando pipeline simple...\n</pre> <pre>08:00:22.175 | INFO    | prefect - Starting temporary server on http://127.0.0.1:8096\nSee https://docs.prefect.io/v3/concepts/server#how-to-guides for more information on running a dedicated Prefect server.\n</pre> <pre>08:00:26.183 | INFO    | Flow run 'rigorous-lizard' - Beginning flow run 'rigorous-lizard' for flow 'Pipeline Simple NYC Taxi'\n</pre> <pre>08:00:26.185 | INFO    | Flow run 'rigorous-lizard' - Iniciando pipeline simple...\n</pre> <pre>08:00:26.186 | INFO    | Flow run 'rigorous-lizard' - Paso 1: Cargando datos...\n</pre> <pre>08:00:26.201 | INFO    | Task run 'Cargar Datos-322' - Cargando parquet desde: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\n</pre> <pre>08:00:26.207 | INFO    | Task run 'Cargar Datos-322' - Task run failed with exception: ArrowKeyError('A type extension with name pandas.period already defined') - Retry 1/3 will start 10 second(s) from now\n</pre> <pre>08:00:36.212 | INFO    | Task run 'Cargar Datos-322' - Cargando parquet desde: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\n</pre> <pre>08:00:36.217 | INFO    | Task run 'Cargar Datos-322' - Task run failed with exception: ArrowKeyError('A type extension with name pandas.period already defined') - Retry 2/3 will start 10 second(s) from now\n</pre> <pre>08:00:46.227 | INFO    | Task run 'Cargar Datos-322' - Cargando parquet desde: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\n</pre> <pre>08:00:46.233 | INFO    | Task run 'Cargar Datos-322' - Task run failed with exception: ArrowKeyError('A type extension with name pandas.period already defined') - Retry 3/3 will start 10 second(s) from now\n</pre> <pre>08:00:56.240 | INFO    | Task run 'Cargar Datos-322' - Cargando parquet desde: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\n</pre> <pre>08:00:56.244 | ERROR   | Task run 'Cargar Datos-322' - Task run failed with exception: ArrowKeyError('A type extension with name pandas.period already defined') - Retries are exhausted\nTraceback (most recent call last):\n  File \"/Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/task_engine.py\", line 869, in run_context\n    yield self\n  File \"/Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/task_engine.py\", line 1505, in run_task_sync\n    engine.call_task_fn(txn)\n  File \"/Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/task_engine.py\", line 886, in call_task_fn\n    result = call_with_parameters(self.task.fn, parameters)\n  File \"/Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/utilities/callables.py\", line 210, in call_with_parameters\n    return fn(*args, **kwargs)\n  File \"/var/folders/j9/vw8n5h8j7lzbcpd4gp5z5r280000gn/T/ipykernel_11937/1823903112.py\", line 21, in cargar_datos\n    return pd.read_parquet(url, engine=\"pyarrow\")\n  File \"/Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pandas/io/parquet.py\", line 651, in read_parquet\n    impl = get_engine(engine)\n  File \"/Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pandas/io/parquet.py\", line 78, in get_engine\n    return PyArrowImpl()\n  File \"/Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pandas/io/parquet.py\", line 169, in __init__\n    import pandas.core.arrays.arrow.extension_types  # pyright: ignore[reportUnusedImport] # noqa: F401\n  File \"/Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pandas/core/arrays/arrow/extension_types.py\", line 59, in &lt;module&gt;\n    pyarrow.register_extension_type(_period_type)\n  File \"pyarrow/types.pxi\", line 2226, in pyarrow.lib.register_extension_type\n  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\npyarrow.lib.ArrowKeyError: A type extension with name pandas.period already defined\n</pre> <pre>08:00:56.252 | ERROR   | Task run 'Cargar Datos-322' - Finished in state Failed('Task run encountered an exception ArrowKeyError: A type extension with name pandas.period already defined')\n</pre> <pre>08:00:56.253 | ERROR   | Flow run 'rigorous-lizard' - Encountered exception during execution: ArrowKeyError('A type extension with name pandas.period already defined')\nTraceback (most recent call last):\n  File \"/Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/flow_engine.py\", line 782, in run_context\n    yield self\n  File \"/Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/flow_engine.py\", line 1397, in run_flow_sync\n    engine.call_flow_fn()\n  File \"/Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/flow_engine.py\", line 802, in call_flow_fn\n    result = call_with_parameters(self.flow.fn, self.parameters)\n  File \"/Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/utilities/callables.py\", line 210, in call_with_parameters\n    return fn(*args, **kwargs)\n  File \"/var/folders/j9/vw8n5h8j7lzbcpd4gp5z5r280000gn/T/ipykernel_11937/3170529249.py\", line 17, in pipeline_taxi_simple\n    trips = cargar_datos(trips_url, \"parquet\")  # tipo de datos trips\n  File \"/Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/tasks.py\", line 1139, in __call__\n    return run_task(\n  File \"/Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/task_engine.py\", line 1732, in run_task\n    return run_task_sync(**kwargs)\n  File \"/Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/task_engine.py\", line 1507, in run_task_sync\n    return engine.state if return_type == \"state\" else engine.result()\n  File \"/Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/task_engine.py\", line 494, in result\n    raise self._raised\n  File \"/Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/task_engine.py\", line 869, in run_context\n    yield self\n  File \"/Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/task_engine.py\", line 1505, in run_task_sync\n    engine.call_task_fn(txn)\n  File \"/Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/task_engine.py\", line 886, in call_task_fn\n    result = call_with_parameters(self.task.fn, parameters)\n  File \"/Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/utilities/callables.py\", line 210, in call_with_parameters\n    return fn(*args, **kwargs)\n  File \"/var/folders/j9/vw8n5h8j7lzbcpd4gp5z5r280000gn/T/ipykernel_11937/1823903112.py\", line 21, in cargar_datos\n    return pd.read_parquet(url, engine=\"pyarrow\")\n  File \"/Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pandas/io/parquet.py\", line 651, in read_parquet\n    impl = get_engine(engine)\n  File \"/Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pandas/io/parquet.py\", line 78, in get_engine\n    return PyArrowImpl()\n  File \"/Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pandas/io/parquet.py\", line 169, in __init__\n    import pandas.core.arrays.arrow.extension_types  # pyright: ignore[reportUnusedImport] # noqa: F401\n  File \"/Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pandas/core/arrays/arrow/extension_types.py\", line 59, in &lt;module&gt;\n    pyarrow.register_extension_type(_period_type)\n  File \"pyarrow/types.pxi\", line 2226, in pyarrow.lib.register_extension_type\n  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\npyarrow.lib.ArrowKeyError: A type extension with name pandas.period already defined\n</pre> <pre>08:00:56.272 | INFO    | Flow run 'rigorous-lizard' - Finished in state Failed('Flow run encountered an exception: ArrowKeyError: A type extension with name pandas.period already defined')\n</pre> <pre>\n---------------------------------------------------------------------------\nArrowKeyError                             Traceback (most recent call last)\nCell In[64], line 7\n      4 print(\"Ejecutando pipeline simple...\")\n      6 # Ejecutar el flow\n----&gt; 7 resultado = pipeline_taxi_simple()  # nombre de la funci\u00f3n del flow\n      9 print(\"\\nRESULTADOS FINALES:\")\n     10 print(f\"   Total registros: {resultado['total_registros']:,}\")\n\nFile ~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/flows.py:1702, in Flow.__call__(self, return_state, wait_for, *args, **kwargs)\n   1698     return track_viz_task(self.isasync, self.name, parameters)\n   1700 from prefect.flow_engine import run_flow\n-&gt; 1702 return run_flow(\n   1703     flow=self,\n   1704     parameters=parameters,\n   1705     wait_for=wait_for,\n   1706     return_type=return_type,\n   1707 )\n\nFile ~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/flow_engine.py:1554, in run_flow(flow, flow_run, parameters, wait_for, return_type, error_logger, context)\n   1552         ret_val = run_flow_async(**kwargs)\n   1553     else:\n-&gt; 1554         ret_val = run_flow_sync(**kwargs)\n   1555 except (Abort, Pause):\n   1556     raise\n\nFile ~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/flow_engine.py:1399, in run_flow_sync(flow, flow_run, parameters, wait_for, return_type, context)\n   1396         with engine.run_context():\n   1397             engine.call_flow_fn()\n-&gt; 1399 return engine.state if return_type == \"state\" else engine.result()\n\nFile ~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/flow_engine.py:361, in FlowRunEngine.result(self, raise_on_failure)\n    359 if self._raised is not NotSet:\n    360     if raise_on_failure:\n--&gt; 361         raise self._raised\n    362     return self._raised\n    364 # This is a fall through case which leans on the existing state result mechanics to get the\n    365 # return value. This is necessary because we currently will return a State object if the\n    366 # the State was Prefect-created.\n    367 # TODO: Remove the need to get the result from a State except in cases where the return value\n    368 # is a State object.\n\nFile ~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/flow_engine.py:782, in FlowRunEngine.run_context(self)\n    775     with timeout_context(\n    776         seconds=self.flow.timeout_seconds,\n    777         timeout_exc_type=FlowRunTimeoutError,\n    778     ):\n    779         self.logger.debug(\n    780             f\"Executing flow {self.flow.name!r} for flow run {self.flow_run.name!r}...\"\n    781         )\n--&gt; 782         yield self\n    783 except TimeoutError as exc:\n    784     self.handle_timeout(exc)\n\nFile ~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/flow_engine.py:1397, in run_flow_sync(flow, flow_run, parameters, wait_for, return_type, context)\n   1395     while engine.is_running():\n   1396         with engine.run_context():\n-&gt; 1397             engine.call_flow_fn()\n   1399 return engine.state if return_type == \"state\" else engine.result()\n\nFile ~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/flow_engine.py:802, in FlowRunEngine.call_flow_fn(self)\n    800     return _call_flow_fn()\n    801 else:\n--&gt; 802     result = call_with_parameters(self.flow.fn, self.parameters)\n    803     self.handle_success(result)\n\nFile ~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/utilities/callables.py:210, in call_with_parameters(fn, parameters)\n    202 \"\"\"\n    203 Call a function with parameters extracted with `get_call_parameters`\n    204 \n   (...)\n    207 the args/kwargs using `parameters_to_positional_and_keyword` directly\n    208 \"\"\"\n    209 args, kwargs = parameters_to_args_kwargs(fn, parameters)\n--&gt; 210 return fn(*args, **kwargs)\n\nCell In[63], line 17, in pipeline_taxi_simple()\n     15 # PASO 1: Cargar datos (con retry autom\u00e1tico si falla)\n     16 logger.info(\"Paso 1: Cargando datos...\")\n---&gt; 17 trips = cargar_datos(trips_url, \"parquet\")  # tipo de datos trips\n     18 zones = cargar_datos(zones_url, \"csv\")  # tipo de datos zones\n     20 # PASO 2: Hacer join\n\nFile ~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/tasks.py:1139, in Task.__call__(self, return_state, wait_for, *args, **kwargs)\n   1133     return track_viz_task(\n   1134         self.isasync, self.name, parameters, self.viz_return_value\n   1135     )\n   1137 from prefect.task_engine import run_task\n-&gt; 1139 return run_task(\n   1140     task=self,\n   1141     parameters=parameters,\n   1142     wait_for=wait_for,\n   1143     return_type=return_type,\n   1144 )\n\nFile ~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/task_engine.py:1732, in run_task(task, task_run_id, task_run, parameters, wait_for, return_type, dependencies, context)\n   1730     return run_task_async(**kwargs)\n   1731 else:\n-&gt; 1732     return run_task_sync(**kwargs)\n\nFile ~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/task_engine.py:1507, in run_task_sync(task, task_run_id, task_run, parameters, wait_for, return_type, dependencies, context)\n   1500         with (\n   1501             engine.asset_context(),\n   1502             engine.run_context(),\n   1503             engine.transaction_context() as txn,\n   1504         ):\n   1505             engine.call_task_fn(txn)\n-&gt; 1507 return engine.state if return_type == \"state\" else engine.result()\n\nFile ~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/task_engine.py:494, in SyncTaskRunEngine.result(self, raise_on_failure)\n    491 if self._raised is not NotSet:\n    492     # if the task raised an exception, raise it\n    493     if raise_on_failure:\n--&gt; 494         raise self._raised\n    496     # otherwise, return the exception\n    497     return self._raised\n\nFile ~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/task_engine.py:869, in SyncTaskRunEngine.run_context(self)\n    866         if self.is_cancelled():\n    867             raise CancelledError(\"Task run cancelled by the task runner\")\n--&gt; 869         yield self\n    870 except TimeoutError as exc:\n    871     self.handle_timeout(exc)\n\nFile ~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/task_engine.py:1505, in run_task_sync(task, task_run_id, task_run, parameters, wait_for, return_type, dependencies, context)\n   1499         run_coro_as_sync(engine.wait_until_ready())\n   1500         with (\n   1501             engine.asset_context(),\n   1502             engine.run_context(),\n   1503             engine.transaction_context() as txn,\n   1504         ):\n-&gt; 1505             engine.call_task_fn(txn)\n   1507 return engine.state if return_type == \"state\" else engine.result()\n\nFile ~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/task_engine.py:886, in SyncTaskRunEngine.call_task_fn(self, transaction)\n    884     result = transaction.read()\n    885 else:\n--&gt; 886     result = call_with_parameters(self.task.fn, parameters)\n    887 self.handle_success(result, transaction=transaction)\n    888 return result\n\nFile ~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/prefect/utilities/callables.py:210, in call_with_parameters(fn, parameters)\n    202 \"\"\"\n    203 Call a function with parameters extracted with `get_call_parameters`\n    204 \n   (...)\n    207 the args/kwargs using `parameters_to_positional_and_keyword` directly\n    208 \"\"\"\n    209 args, kwargs = parameters_to_args_kwargs(fn, parameters)\n--&gt; 210 return fn(*args, **kwargs)\n\nCell In[62], line 21, in cargar_datos(url, tipo)\n     18 tipo = tipo.lower().strip()\n     19 if tipo == \"parquet\":\n     20     # Requiere pyarrow instalado\n---&gt; 21     return pd.read_parquet(url, engine=\"pyarrow\")\n     22 elif tipo == \"csv\":\n     23     # Maneja compresi\u00f3n (gzip) autom\u00e1ticamente\n     24     return pd.read_csv(url, encoding=\"utf-8\", low_memory=False, compression=\"infer\")\n\nFile ~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pandas/io/parquet.py:651, in read_parquet(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\n    498 @doc(storage_options=_shared_docs[\"storage_options\"])\n    499 def read_parquet(\n    500     path: FilePath | ReadBuffer[bytes],\n   (...)\n    508     **kwargs,\n    509 ) -&gt; DataFrame:\n    510     \"\"\"\n    511     Load a parquet object from the file path, returning a DataFrame.\n    512 \n   (...)\n    648     1    4    9\n    649     \"\"\"\n--&gt; 651     impl = get_engine(engine)\n    653     if use_nullable_dtypes is not lib.no_default:\n    654         msg = (\n    655             \"The argument 'use_nullable_dtypes' is deprecated and will be removed \"\n    656             \"in a future version.\"\n    657         )\n\nFile ~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pandas/io/parquet.py:78, in get_engine(engine)\n     67     raise ImportError(\n     68         \"Unable to find a usable engine; \"\n     69         \"tried using: 'pyarrow', 'fastparquet'.\\n\"\n   (...)\n     74         f\"{error_msgs}\"\n     75     )\n     77 if engine == \"pyarrow\":\n---&gt; 78     return PyArrowImpl()\n     79 elif engine == \"fastparquet\":\n     80     return FastParquetImpl()\n\nFile ~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pandas/io/parquet.py:169, in PyArrowImpl.__init__(self)\n    166 import pyarrow.parquet\n    168 # import utils to register the pyarrow extension types\n--&gt; 169 import pandas.core.arrays.arrow.extension_types  # pyright: ignore[reportUnusedImport] # noqa: F401\n    171 self.api = pyarrow\n\nFile ~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pandas/core/arrays/arrow/extension_types.py:59\n     57 # register the type with a dummy instance\n     58 _period_type = ArrowPeriodType(\"D\")\n---&gt; 59 pyarrow.register_extension_type(_period_type)\n     62 class ArrowIntervalType(pyarrow.ExtensionType):\n     63     def __init__(self, subtype, closed: IntervalClosedType) -&gt; None:\n     64         # attributes need to be set first before calling\n     65         # super init (as that calls serialize)\n\nFile ~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pyarrow/types.pxi:2226, in pyarrow.lib.register_extension_type()\n\nFile ~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pyarrow/error.pxi:92, in pyarrow.lib.check_status()\n\nArrowKeyError: A type extension with name pandas.period already defined</pre> In\u00a0[\u00a0]: Copied! <pre># ANTES - C\u00f3digo normal:\ndef cargar_datos(url):\n    return pd.read_csv(url)  # Si falla, todo se rompe\n\n# DESPU\u00c9S - Con Prefect:\n@task(retries=2)\ndef cargar_datos(url):\n    return pd.read_csv(url)  # Si falla, lo intenta 2 veces m\u00e1s\n</pre> # ANTES - C\u00f3digo normal: def cargar_datos(url):     return pd.read_csv(url)  # Si falla, todo se rompe  # DESPU\u00c9S - Con Prefect: @task(retries=2) def cargar_datos(url):     return pd.read_csv(url)  # Si falla, lo intenta 2 veces m\u00e1s"},{"location":"portfolio/entregas/tres/#preguntas-finales","title":"PREGUNTAS FINALES\u00b6","text":"<ol> <li>\u00bfQu\u00e9 diferencia hay entre un LEFT JOIN y un INNER JOIN? PISTA: Gu\u00eda visual de joins</li> </ol> <p>Usando el left join te quedas con los datos de la tabla, es la que se mantiene, mientras que de la derecha son los valores que se agregan. Usando inner join se hace interseccion de las dos tablas.</p> <ol> <li>\u00bfPor qu\u00e9 usamos LEFT JOIN en lugar de INNER JOIN para trips+zones? PISTA: \u00bfQu\u00e9 pasar\u00eda si algunos viajes no tienen zona asignada?</li> </ol> <p>Porque al hacer left te aseguras que vas a mantener toda la informacion de los viajes agregando las zonas correspondientes a los mismos. Si hicieramos inner vamos a perder la informacion de los trips que no tienen zona asignada.</p> <ol> <li>\u00bfQu\u00e9 problemas pueden surgir al hacer joins con datos de fechas? PISTA: Tipos de datos, formatos, zonas horarias</li> </ol> <p>\u2022 Diferencias en el tipo de dato, por ejemplo: string o datetime. \u2022 Formatos de fecha distintos, por ejemplo: YYYY-MM-DD o DD/MM/YYYY. \u2022 Valores nulos o fechas faltantes que pueden impedir el join.</p> <ol> <li>\u00bfCu\u00e1l es la ventaja de integrar m\u00faltiples fuentes de datos? PISTA: An\u00e1lisis m\u00e1s rico, contexto completo, insights cruzados</li> </ol> <p>Nos permite realizar un an\u00e1lisis m\u00e1s completo y contextualizado. Adem\u00e1s, se pueden cruzar variables de diferentes bases para descubrir patrones que no ser\u00edan visibles en un solo dataset; esto enriquece la informaci\u00f3n y habilita conclusiones m\u00e1s profundos.</p> <ol> <li>\u00bfQu\u00e9 insights de negocio obtuviste del an\u00e1lisis integrado? PISTA: Patrones por zona, impacto de eventos especiales, oportunidades</li> </ol> <p>Manhattan concentra la mayor\u00eda de los viajes, los viajes en Queens son m\u00e1s largos y costosos en promedio y el mejor revenue por km son de EWR. Hay diferencias claras en el revenue por kil\u00f3metro y en la tasa de propinas entre boroughs y los d\u00edas especiales pueden tener impacto en la distancia y tarifa promedio.</p>"},{"location":"portfolio/entregas/tres/#bonus","title":"BONUS\u00b6","text":""},{"location":"portfolio/entregas/tres/#preguntas-bonus","title":"PREGUNTAS BONUS\u00b6","text":"<ol> <li><p>\u00bfQu\u00e9 ventaja tiene usar @task en lugar de una funci\u00f3n normal? PISTA: \u00bfQu\u00e9 pasa si la carga de datos falla temporalmente?</p> <p>Una funci\u00f3n normal en Python se ejecuta sin ning\u00fan control extra.</p> <p>Cuando usas @task:</p> <ul> <li><p>Pod\u00e9s definir reintentos autom\u00e1ticos si falla por algo temporal (ej: red ca\u00edda al bajar datos).</p> </li> <li><p>Pod\u00e9s definir cortes por tiempo (si un paso tarda demasiado, Prefect lo corta).</p> </li> <li><p>Ten\u00e9s logs integrados en la interfaz de Prefect.</p> </li> <li><p>Cada task queda orquestado y monitoreado: pod\u00e9s ver qu\u00e9 fall\u00f3 y reintentar s\u00f3lo ese paso.</p> </li> </ul> </li> <li><p>\u00bfPara qu\u00e9 sirve el @flow decorator? PISTA: \u00bfC\u00f3mo conecta y organiza los tasks? El @flow define un pipeline completo, que organiza y conecta varios @task.</p> <p>Le dice a Prefect: \u201cEsto no es s\u00f3lo un script de Python, es un flujo de trabajo con dependencias y monitoreo\u201d.</p> <p>Permite:</p> <ul> <li><p>Ejecutar tasks en orden y pasar resultados de uno a otro.</p> </li> <li><p>Monitorear el flow run entero.</p> </li> </ul> </li> <li><p>\u00bfEn qu\u00e9 casos reales usar\u00edas esto? PISTA: Reportes diarios, an\u00e1lisis autom\u00e1ticos, pipelines de ML</p> <p>Reportes diarios: Automatizar que cada ma\u00f1ana se bajen datos de ventas, se limpien y se env\u00eden reportes a un dashboard.</p> <p>An\u00e1lisis autom\u00e1ticos: Procesar logs de usuarios, detectar anomal\u00edas o generar alertas de fraude sin intervenci\u00f3n manual.</p> <p>Pipelines de ML: cargar dataset - limpiar/preprocesar - entrenar modelo - guardar m\u00e9tricas</p> </li> </ol>"},{"location":"portfolio/entregas/uno/","title":"PREGUNTAS A RESPONDER","text":"In\u00a0[2]: Copied! <pre># Importar librer\u00edas que vamos a usar\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport kagglehub\n</pre> # Importar librer\u00edas que vamos a usar import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import kagglehub In\u00a0[3]: Copied! <pre># === CARGAR DATOS DE NETFLIX ===\n\n# 1. Cargar el dataset desde una URL\nurl = \"https://raw.githubusercontent.com/swapnilg4u/Netflix-Data-Analysis/refs/heads/master/netflix_titles.csv\"\nnetflix = pd.read_csv(url)  # funci\u00f3n para leer archivos CSV desde URL o archivo local\n\nprint(\"DATASET: Netflix Titles\")\nprint(f\"Forma: {netflix.shape}\")\nprint(f\"Columnas: {list(netflix.columns)}\")\n\n# 2. Explorar los datos b\u00e1sicamente\nprint(\"\\nPrimeras 5 filas:\")\nprint(netflix.head())  # m\u00e9todo para mostrar las primeras filas del DataFrame\n\n# 3. Informaci\u00f3n sobre tipos de datos y memoria\nprint(\"\\nINFORMACI\u00d3N GENERAL:\")\nprint(netflix.info())  # m\u00e9todo que muestra tipos de datos, memoria y valores no nulos\n\n# 4. Estad\u00edsticas b\u00e1sicas para columnas num\u00e9ricas\nprint(\"\\nESTAD\u00cdSTICAS B\u00c1SICAS:\")\nprint(\"Promedio de a\u00f1o\", round(netflix[\"release_year\"].describe()))  # m\u00e9todo que calcula estad\u00edsticas descriptivas (mean, std, min, max, etc.)\n</pre> # === CARGAR DATOS DE NETFLIX ===  # 1. Cargar el dataset desde una URL url = \"https://raw.githubusercontent.com/swapnilg4u/Netflix-Data-Analysis/refs/heads/master/netflix_titles.csv\" netflix = pd.read_csv(url)  # funci\u00f3n para leer archivos CSV desde URL o archivo local  print(\"DATASET: Netflix Titles\") print(f\"Forma: {netflix.shape}\") print(f\"Columnas: {list(netflix.columns)}\")  # 2. Explorar los datos b\u00e1sicamente print(\"\\nPrimeras 5 filas:\") print(netflix.head())  # m\u00e9todo para mostrar las primeras filas del DataFrame  # 3. Informaci\u00f3n sobre tipos de datos y memoria print(\"\\nINFORMACI\u00d3N GENERAL:\") print(netflix.info())  # m\u00e9todo que muestra tipos de datos, memoria y valores no nulos  # 4. Estad\u00edsticas b\u00e1sicas para columnas num\u00e9ricas print(\"\\nESTAD\u00cdSTICAS B\u00c1SICAS:\") print(\"Promedio de a\u00f1o\", round(netflix[\"release_year\"].describe()))  # m\u00e9todo que calcula estad\u00edsticas descriptivas (mean, std, min, max, etc.)  <pre>DATASET: Netflix Titles\nForma: (6234, 12)\nColumnas: ['show_id', 'type', 'title', 'director', 'cast', 'country', 'date_added', 'release_year', 'rating', 'duration', 'listed_in', 'description']\n\nPrimeras 5 filas:\n    show_id     type                                    title  \\\n0  81145628    Movie  Norm of the North: King Sized Adventure   \n1  80117401    Movie               Jandino: Whatever it Takes   \n2  70234439  TV Show                       Transformers Prime   \n3  80058654  TV Show         Transformers: Robots in Disguise   \n4  80125979    Movie                             #realityhigh   \n\n                   director  \\\n0  Richard Finn, Tim Maltby   \n1                       NaN   \n2                       NaN   \n3                       NaN   \n4          Fernando Lebrija   \n\n                                                cast  \\\n0  Alan Marriott, Andrew Toth, Brian Dobson, Cole...   \n1                                   Jandino Asporaat   \n2  Peter Cullen, Sumalee Montano, Frank Welker, J...   \n3  Will Friedle, Darren Criss, Constance Zimmer, ...   \n4  Nesta Cooper, Kate Walsh, John Michael Higgins...   \n\n                                    country         date_added  release_year  \\\n0  United States, India, South Korea, China  September 9, 2019          2019   \n1                            United Kingdom  September 9, 2016          2016   \n2                             United States  September 8, 2018          2013   \n3                             United States  September 8, 2018          2016   \n4                             United States  September 8, 2017          2017   \n\n     rating  duration                           listed_in  \\\n0     TV-PG    90 min  Children &amp; Family Movies, Comedies   \n1     TV-MA    94 min                     Stand-Up Comedy   \n2  TV-Y7-FV  1 Season                            Kids' TV   \n3     TV-Y7  1 Season                            Kids' TV   \n4     TV-14    99 min                            Comedies   \n\n                                         description  \n0  Before planning an awesome wedding for his gra...  \n1  Jandino Asporaat riffs on the challenges of ra...  \n2  With the help of three human allies, the Autob...  \n3  When a prison ship crash unleashes hundreds of...  \n4  When nerdy high schooler Dani finally attracts...  \n\nINFORMACI\u00d3N GENERAL:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 6234 entries, 0 to 6233\nData columns (total 12 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   show_id       6234 non-null   int64 \n 1   type          6234 non-null   object\n 2   title         6234 non-null   object\n 3   director      4265 non-null   object\n 4   cast          5664 non-null   object\n 5   country       5758 non-null   object\n 6   date_added    6223 non-null   object\n 7   release_year  6234 non-null   int64 \n 8   rating        6224 non-null   object\n 9   duration      6234 non-null   object\n 10  listed_in     6234 non-null   object\n 11  description   6234 non-null   object\ndtypes: int64(2), object(10)\nmemory usage: 584.6+ KB\nNone\n\nESTAD\u00cdSTICAS B\u00c1SICAS:\nPromedio de a\u00f1o count    6234.0\nmean     2013.0\nstd         9.0\nmin      1925.0\n25%      2013.0\n50%      2016.0\n75%      2018.0\nmax      2020.0\nName: release_year, dtype: float64\n</pre> In\u00a0[4]: Copied! <pre># === DETECTAR Y VISUALIZAR DATOS FALTANTES ===\n\n# 1. Calcular datos faltantes por columna\nmissing_data = netflix.isnull().sum().sort_values(ascending=False)  # detectar valores nulos y contar por columna\nmissing_percent = (netflix.isnull().sum() / len(netflix) * 100).sort_values(ascending=False)  # calcular porcentaje de nulos\n\nprint(\"DATOS FALTANTES:\")\nprint(missing_data[missing_data &gt; 0])\nprint(\"\\n PORCENTAJES:\")\nprint(missing_percent[missing_percent &gt; 0])\n\n# 2. Crear visualizaci\u00f3n de datos faltantes\nplt.figure(figsize=(12, 6))\n\n# Subplot 1: Gr\u00e1fico de barras de datos faltantes\nplt.subplot(1, 2, 1)\nsns.barplot(x=missing_percent[missing_percent &gt; 0].values,  # funci\u00f3n para crear barras horizontales\n            y=missing_percent[missing_percent &gt; 0].index,\n            palette='Reds_r')\nplt.title('Porcentaje de Datos Faltantes por Columna')\nplt.xlabel('Porcentaje (%)')\n\n# Subplot 2: Heatmap de datos faltantes\nplt.subplot(1, 2, 2)\nsns.heatmap(netflix.isnull(), cbar=True, cmap='viridis')  # funci\u00f3n para crear mapa de calor de valores booleanos\nplt.title('Patr\u00f3n de Datos Faltantes')\nplt.xticks(rotation=45)\n\nplt.tight_layout()\nplt.show()  # funci\u00f3n para mostrar/renderizar los gr\u00e1ficos en pantalla\n</pre> # === DETECTAR Y VISUALIZAR DATOS FALTANTES ===  # 1. Calcular datos faltantes por columna missing_data = netflix.isnull().sum().sort_values(ascending=False)  # detectar valores nulos y contar por columna missing_percent = (netflix.isnull().sum() / len(netflix) * 100).sort_values(ascending=False)  # calcular porcentaje de nulos  print(\"DATOS FALTANTES:\") print(missing_data[missing_data &gt; 0]) print(\"\\n PORCENTAJES:\") print(missing_percent[missing_percent &gt; 0])  # 2. Crear visualizaci\u00f3n de datos faltantes plt.figure(figsize=(12, 6))  # Subplot 1: Gr\u00e1fico de barras de datos faltantes plt.subplot(1, 2, 1) sns.barplot(x=missing_percent[missing_percent &gt; 0].values,  # funci\u00f3n para crear barras horizontales             y=missing_percent[missing_percent &gt; 0].index,             palette='Reds_r') plt.title('Porcentaje de Datos Faltantes por Columna') plt.xlabel('Porcentaje (%)')  # Subplot 2: Heatmap de datos faltantes plt.subplot(1, 2, 2) sns.heatmap(netflix.isnull(), cbar=True, cmap='viridis')  # funci\u00f3n para crear mapa de calor de valores booleanos plt.title('Patr\u00f3n de Datos Faltantes') plt.xticks(rotation=45)  plt.tight_layout() plt.show()  # funci\u00f3n para mostrar/renderizar los gr\u00e1ficos en pantalla  <pre>DATOS FALTANTES:\ndirector      1969\ncast           570\ncountry        476\ndate_added      11\nrating          10\ndtype: int64\n\n PORCENTAJES:\ndirector      31.584857\ncast           9.143407\ncountry        7.635547\ndate_added     0.176452\nrating         0.160411\ndtype: float64\n</pre> <pre>/var/folders/j9/vw8n5h8j7lzbcpd4gp5z5r280000gn/T/ipykernel_26073/2022276741.py:17: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x=missing_percent[missing_percent &gt; 0].values,  # funci\u00f3n para crear barras horizontales\n</pre> In\u00a0[5]: Copied! <pre># === DETECCI\u00d3N DE OUTLIERS Y ANOMAL\u00cdAS ===\n\n# 1. Analizar a\u00f1os de lanzamiento at\u00edpicos\nprint(\"AN\u00c1LISIS DE OUTLIERS EN A\u00d1OS:\")\nnetflix['release_year_clean'] = pd.to_numeric(netflix['release_year'], errors='coerce')\nyear_stats = netflix['release_year_clean'].describe()\nprint(year_stats)\n\n# Identificar a\u00f1os sospechosos\nvery_old = netflix[netflix['release_year_clean'] &lt; 1950]\nfuture_releases = netflix[netflix['release_year_clean'] &gt; 2025]\n\nprint(f\"\\nContenido muy antiguo (&lt; 1950): {len(very_old)} t\u00edtulos\")\nif len(very_old) &gt; 0:\n    print(\"Ejemplos:\")\n    print(very_old[['title', 'release_year', 'type']].head())\n\nprint(f\"\\nLanzamientos futuros (&gt; 2025): {len(future_releases)} t\u00edtulos\")\nif len(future_releases) &gt; 0:\n    print(\"Ejemplos:\")\n    print(future_releases[['title', 'release_year', 'type']].head())\n\n# 2. Crear visualizaciones para detectar outliers\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Gr\u00e1fico 1: Box plot para detectar outliers en a\u00f1os\nsns.boxplot(data=netflix, y='release_year_clean', ax=axes[0, 0])  # funci\u00f3n para mostrar outliers con cajas\naxes[0, 0].set_title('Box Plot - A\u00f1os de Lanzamiento (Outliers)')\naxes[0, 0].set_ylabel('A\u00f1o de Lanzamiento')\n\n# Gr\u00e1fico 2: Histograma de a\u00f1os para ver distribuci\u00f3n\naxes[0, 1].hist(netflix['release_year_clean'].dropna(), bins=50, alpha=0.7, color='skyblue', edgecolor='black')  # histograma con muchos bins\naxes[0, 1].set_title('Distribuci\u00f3n de A\u00f1os de Lanzamiento')\naxes[0, 1].set_xlabel('A\u00f1o')\naxes[0, 1].set_ylabel('Frecuencia')\naxes[0, 1].axvline(netflix['release_year_clean'].mean(), color='red', linestyle='--', label='Media')\naxes[0, 1].legend()\n\n# Gr\u00e1fico 3: An\u00e1lisis de t\u00edtulos duplicados\ntitle_counts = netflix['title'].value_counts()  # contar frecuencias de t\u00edtulos\nduplicated_titles = title_counts[title_counts &gt; 1]\n\nprint(f\"\\nT\u00cdTULOS DUPLICADOS: {len(duplicated_titles)} t\u00edtulos aparecen m\u00faltiples veces\")\nif len(duplicated_titles) &gt; 0:\n    top_duplicates = duplicated_titles.head(10)\n    sns.barplot(y=top_duplicates.index, x=top_duplicates.values, ax=axes[1, 0], palette='Reds')  # barras horizontales\n    axes[1, 0].set_title('Top 10 T\u00edtulos Duplicados')\n    axes[1, 0].set_xlabel('Cantidad de Apariciones')\nelse:\n    axes[1, 0].text(0.5, 0.5, 'No se encontraron\\nt\u00edtulos duplicados', \n                    ha='center', va='center', transform=axes[1, 0].transAxes)\n    axes[1, 0].set_title('T\u00edtulos Duplicados - Sin Datos')\n\n# Gr\u00e1fico 4: Longitud de t\u00edtulos (outliers en texto)\nnetflix['title_length'] = netflix['title'].str.len()\ntitle_length_stats = netflix['title_length'].describe()\n\nsns.boxplot(data=netflix, y='title_length', ax=axes[1, 1])  # box plot para longitud de t\u00edtulos\naxes[1, 1].set_title('Box Plot - Longitud de T\u00edtulos')\naxes[1, 1].set_ylabel('Caracteres en el T\u00edtulo')\n\n# Identificar t\u00edtulos extremadamente largos o cortos\nvery_long_titles = netflix[netflix['title_length'] &gt; netflix['title_length'].quantile(0.99)]\nvery_short_titles = netflix[netflix['title_length'] &lt; 5]\n\nprint(f\"\\nT\u00cdTULOS EXTREMOS:\")\nprint(f\"   Muy largos (&gt; percentil 99): {len(very_long_titles)} t\u00edtulos\")\nif len(very_long_titles) &gt; 0:\n    print(f\"   Ejemplo m\u00e1s largo: '{very_long_titles.loc[very_long_titles['title_length'].idxmax(), 'title']}'\")\n\nprint(f\"   Muy cortos (&lt; 5 caracteres): {len(very_short_titles)} t\u00edtulos\")\nif len(very_short_titles) &gt; 0:\n    print(\"   Ejemplos:\")\n    print(very_short_titles[['title', 'title_length', 'type']].head())\n\nplt.tight_layout()\nplt.show()\n\nprint(\"An\u00e1lisis de outliers completado!\")\n</pre> # === DETECCI\u00d3N DE OUTLIERS Y ANOMAL\u00cdAS ===  # 1. Analizar a\u00f1os de lanzamiento at\u00edpicos print(\"AN\u00c1LISIS DE OUTLIERS EN A\u00d1OS:\") netflix['release_year_clean'] = pd.to_numeric(netflix['release_year'], errors='coerce') year_stats = netflix['release_year_clean'].describe() print(year_stats)  # Identificar a\u00f1os sospechosos very_old = netflix[netflix['release_year_clean'] &lt; 1950] future_releases = netflix[netflix['release_year_clean'] &gt; 2025]  print(f\"\\nContenido muy antiguo (&lt; 1950): {len(very_old)} t\u00edtulos\") if len(very_old) &gt; 0:     print(\"Ejemplos:\")     print(very_old[['title', 'release_year', 'type']].head())  print(f\"\\nLanzamientos futuros (&gt; 2025): {len(future_releases)} t\u00edtulos\") if len(future_releases) &gt; 0:     print(\"Ejemplos:\")     print(future_releases[['title', 'release_year', 'type']].head())  # 2. Crear visualizaciones para detectar outliers fig, axes = plt.subplots(2, 2, figsize=(15, 10))  # Gr\u00e1fico 1: Box plot para detectar outliers en a\u00f1os sns.boxplot(data=netflix, y='release_year_clean', ax=axes[0, 0])  # funci\u00f3n para mostrar outliers con cajas axes[0, 0].set_title('Box Plot - A\u00f1os de Lanzamiento (Outliers)') axes[0, 0].set_ylabel('A\u00f1o de Lanzamiento')  # Gr\u00e1fico 2: Histograma de a\u00f1os para ver distribuci\u00f3n axes[0, 1].hist(netflix['release_year_clean'].dropna(), bins=50, alpha=0.7, color='skyblue', edgecolor='black')  # histograma con muchos bins axes[0, 1].set_title('Distribuci\u00f3n de A\u00f1os de Lanzamiento') axes[0, 1].set_xlabel('A\u00f1o') axes[0, 1].set_ylabel('Frecuencia') axes[0, 1].axvline(netflix['release_year_clean'].mean(), color='red', linestyle='--', label='Media') axes[0, 1].legend()  # Gr\u00e1fico 3: An\u00e1lisis de t\u00edtulos duplicados title_counts = netflix['title'].value_counts()  # contar frecuencias de t\u00edtulos duplicated_titles = title_counts[title_counts &gt; 1]  print(f\"\\nT\u00cdTULOS DUPLICADOS: {len(duplicated_titles)} t\u00edtulos aparecen m\u00faltiples veces\") if len(duplicated_titles) &gt; 0:     top_duplicates = duplicated_titles.head(10)     sns.barplot(y=top_duplicates.index, x=top_duplicates.values, ax=axes[1, 0], palette='Reds')  # barras horizontales     axes[1, 0].set_title('Top 10 T\u00edtulos Duplicados')     axes[1, 0].set_xlabel('Cantidad de Apariciones') else:     axes[1, 0].text(0.5, 0.5, 'No se encontraron\\nt\u00edtulos duplicados',                      ha='center', va='center', transform=axes[1, 0].transAxes)     axes[1, 0].set_title('T\u00edtulos Duplicados - Sin Datos')  # Gr\u00e1fico 4: Longitud de t\u00edtulos (outliers en texto) netflix['title_length'] = netflix['title'].str.len() title_length_stats = netflix['title_length'].describe()  sns.boxplot(data=netflix, y='title_length', ax=axes[1, 1])  # box plot para longitud de t\u00edtulos axes[1, 1].set_title('Box Plot - Longitud de T\u00edtulos') axes[1, 1].set_ylabel('Caracteres en el T\u00edtulo')  # Identificar t\u00edtulos extremadamente largos o cortos very_long_titles = netflix[netflix['title_length'] &gt; netflix['title_length'].quantile(0.99)] very_short_titles = netflix[netflix['title_length'] &lt; 5]  print(f\"\\nT\u00cdTULOS EXTREMOS:\") print(f\"   Muy largos (&gt; percentil 99): {len(very_long_titles)} t\u00edtulos\") if len(very_long_titles) &gt; 0:     print(f\"   Ejemplo m\u00e1s largo: '{very_long_titles.loc[very_long_titles['title_length'].idxmax(), 'title']}'\")  print(f\"   Muy cortos (&lt; 5 caracteres): {len(very_short_titles)} t\u00edtulos\") if len(very_short_titles) &gt; 0:     print(\"   Ejemplos:\")     print(very_short_titles[['title', 'title_length', 'type']].head())  plt.tight_layout() plt.show()  print(\"An\u00e1lisis de outliers completado!\") <pre>AN\u00c1LISIS DE OUTLIERS EN A\u00d1OS:\ncount    6234.00000\nmean     2013.35932\nstd         8.81162\nmin      1925.00000\n25%      2013.00000\n50%      2016.00000\n75%      2018.00000\nmax      2020.00000\nName: release_year_clean, dtype: float64\n\nContenido muy antiguo (&lt; 1950): 16 t\u00edtulos\nEjemplos:\n                         title  release_year   type\n2005   Know Your Enemy - Japan          1945  Movie\n2006        Let There Be Light          1946  Movie\n2009  Nazi Concentration Camps          1945  Movie\n2011            Prelude to War          1942  Movie\n2012                San Pietro          1945  Movie\n\nLanzamientos futuros (&gt; 2025): 0 t\u00edtulos\n\nT\u00cdTULOS DUPLICADOS: 57 t\u00edtulos aparecen m\u00faltiples veces\n\nT\u00cdTULOS EXTREMOS:\n   Muy largos (&gt; percentil 99): 62 t\u00edtulos\n   Ejemplo m\u00e1s largo: 'Jim &amp; Andy: The Great Beyond - Featuring a Very Special, Contractually Obligated Mention of Tony Clifton'\n   Muy cortos (&lt; 5 caracteres): 173 t\u00edtulos\n   Ejemplos:\n    title  title_length   type\n19   Love             4  Movie\n41     PK             2  Movie\n45    ATM             3  Movie\n89    5CM             3  Movie\n209   ARQ             3  Movie\n</pre> <pre>/var/folders/j9/vw8n5h8j7lzbcpd4gp5z5r280000gn/T/ipykernel_26073/3322448500.py:46: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(y=top_duplicates.index, x=top_duplicates.values, ax=axes[1, 0], palette='Reds')  # barras horizontales\n</pre> <pre>An\u00e1lisis de outliers completado!\n</pre> In\u00a0[6]: Copied! <pre># === AN\u00c1LISIS DE TIPOS DE CONTENIDO ===\n\n# 1. Calcular frecuencias\ntype_counts = netflix['type'].value_counts()  # m\u00e9todo para contar frecuencias de cada categor\u00eda \u00fanica\ntype_percent = netflix['type'].value_counts(normalize=True) * 100  # mismo m\u00e9todo pero calculando porcentajes\n\nprint(\"TIPOS DE CONTENIDO:\")\nprint(type_counts)\nprint(f\"\\nPorcentajes:\")\nprint(type_percent)\n\n# 2. Crear visualizaciones m\u00faltiples\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Gr\u00e1fico 1: Countplot b\u00e1sico\nsns.countplot(data=netflix, x='type', ax=axes[0, 0], palette='Set2')  # funci\u00f3n para contar y graficar categor\u00edas\naxes[0, 0].set_title('Distribuci\u00f3n: Movies vs TV Shows')\naxes[0, 0].set_ylabel('Cantidad')\n\n# Gr\u00e1fico 2: Pie chart\naxes[0, 1].pie(type_counts.values, labels=type_counts.index,  # funci\u00f3n para crear gr\u00e1fico circular/torta\n                  autopct='%1.1f%%', startangle=90, colors=['skyblue', 'lightcoral'])\naxes[0, 1].set_title('Proporci\u00f3n Movies vs TV Shows')\n\n# Gr\u00e1fico 3: Barplot horizontal\nsns.barplot(y=type_counts.index, x=type_counts.values, ax=axes[1, 0], palette='viridis')  # funci\u00f3n para barras horizontales\naxes[1, 0].set_title('Cantidad por Tipo (Horizontal)')\naxes[1, 0].set_xlabel('Cantidad')\n\n# Gr\u00e1fico 4: Donut chart (m\u00e1s avanzado)\nwedges, texts, autotexts = axes[1, 1].pie(type_counts.values, labels=type_counts.index,  # misma funci\u00f3n de torta para donut\n                                             autopct='%1.1f%%', startangle=90,\n                                             colors=['gold', 'lightgreen'])\n# Crear el hueco del donut\ncentre_circle = plt.Circle((0,0), 0.70, fc='white')\naxes[1, 1].add_artist(centre_circle)\naxes[1, 1].set_title('Donut Chart - Tipos de Contenido')\n\nplt.tight_layout()\nplt.show()\n</pre> # === AN\u00c1LISIS DE TIPOS DE CONTENIDO ===  # 1. Calcular frecuencias type_counts = netflix['type'].value_counts()  # m\u00e9todo para contar frecuencias de cada categor\u00eda \u00fanica type_percent = netflix['type'].value_counts(normalize=True) * 100  # mismo m\u00e9todo pero calculando porcentajes  print(\"TIPOS DE CONTENIDO:\") print(type_counts) print(f\"\\nPorcentajes:\") print(type_percent)  # 2. Crear visualizaciones m\u00faltiples fig, axes = plt.subplots(2, 2, figsize=(15, 10))  # Gr\u00e1fico 1: Countplot b\u00e1sico sns.countplot(data=netflix, x='type', ax=axes[0, 0], palette='Set2')  # funci\u00f3n para contar y graficar categor\u00edas axes[0, 0].set_title('Distribuci\u00f3n: Movies vs TV Shows') axes[0, 0].set_ylabel('Cantidad')  # Gr\u00e1fico 2: Pie chart axes[0, 1].pie(type_counts.values, labels=type_counts.index,  # funci\u00f3n para crear gr\u00e1fico circular/torta                   autopct='%1.1f%%', startangle=90, colors=['skyblue', 'lightcoral']) axes[0, 1].set_title('Proporci\u00f3n Movies vs TV Shows')  # Gr\u00e1fico 3: Barplot horizontal sns.barplot(y=type_counts.index, x=type_counts.values, ax=axes[1, 0], palette='viridis')  # funci\u00f3n para barras horizontales axes[1, 0].set_title('Cantidad por Tipo (Horizontal)') axes[1, 0].set_xlabel('Cantidad')  # Gr\u00e1fico 4: Donut chart (m\u00e1s avanzado) wedges, texts, autotexts = axes[1, 1].pie(type_counts.values, labels=type_counts.index,  # misma funci\u00f3n de torta para donut                                              autopct='%1.1f%%', startangle=90,                                              colors=['gold', 'lightgreen']) # Crear el hueco del donut centre_circle = plt.Circle((0,0), 0.70, fc='white') axes[1, 1].add_artist(centre_circle) axes[1, 1].set_title('Donut Chart - Tipos de Contenido')  plt.tight_layout() plt.show() <pre>TIPOS DE CONTENIDO:\ntype\nMovie      4265\nTV Show    1969\nName: count, dtype: int64\n\nPorcentajes:\ntype\nMovie      68.415143\nTV Show    31.584857\nName: proportion, dtype: float64\n</pre> <pre>/var/folders/j9/vw8n5h8j7lzbcpd4gp5z5r280000gn/T/ipykernel_26073/554739691.py:16: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.countplot(data=netflix, x='type', ax=axes[0, 0], palette='Set2')  # funci\u00f3n para contar y graficar categor\u00edas\n/var/folders/j9/vw8n5h8j7lzbcpd4gp5z5r280000gn/T/ipykernel_26073/554739691.py:26: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(y=type_counts.index, x=type_counts.values, ax=axes[1, 0], palette='viridis')  # funci\u00f3n para barras horizontales\n</pre> In\u00a0[7]: Copied! <pre># === AN\u00c1LISIS DE TENDENCIAS TEMPORALES ===\n\n# 1. Preparar datos temporales\nnetflix['release_year'] = pd.to_numeric(netflix['release_year'], errors='coerce')  # convertir a num\u00e9rico, NaN si no es posible\nyearly_releases = netflix['release_year'].value_counts().sort_index()  # contar frecuencias por a\u00f1o y ordenar\n\n# Filtrar a\u00f1os recientes para mejor visualizaci\u00f3n\nrecent_years = yearly_releases[yearly_releases.index &gt;= 2000]\n\n# 2. Crear visualizaciones temporales m\u00faltiples\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\n# Gr\u00e1fico 1: L\u00ednea temporal\naxes[0, 0].plot(recent_years.index, recent_years.values,  # funci\u00f3n para crear l\u00edneas conectando puntos\n                  marker='o', linewidth=2, markersize=4, color='darkblue')\naxes[0, 0].set_title('Cantidad de Contenido por A\u00f1o (2000-2021)')\naxes[0, 0].set_xlabel('A\u00f1o')\naxes[0, 0].set_ylabel('Cantidad de T\u00edtulos')\naxes[0, 0].grid(True, alpha=0.3)\n\n# Gr\u00e1fico 2: \u00c1rea bajo la curva\naxes[0, 1].fill_between(recent_years.index, recent_years.values,  # funci\u00f3n para rellenar \u00e1rea bajo la l\u00ednea\n                  alpha=0.7, color='lightcoral')\naxes[0, 1].set_title('\u00c1rea - Lanzamientos por A\u00f1o')\naxes[0, 1].set_xlabel('A\u00f1o')\naxes[0, 1].set_ylabel('Cantidad')\n\n# Gr\u00e1fico 3: An\u00e1lisis por tipo de contenido\nnetflix_recent = netflix[netflix['release_year'] &gt;= 2010]\nyearly_by_type = netflix_recent.groupby(['release_year', 'type']).size().unstack(fill_value=0)\n\nyearly_by_type.plot(kind='bar', ax=axes[1, 0],  # tipo de gr\u00e1fico con barras lado a lado (no apiladas)\n                    color=['skyblue', 'lightgreen'], alpha=0.8)\naxes[1, 0].set_title('Lanzamientos por Tipo (2010-2021)')\naxes[1, 0].set_xlabel('A\u00f1o')\naxes[1, 0].set_ylabel('Cantidad')\naxes[1, 0].legend(title='Tipo')\n\n# Gr\u00e1fico 4: Heatmap de lanzamientos por d\u00e9cada y tipo\nnetflix['decade'] = (netflix['release_year'] // 10) * 10\ndecade_type = netflix.groupby(['decade', 'type']).size().unstack(fill_value=0)\nsns.heatmap(decade_type, annot=True, fmt='d', ax=axes[1, 1], cmap='YlOrRd')  # funci\u00f3n para mapa de calor con anotaciones\naxes[1, 1].set_title('Heatmap: Lanzamientos por D\u00e9cada y Tipo')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"A\u00d1OS CON M\u00c1S LANZAMIENTOS:\")\nprint(yearly_releases.tail(10))\n</pre> # === AN\u00c1LISIS DE TENDENCIAS TEMPORALES ===  # 1. Preparar datos temporales netflix['release_year'] = pd.to_numeric(netflix['release_year'], errors='coerce')  # convertir a num\u00e9rico, NaN si no es posible yearly_releases = netflix['release_year'].value_counts().sort_index()  # contar frecuencias por a\u00f1o y ordenar  # Filtrar a\u00f1os recientes para mejor visualizaci\u00f3n recent_years = yearly_releases[yearly_releases.index &gt;= 2000]  # 2. Crear visualizaciones temporales m\u00faltiples fig, axes = plt.subplots(2, 2, figsize=(16, 12))  # Gr\u00e1fico 1: L\u00ednea temporal axes[0, 0].plot(recent_years.index, recent_years.values,  # funci\u00f3n para crear l\u00edneas conectando puntos                   marker='o', linewidth=2, markersize=4, color='darkblue') axes[0, 0].set_title('Cantidad de Contenido por A\u00f1o (2000-2021)') axes[0, 0].set_xlabel('A\u00f1o') axes[0, 0].set_ylabel('Cantidad de T\u00edtulos') axes[0, 0].grid(True, alpha=0.3)  # Gr\u00e1fico 2: \u00c1rea bajo la curva axes[0, 1].fill_between(recent_years.index, recent_years.values,  # funci\u00f3n para rellenar \u00e1rea bajo la l\u00ednea                   alpha=0.7, color='lightcoral') axes[0, 1].set_title('\u00c1rea - Lanzamientos por A\u00f1o') axes[0, 1].set_xlabel('A\u00f1o') axes[0, 1].set_ylabel('Cantidad')  # Gr\u00e1fico 3: An\u00e1lisis por tipo de contenido netflix_recent = netflix[netflix['release_year'] &gt;= 2010] yearly_by_type = netflix_recent.groupby(['release_year', 'type']).size().unstack(fill_value=0)  yearly_by_type.plot(kind='bar', ax=axes[1, 0],  # tipo de gr\u00e1fico con barras lado a lado (no apiladas)                     color=['skyblue', 'lightgreen'], alpha=0.8) axes[1, 0].set_title('Lanzamientos por Tipo (2010-2021)') axes[1, 0].set_xlabel('A\u00f1o') axes[1, 0].set_ylabel('Cantidad') axes[1, 0].legend(title='Tipo')  # Gr\u00e1fico 4: Heatmap de lanzamientos por d\u00e9cada y tipo netflix['decade'] = (netflix['release_year'] // 10) * 10 decade_type = netflix.groupby(['decade', 'type']).size().unstack(fill_value=0) sns.heatmap(decade_type, annot=True, fmt='d', ax=axes[1, 1], cmap='YlOrRd')  # funci\u00f3n para mapa de calor con anotaciones axes[1, 1].set_title('Heatmap: Lanzamientos por D\u00e9cada y Tipo')  plt.tight_layout() plt.show()  print(\"A\u00d1OS CON M\u00c1S LANZAMIENTOS:\") print(yearly_releases.tail(10)) <pre>A\u00d1OS CON M\u00c1S LANZAMIENTOS:\nrelease_year\n2011     136\n2012     183\n2013     237\n2014     288\n2015     517\n2016     830\n2017     959\n2018    1063\n2019     843\n2020      25\nName: count, dtype: int64\n</pre> In\u00a0[8]: Copied! <pre># === AN\u00c1LISIS DE PA\u00cdSES CON VISUALIZACIONES ===\n\n# 1. Preparar datos de pa\u00edses (limpiar y separar pa\u00edses m\u00faltiples)\nnetflix_countries = netflix.dropna(subset=['country']).copy()\n\n# Separar pa\u00edses que est\u00e1n separados por comas\ncountries_expanded = netflix_countries['country'].str.split(', ').explode()\ncountry_counts = countries_expanded.value_counts().head(20)  # contar frecuencias de pa\u00edses y tomar top 20\n\nprint(\"TOP 20 PA\u00cdSES CON M\u00c1S CONTENIDO:\")\nprint(country_counts)\n\n# 2. Crear visualizaciones geogr\u00e1ficas\nfig, axes = plt.subplots(2, 2, figsize=(18, 12))\n\n# Gr\u00e1fico 1: Top 15 pa\u00edses - barras horizontales\ntop_15_countries = country_counts.head(15)\nsns.barplot(y=top_15_countries.index, x=top_15_countries.values,  # funci\u00f3n para barras horizontales\n           ax=axes[0, 0], palette='viridis')\naxes[0, 0].set_title('Top 15 Pa\u00edses con M\u00e1s Contenido')\naxes[0, 0].set_xlabel('Cantidad de T\u00edtulos')\n\n# Gr\u00e1fico 2: Treemap simulado con scatter\ntop_10 = country_counts.head(10)\ncolors = plt.cm.Set3(np.linspace(0, 1, len(top_10)))\naxes[0, 1].scatter(range(len(top_10)), top_10.values,  # funci\u00f3n para gr\u00e1fico de burbujas/puntos\n                  s=top_10.values*3, c=colors, alpha=0.7)\nfor i, (country, count) in enumerate(top_10.items()):\n    axes[0, 1].annotate(f'{country}\\n({count})', \n                       (i, count), ha='center', va='center')\naxes[0, 1].set_title('Bubble Chart - Top 10 Pa\u00edses')\naxes[0, 1].set_xticks(range(len(top_10)))\naxes[0, 1].set_xticklabels(top_10.index, rotation=45)\n\n# Gr\u00e1fico 3: An\u00e1lisis de contenido por pa\u00eds y tipo\ntop_countries = country_counts.head(10).index\nnetflix_top_countries = netflix_countries[netflix_countries['country'].isin(top_countries)]\ncountry_type = netflix_top_countries.groupby(['country', 'type']).size().unstack(fill_value=0)\n\ncountry_type.plot(kind='bar', ax=axes[1, 0],  # tipo de gr\u00e1fico con barras agrupadas lado a lado\n                  color=['lightblue', 'salmon'], width=0.8)\naxes[1, 0].set_title('Movies vs TV Shows por Pa\u00eds (Top 10)')\naxes[1, 0].set_ylabel('Cantidad')\naxes[1, 0].legend(title='Tipo')\naxes[1, 0].tick_params(axis='x', rotation=45)\n\n# Gr\u00e1fico 4: Heatmap de correlaci\u00f3n entre pa\u00edses principales\n# Crear matriz de co-ocurrencia de pa\u00edses\nfrom itertools import combinations\nco_occurrence = {}\nfor countries_str in netflix_countries['country']:\n    if pd.notna(countries_str) and ',' in countries_str:\n        countries_list = [c.strip() for c in countries_str.split(',')]\n        for c1, c2 in combinations(countries_list, 2):\n            if c1 in top_10.index and c2 in top_10.index:\n                key = tuple(sorted([c1, c2]))\n                co_occurrence[key] = co_occurrence.get(key, 0) + 1\n\n# Crear matriz para heatmap\nco_matrix = np.zeros((len(top_10), len(top_10)))\nfor i, c1 in enumerate(top_10.index):\n    for j, c2 in enumerate(top_10.index):\n        if i != j:\n            key = tuple(sorted([c1, c2]))\n            co_matrix[i, j] = co_occurrence.get(key, 0)\n\nsns.heatmap(co_matrix, annot=True, fmt='.0f',  # funci\u00f3n para mapa de calor (formato .0f para n\u00fameros flotantes)\n           xticklabels=top_10.index, yticklabels=top_10.index,\n           ax=axes[1, 1], cmap='Reds')\naxes[1, 1].set_title('Co-producci\u00f3n entre Pa\u00edses')\naxes[1, 1].tick_params(axis='x', rotation=45)\naxes[1, 1].tick_params(axis='y', rotation=0)\n\nplt.tight_layout()\nplt.show()\n</pre> # === AN\u00c1LISIS DE PA\u00cdSES CON VISUALIZACIONES ===  # 1. Preparar datos de pa\u00edses (limpiar y separar pa\u00edses m\u00faltiples) netflix_countries = netflix.dropna(subset=['country']).copy()  # Separar pa\u00edses que est\u00e1n separados por comas countries_expanded = netflix_countries['country'].str.split(', ').explode() country_counts = countries_expanded.value_counts().head(20)  # contar frecuencias de pa\u00edses y tomar top 20  print(\"TOP 20 PA\u00cdSES CON M\u00c1S CONTENIDO:\") print(country_counts)  # 2. Crear visualizaciones geogr\u00e1ficas fig, axes = plt.subplots(2, 2, figsize=(18, 12))  # Gr\u00e1fico 1: Top 15 pa\u00edses - barras horizontales top_15_countries = country_counts.head(15) sns.barplot(y=top_15_countries.index, x=top_15_countries.values,  # funci\u00f3n para barras horizontales            ax=axes[0, 0], palette='viridis') axes[0, 0].set_title('Top 15 Pa\u00edses con M\u00e1s Contenido') axes[0, 0].set_xlabel('Cantidad de T\u00edtulos')  # Gr\u00e1fico 2: Treemap simulado con scatter top_10 = country_counts.head(10) colors = plt.cm.Set3(np.linspace(0, 1, len(top_10))) axes[0, 1].scatter(range(len(top_10)), top_10.values,  # funci\u00f3n para gr\u00e1fico de burbujas/puntos                   s=top_10.values*3, c=colors, alpha=0.7) for i, (country, count) in enumerate(top_10.items()):     axes[0, 1].annotate(f'{country}\\n({count})',                         (i, count), ha='center', va='center') axes[0, 1].set_title('Bubble Chart - Top 10 Pa\u00edses') axes[0, 1].set_xticks(range(len(top_10))) axes[0, 1].set_xticklabels(top_10.index, rotation=45)  # Gr\u00e1fico 3: An\u00e1lisis de contenido por pa\u00eds y tipo top_countries = country_counts.head(10).index netflix_top_countries = netflix_countries[netflix_countries['country'].isin(top_countries)] country_type = netflix_top_countries.groupby(['country', 'type']).size().unstack(fill_value=0)  country_type.plot(kind='bar', ax=axes[1, 0],  # tipo de gr\u00e1fico con barras agrupadas lado a lado                   color=['lightblue', 'salmon'], width=0.8) axes[1, 0].set_title('Movies vs TV Shows por Pa\u00eds (Top 10)') axes[1, 0].set_ylabel('Cantidad') axes[1, 0].legend(title='Tipo') axes[1, 0].tick_params(axis='x', rotation=45)  # Gr\u00e1fico 4: Heatmap de correlaci\u00f3n entre pa\u00edses principales # Crear matriz de co-ocurrencia de pa\u00edses from itertools import combinations co_occurrence = {} for countries_str in netflix_countries['country']:     if pd.notna(countries_str) and ',' in countries_str:         countries_list = [c.strip() for c in countries_str.split(',')]         for c1, c2 in combinations(countries_list, 2):             if c1 in top_10.index and c2 in top_10.index:                 key = tuple(sorted([c1, c2]))                 co_occurrence[key] = co_occurrence.get(key, 0) + 1  # Crear matriz para heatmap co_matrix = np.zeros((len(top_10), len(top_10))) for i, c1 in enumerate(top_10.index):     for j, c2 in enumerate(top_10.index):         if i != j:             key = tuple(sorted([c1, c2]))             co_matrix[i, j] = co_occurrence.get(key, 0)  sns.heatmap(co_matrix, annot=True, fmt='.0f',  # funci\u00f3n para mapa de calor (formato .0f para n\u00fameros flotantes)            xticklabels=top_10.index, yticklabels=top_10.index,            ax=axes[1, 1], cmap='Reds') axes[1, 1].set_title('Co-producci\u00f3n entre Pa\u00edses') axes[1, 1].tick_params(axis='x', rotation=45) axes[1, 1].tick_params(axis='y', rotation=0)  plt.tight_layout() plt.show() <pre>TOP 20 PA\u00cdSES CON M\u00c1S CONTENIDO:\ncountry\nUnited States     2609\nIndia              838\nUnited Kingdom     601\nCanada             318\nFrance             271\nJapan              231\nSpain              178\nSouth Korea        162\nGermany            151\nMexico             129\nAustralia          126\nChina              120\nHong Kong           97\nTurkey              87\nTaiwan              75\nArgentina           68\nItaly               67\nBelgium             66\nBrazil              66\nThailand            56\nName: count, dtype: int64\n</pre> <pre>/var/folders/j9/vw8n5h8j7lzbcpd4gp5z5r280000gn/T/ipykernel_26073/3447908643.py:18: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(y=top_15_countries.index, x=top_15_countries.values,  # funci\u00f3n para barras horizontales\n</pre> In\u00a0[9]: Copied! <pre># === AN\u00c1LISIS DE RATINGS Y G\u00c9NEROS ===\n\n# 1. Preparar datos de ratings\nrating_counts = netflix['rating'].value_counts().head(10)  # contar frecuencias de ratings y tomar top 10\n\nprint(\"TOP 10 RATINGS M\u00c1S COMUNES:\")\nprint(rating_counts)\n\n# 2. Crear dashboard de ratings\nfig, axes = plt.subplots(2, 3, figsize=(20, 12))\n\n# Gr\u00e1fico 1: Countplot de ratings\nsns.countplot(data=netflix, x='rating', order=rating_counts.index,  # funci\u00f3n para contar y graficar categor\u00edas ordenadas\n           ax=axes[0, 0], palette='Set1')\naxes[0, 0].set_title('Distribuci\u00f3n de Ratings')\naxes[0, 0].tick_params(axis='x', rotation=45)\n\n# Gr\u00e1fico 2: Ratings por tipo de contenido\nsns.countplot(data=netflix, x='rating', hue='type',  # misma funci\u00f3n pero separando por otra variable (hue)\n           order=rating_counts.index, ax=axes[0, 1])\naxes[0, 1].set_title('Ratings por Tipo de Contenido')\naxes[0, 1].tick_params(axis='x', rotation=45)\naxes[0, 1].legend(title='Tipo')\n\n# Gr\u00e1fico 3: Pie chart de ratings principales\ntop_5_ratings = rating_counts.head(5)\naxes[0, 2].pie(top_5_ratings.values, labels=top_5_ratings.index,  # funci\u00f3n para gr\u00e1fico circular/torta\n                  autopct='%1.1f%%', startangle=90)\naxes[0, 2].set_title('Top 5 Ratings - Proporci\u00f3n')\n\n# Gr\u00e1fico 4: Box plot de a\u00f1os de lanzamiento por rating\nnetflix_clean = netflix.dropna(subset=['rating', 'release_year'])\ntop_ratings = rating_counts.head(6).index\nnetflix_top_ratings = netflix_clean[netflix_clean['rating'].isin(top_ratings)]\n\nsns.boxplot(data=netflix_top_ratings, x='rating', y='release_year',  # funci\u00f3n para mostrar distribuci\u00f3n con cajas y bigotes\n           order=top_ratings, ax=axes[1, 0])\naxes[1, 0].set_title('Distribuci\u00f3n de A\u00f1os por Rating')\naxes[1, 0].tick_params(axis='x', rotation=45)\n\n# Gr\u00e1fico 5: Violin plot alternativo\nsns.violinplot(data=netflix_top_ratings, x='rating', y='release_year',  # funci\u00f3n para mostrar densidad como \"violines\"\n           order=top_ratings, ax=axes[1, 1], palette='muted')\naxes[1, 1].set_title('Densidad de A\u00f1os por Rating')\naxes[1, 1].tick_params(axis='x', rotation=45)\n\n# Gr\u00e1fico 6: Heatmap de rating vs d\u00e9cada\nnetflix_clean['decade'] = (netflix_clean['release_year'] // 10) * 10\nrating_decade = netflix_clean.groupby(['rating', 'decade']).size().unstack(fill_value=0)\n# Filtrar solo ratings principales y d\u00e9cadas recientes\nrating_decade_filtered = rating_decade.loc[top_ratings, rating_decade.columns &gt;= 1980]\n\nsns.heatmap(rating_decade_filtered, annot=True, fmt='d',  # funci\u00f3n para mapa de calor con valores enteros\n           ax=axes[1, 2], cmap='Blues')\naxes[1, 2].set_title('Heatmap: Rating vs D\u00e9cada')\naxes[1, 2].tick_params(axis='x', rotation=45)\n\nplt.tight_layout()\nplt.show()\n</pre> # === AN\u00c1LISIS DE RATINGS Y G\u00c9NEROS ===  # 1. Preparar datos de ratings rating_counts = netflix['rating'].value_counts().head(10)  # contar frecuencias de ratings y tomar top 10  print(\"TOP 10 RATINGS M\u00c1S COMUNES:\") print(rating_counts)  # 2. Crear dashboard de ratings fig, axes = plt.subplots(2, 3, figsize=(20, 12))  # Gr\u00e1fico 1: Countplot de ratings sns.countplot(data=netflix, x='rating', order=rating_counts.index,  # funci\u00f3n para contar y graficar categor\u00edas ordenadas            ax=axes[0, 0], palette='Set1') axes[0, 0].set_title('Distribuci\u00f3n de Ratings') axes[0, 0].tick_params(axis='x', rotation=45)  # Gr\u00e1fico 2: Ratings por tipo de contenido sns.countplot(data=netflix, x='rating', hue='type',  # misma funci\u00f3n pero separando por otra variable (hue)            order=rating_counts.index, ax=axes[0, 1]) axes[0, 1].set_title('Ratings por Tipo de Contenido') axes[0, 1].tick_params(axis='x', rotation=45) axes[0, 1].legend(title='Tipo')  # Gr\u00e1fico 3: Pie chart de ratings principales top_5_ratings = rating_counts.head(5) axes[0, 2].pie(top_5_ratings.values, labels=top_5_ratings.index,  # funci\u00f3n para gr\u00e1fico circular/torta                   autopct='%1.1f%%', startangle=90) axes[0, 2].set_title('Top 5 Ratings - Proporci\u00f3n')  # Gr\u00e1fico 4: Box plot de a\u00f1os de lanzamiento por rating netflix_clean = netflix.dropna(subset=['rating', 'release_year']) top_ratings = rating_counts.head(6).index netflix_top_ratings = netflix_clean[netflix_clean['rating'].isin(top_ratings)]  sns.boxplot(data=netflix_top_ratings, x='rating', y='release_year',  # funci\u00f3n para mostrar distribuci\u00f3n con cajas y bigotes            order=top_ratings, ax=axes[1, 0]) axes[1, 0].set_title('Distribuci\u00f3n de A\u00f1os por Rating') axes[1, 0].tick_params(axis='x', rotation=45)  # Gr\u00e1fico 5: Violin plot alternativo sns.violinplot(data=netflix_top_ratings, x='rating', y='release_year',  # funci\u00f3n para mostrar densidad como \"violines\"            order=top_ratings, ax=axes[1, 1], palette='muted') axes[1, 1].set_title('Densidad de A\u00f1os por Rating') axes[1, 1].tick_params(axis='x', rotation=45)  # Gr\u00e1fico 6: Heatmap de rating vs d\u00e9cada netflix_clean['decade'] = (netflix_clean['release_year'] // 10) * 10 rating_decade = netflix_clean.groupby(['rating', 'decade']).size().unstack(fill_value=0) # Filtrar solo ratings principales y d\u00e9cadas recientes rating_decade_filtered = rating_decade.loc[top_ratings, rating_decade.columns &gt;= 1980]  sns.heatmap(rating_decade_filtered, annot=True, fmt='d',  # funci\u00f3n para mapa de calor con valores enteros            ax=axes[1, 2], cmap='Blues') axes[1, 2].set_title('Heatmap: Rating vs D\u00e9cada') axes[1, 2].tick_params(axis='x', rotation=45)  plt.tight_layout() plt.show() <pre>TOP 10 RATINGS M\u00c1S COMUNES:\nrating\nTV-MA    2027\nTV-14    1698\nTV-PG     701\nR         508\nPG-13     286\nNR        218\nPG        184\nTV-Y7     169\nTV-G      149\nTV-Y      143\nName: count, dtype: int64\n</pre> <pre>/var/folders/j9/vw8n5h8j7lzbcpd4gp5z5r280000gn/T/ipykernel_26073/3465889314.py:13: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.countplot(data=netflix, x='rating', order=rating_counts.index,  # funci\u00f3n para contar y graficar categor\u00edas ordenadas\n/var/folders/j9/vw8n5h8j7lzbcpd4gp5z5r280000gn/T/ipykernel_26073/3465889314.py:42: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.violinplot(data=netflix_top_ratings, x='rating', y='release_year',  # funci\u00f3n para mostrar densidad como \"violines\"\n/var/folders/j9/vw8n5h8j7lzbcpd4gp5z5r280000gn/T/ipykernel_26073/3465889314.py:48: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  netflix_clean['decade'] = (netflix_clean['release_year'] // 10) * 10\n</pre> In\u00a0[11]: Copied! <pre># === CREAR DASHBOARD FINAL INTERACTIVO ===\n\n# 1. Calcular estad\u00edsticas finales\ntotal_titles = len(netflix)\ntotal_movies = len(netflix[netflix['type'] == 'Movie'])\ntotal_shows = len(netflix[netflix['type'] == 'TV Show'])\nlatest_year = netflix['release_year'].max()\noldest_year = netflix['release_year'].min()\n\nprint(f\"RESUMEN EJECUTIVO NETFLIX:\")\nprint(f\"   Total de t\u00edtulos: {total_titles:,}\")\nprint(f\"   Pel\u00edculas: {total_movies:,} ({total_movies/total_titles*100:.1f}%)\")\nprint(f\"   Series: {total_shows:,} ({total_shows/total_titles*100:.1f}%)\")\nprint(f\"   Rango de a\u00f1os: {oldest_year} - {latest_year}\")\n\n# 2. Crear figura principal con subplots\nfig = plt.figure(figsize=(20, 15))\ngs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n\n# Dashboard panel 1: Tipos (grande)\nax1 = fig.add_subplot(gs[0, :2])\ntype_data = netflix['type'].value_counts()\ncolors = ['#FF6B6B', '#4ECDC4']\nwedges, texts, autotexts = ax1.pie(type_data.values, labels=type_data.index,  # funci\u00f3n para gr\u00e1fico de torta en dashboard\n                                       autopct='%1.1f%%', startangle=90,\n                                       colors=colors, textprops={'fontsize': 12})\nax1.set_title('Distribuci\u00f3n Movies vs TV Shows', fontsize=14, fontweight='bold')\n\n# Dashboard panel 2: Timeline\nax2 = fig.add_subplot(gs[0, 2:])\nyearly = netflix.groupby('release_year').size()\nrecent_years = yearly[yearly.index &gt;= 2000]\nax2.fill_between(recent_years.index, recent_years.values, color='#FF6B6B', alpha=0.7)  # funci\u00f3n para rellenar \u00e1rea bajo curva\nax2.plot(recent_years.index, recent_years.values, color='darkred', linewidth=2)\nax2.set_title('Evoluci\u00f3n Temporal (2000+)', fontsize=14, fontweight='bold')\nax2.set_xlabel('A\u00f1o')\nax2.set_ylabel('T\u00edtulos Lanzados')\nax2.grid(True, alpha=0.3)\n\n# Dashboard panel 3: Top pa\u00edses\nax3 = fig.add_subplot(gs[1, :2])\ncountry_clean = netflix.dropna(subset=['country'])['country'].str.split(', ').explode()\ntop_countries = country_clean.value_counts().head(10)\nsns.barplot(y=top_countries.index, x=top_countries.values, ax=ax3, palette='viridis')  # funci\u00f3n para barras horizontales\nax3.set_title('Top 10 Pa\u00edses Productores', fontsize=14, fontweight='bold')\nax3.set_xlabel('Cantidad de T\u00edtulos')\n\n# Dashboard panel 4: Ratings\nax4 = fig.add_subplot(gs[1, 2:])\ntop_ratings = netflix['rating'].value_counts().head(8)\nsns.countplot(data=netflix[netflix['rating'].isin(top_ratings.index)],  # funci\u00f3n para contar categor\u00edas separadas por hue\n           x='rating', hue='type', order=top_ratings.index, ax=ax4)\nax4.set_title('Ratings por Tipo de Contenido', fontsize=14, fontweight='bold')\nax4.tick_params(axis='x', rotation=45)\nax4.legend(title='Tipo')\n\n# Dashboard panel 5: Heatmap temporal (full width)\nax5 = fig.add_subplot(gs[2, :])\nnetflix['decade'] = (netflix['release_year'] // 10) * 10\nyear_type_decade = netflix.groupby(['decade', 'type']).size().unstack(fill_value=0)\nsns.heatmap(year_type_decade.T, annot=True, fmt='d', ax=ax5,  # funci\u00f3n para mapa de calor transpuesto (.T)\n           cmap='YlOrRd', cbar_kws={'label': 'Cantidad de T\u00edtulos'})\nax5.set_title('Evoluci\u00f3n por D\u00e9cadas y Tipo de Contenido', fontsize=14, fontweight='bold')\nax5.set_xlabel('D\u00e9cada')\nax5.set_ylabel('Tipo de Contenido')\n\n# Guardar el dashboard\nplt.suptitle('NETFLIX CONTENT ANALYSIS DASHBOARD', fontsize=18, fontweight='bold', y=0.95)\nplt.savefig(f'netflix_dashboard.png', dpi=300, bbox_inches='tight')  # funci\u00f3n para guardar figura como archivo\nplt.show()\n\nprint(\"\\n Dashboard guardado como 'netflix_dashboard.png'\")\n</pre> # === CREAR DASHBOARD FINAL INTERACTIVO ===  # 1. Calcular estad\u00edsticas finales total_titles = len(netflix) total_movies = len(netflix[netflix['type'] == 'Movie']) total_shows = len(netflix[netflix['type'] == 'TV Show']) latest_year = netflix['release_year'].max() oldest_year = netflix['release_year'].min()  print(f\"RESUMEN EJECUTIVO NETFLIX:\") print(f\"   Total de t\u00edtulos: {total_titles:,}\") print(f\"   Pel\u00edculas: {total_movies:,} ({total_movies/total_titles*100:.1f}%)\") print(f\"   Series: {total_shows:,} ({total_shows/total_titles*100:.1f}%)\") print(f\"   Rango de a\u00f1os: {oldest_year} - {latest_year}\")  # 2. Crear figura principal con subplots fig = plt.figure(figsize=(20, 15)) gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)  # Dashboard panel 1: Tipos (grande) ax1 = fig.add_subplot(gs[0, :2]) type_data = netflix['type'].value_counts() colors = ['#FF6B6B', '#4ECDC4'] wedges, texts, autotexts = ax1.pie(type_data.values, labels=type_data.index,  # funci\u00f3n para gr\u00e1fico de torta en dashboard                                        autopct='%1.1f%%', startangle=90,                                        colors=colors, textprops={'fontsize': 12}) ax1.set_title('Distribuci\u00f3n Movies vs TV Shows', fontsize=14, fontweight='bold')  # Dashboard panel 2: Timeline ax2 = fig.add_subplot(gs[0, 2:]) yearly = netflix.groupby('release_year').size() recent_years = yearly[yearly.index &gt;= 2000] ax2.fill_between(recent_years.index, recent_years.values, color='#FF6B6B', alpha=0.7)  # funci\u00f3n para rellenar \u00e1rea bajo curva ax2.plot(recent_years.index, recent_years.values, color='darkred', linewidth=2) ax2.set_title('Evoluci\u00f3n Temporal (2000+)', fontsize=14, fontweight='bold') ax2.set_xlabel('A\u00f1o') ax2.set_ylabel('T\u00edtulos Lanzados') ax2.grid(True, alpha=0.3)  # Dashboard panel 3: Top pa\u00edses ax3 = fig.add_subplot(gs[1, :2]) country_clean = netflix.dropna(subset=['country'])['country'].str.split(', ').explode() top_countries = country_clean.value_counts().head(10) sns.barplot(y=top_countries.index, x=top_countries.values, ax=ax3, palette='viridis')  # funci\u00f3n para barras horizontales ax3.set_title('Top 10 Pa\u00edses Productores', fontsize=14, fontweight='bold') ax3.set_xlabel('Cantidad de T\u00edtulos')  # Dashboard panel 4: Ratings ax4 = fig.add_subplot(gs[1, 2:]) top_ratings = netflix['rating'].value_counts().head(8) sns.countplot(data=netflix[netflix['rating'].isin(top_ratings.index)],  # funci\u00f3n para contar categor\u00edas separadas por hue            x='rating', hue='type', order=top_ratings.index, ax=ax4) ax4.set_title('Ratings por Tipo de Contenido', fontsize=14, fontweight='bold') ax4.tick_params(axis='x', rotation=45) ax4.legend(title='Tipo')  # Dashboard panel 5: Heatmap temporal (full width) ax5 = fig.add_subplot(gs[2, :]) netflix['decade'] = (netflix['release_year'] // 10) * 10 year_type_decade = netflix.groupby(['decade', 'type']).size().unstack(fill_value=0) sns.heatmap(year_type_decade.T, annot=True, fmt='d', ax=ax5,  # funci\u00f3n para mapa de calor transpuesto (.T)            cmap='YlOrRd', cbar_kws={'label': 'Cantidad de T\u00edtulos'}) ax5.set_title('Evoluci\u00f3n por D\u00e9cadas y Tipo de Contenido', fontsize=14, fontweight='bold') ax5.set_xlabel('D\u00e9cada') ax5.set_ylabel('Tipo de Contenido')  # Guardar el dashboard plt.suptitle('NETFLIX CONTENT ANALYSIS DASHBOARD', fontsize=18, fontweight='bold', y=0.95) plt.savefig(f'netflix_dashboard.png', dpi=300, bbox_inches='tight')  # funci\u00f3n para guardar figura como archivo plt.show()  print(\"\\n Dashboard guardado como 'netflix_dashboard.png'\") <pre>RESUMEN EJECUTIVO NETFLIX:\n   Total de t\u00edtulos: 6,234\n   Pel\u00edculas: 4,265 (68.4%)\n   Series: 1,969 (31.6%)\n   Rango de a\u00f1os: 1925 - 2020\n</pre> <pre>/var/folders/j9/vw8n5h8j7lzbcpd4gp5z5r280000gn/T/ipykernel_26073/251449361.py:44: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(y=top_countries.index, x=top_countries.values, ax=ax3, palette='viridis')  # funci\u00f3n para barras horizontales\n</pre> <pre>\n Dashboard guardado como 'netflix_dashboard.png'\n</pre> In\u00a0[\u00a0]: Copied! <pre># === AN\u00c1LISIS AVANZADO DE G\u00c9NEROS ===\n\n# 1. Separar g\u00e9neros que est\u00e1n en lista separada por comas\ngenres_expanded = netflix.dropna(subset=['listed_in'])['listed_in'].str.split(', ').explode()\ntop_genres = genres_expanded.value_counts().head(15)  # contar frecuencias de g\u00e9neros y tomar top 15\n\nprint(\"TOP 15 G\u00c9NEROS M\u00c1S POPULARES:\")\nprint(top_genres)\n\n# 2. Crear visualizaci\u00f3n de g\u00e9neros\nfig, axes = plt.subplots(2, 2, figsize=(16, 10))\n\n# Word cloud simulado con scatter\naxes[0, 0].scatter(range(len(top_genres)), top_genres.values,  # funci\u00f3n para gr\u00e1fico de burbujas (scatter)\n                  s=top_genres.values*2, alpha=0.6, c=range(len(top_genres)), cmap='viridis')\nfor i, (genre, count) in enumerate(top_genres.items()):\n    axes[0, 0].annotate(genre, (i, count), ha='center', va='center', fontsize=8)\naxes[0, 0].set_title('Bubble Chart - G\u00e9neros Populares')\naxes[0, 0].set_xticks([])\n\n# Barras horizontales de g\u00e9neros\nsns.barplot(y=top_genres.head(10).index, x=top_genres.head(10).values,  # funci\u00f3n para barras horizontales\n           ax=axes[0, 1], palette='Set2')\naxes[0, 1].set_title('Top 10 G\u00e9neros')\naxes[0, 1].set_xlabel('Cantidad')\n\n# An\u00e1lisis de duraci\u00f3n de pel\u00edculas\nmovies_netflix = netflix[netflix['type'] == 'Movie'].copy()\nmovies_netflix['duration_min'] = movies_netflix['duration'].str.extract('(\\d+)').astype(float)\n\naxes[1, 0].hist(movies_netflix['duration_min'], bins=30, alpha=0.7, color='lightcoral', edgecolor='black')  # funci\u00f3n para histograma de frecuencias\naxes[1, 0].set_title('Distribuci\u00f3n Duraci\u00f3n Pel\u00edculas')\naxes[1, 0].set_xlabel('Duraci\u00f3n (minutos)')\naxes[1, 0].set_ylabel('Frecuencia')\naxes[1, 0].axvline(movies_netflix['duration_min'].mean(), color='red', linestyle='--', \n                   label=f'Media: {movies_netflix[\"duration_min\"].mean():.0f} min')\naxes[1, 0].legend()\n\n# An\u00e1lisis de temporadas de series\ntv_shows_netflix = netflix[netflix['type'] == 'TV Show'].copy()\ntv_shows_netflix['seasons'] = tv_shows_netflix['duration'].str.extract('(\\d+)').astype(float)\n\naxes[1, 1].hist(tv_shows_netflix['seasons'], bins=range(1, 20), alpha=0.7, color='lightblue', edgecolor='black')  # funci\u00f3n para histograma con bins personalizados\naxes[1, 1].set_title('Distribuci\u00f3n Temporadas TV Shows')\naxes[1, 1].set_xlabel('N\u00famero de Temporadas')\naxes[1, 1].set_ylabel('Frecuencia')\naxes[1, 1].axvline(tv_shows_netflix['seasons'].mean(), color='blue', linestyle='--',\n                   label=f'Media: {tv_shows_netflix[\"seasons\"].mean():.1f} temporadas')\naxes[1, 1].legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nESTAD\u00cdSTICAS DE DURACI\u00d3N:\")\nprint(f\"   Pel\u00edcula promedio: {movies_netflix['duration_min'].mean():.0f} minutos\")\nprint(f\"   Pel\u00edcula m\u00e1s corta: {movies_netflix['duration_min'].min():.0f} minutos\")\nprint(f\"   Pel\u00edcula m\u00e1s larga: {movies_netflix['duration_min'].max():.0f} minutos\")\nprint(f\"   Serie promedio: {tv_shows_netflix['seasons'].mean():.1f} temporadas\")\nprint(f\"   Serie m\u00e1s larga: {tv_shows_netflix['seasons'].max():.0f} temporadas\")\n</pre> # === AN\u00c1LISIS AVANZADO DE G\u00c9NEROS ===  # 1. Separar g\u00e9neros que est\u00e1n en lista separada por comas genres_expanded = netflix.dropna(subset=['listed_in'])['listed_in'].str.split(', ').explode() top_genres = genres_expanded.value_counts().head(15)  # contar frecuencias de g\u00e9neros y tomar top 15  print(\"TOP 15 G\u00c9NEROS M\u00c1S POPULARES:\") print(top_genres)  # 2. Crear visualizaci\u00f3n de g\u00e9neros fig, axes = plt.subplots(2, 2, figsize=(16, 10))  # Word cloud simulado con scatter axes[0, 0].scatter(range(len(top_genres)), top_genres.values,  # funci\u00f3n para gr\u00e1fico de burbujas (scatter)                   s=top_genres.values*2, alpha=0.6, c=range(len(top_genres)), cmap='viridis') for i, (genre, count) in enumerate(top_genres.items()):     axes[0, 0].annotate(genre, (i, count), ha='center', va='center', fontsize=8) axes[0, 0].set_title('Bubble Chart - G\u00e9neros Populares') axes[0, 0].set_xticks([])  # Barras horizontales de g\u00e9neros sns.barplot(y=top_genres.head(10).index, x=top_genres.head(10).values,  # funci\u00f3n para barras horizontales            ax=axes[0, 1], palette='Set2') axes[0, 1].set_title('Top 10 G\u00e9neros') axes[0, 1].set_xlabel('Cantidad')  # An\u00e1lisis de duraci\u00f3n de pel\u00edculas movies_netflix = netflix[netflix['type'] == 'Movie'].copy() movies_netflix['duration_min'] = movies_netflix['duration'].str.extract('(\\d+)').astype(float)  axes[1, 0].hist(movies_netflix['duration_min'], bins=30, alpha=0.7, color='lightcoral', edgecolor='black')  # funci\u00f3n para histograma de frecuencias axes[1, 0].set_title('Distribuci\u00f3n Duraci\u00f3n Pel\u00edculas') axes[1, 0].set_xlabel('Duraci\u00f3n (minutos)') axes[1, 0].set_ylabel('Frecuencia') axes[1, 0].axvline(movies_netflix['duration_min'].mean(), color='red', linestyle='--',                     label=f'Media: {movies_netflix[\"duration_min\"].mean():.0f} min') axes[1, 0].legend()  # An\u00e1lisis de temporadas de series tv_shows_netflix = netflix[netflix['type'] == 'TV Show'].copy() tv_shows_netflix['seasons'] = tv_shows_netflix['duration'].str.extract('(\\d+)').astype(float)  axes[1, 1].hist(tv_shows_netflix['seasons'], bins=range(1, 20), alpha=0.7, color='lightblue', edgecolor='black')  # funci\u00f3n para histograma con bins personalizados axes[1, 1].set_title('Distribuci\u00f3n Temporadas TV Shows') axes[1, 1].set_xlabel('N\u00famero de Temporadas') axes[1, 1].set_ylabel('Frecuencia') axes[1, 1].axvline(tv_shows_netflix['seasons'].mean(), color='blue', linestyle='--',                    label=f'Media: {tv_shows_netflix[\"seasons\"].mean():.1f} temporadas') axes[1, 1].legend()  plt.tight_layout() plt.show()  print(f\"\\nESTAD\u00cdSTICAS DE DURACI\u00d3N:\") print(f\"   Pel\u00edcula promedio: {movies_netflix['duration_min'].mean():.0f} minutos\") print(f\"   Pel\u00edcula m\u00e1s corta: {movies_netflix['duration_min'].min():.0f} minutos\") print(f\"   Pel\u00edcula m\u00e1s larga: {movies_netflix['duration_min'].max():.0f} minutos\") print(f\"   Serie promedio: {tv_shows_netflix['seasons'].mean():.1f} temporadas\") print(f\"   Serie m\u00e1s larga: {tv_shows_netflix['seasons'].max():.0f} temporadas\")"},{"location":"portfolio/entregas/uno/#preguntas-a-responder","title":"PREGUNTAS A RESPONDER\u00b6","text":"<ol> <li><p>\u00bfQu\u00e9 tipo de visualizaci\u00f3n es m\u00e1s efectiva para mostrar distribuciones temporales? \ud83d\udca1 PISTA: Compara line plot vs area plot vs bar plot</p> </li> <li><p>\u00bfPor qu\u00e9 usamos diferentes tipos de gr\u00e1ficos para diferentes datos? \ud83d\udca1 PISTA: Gu\u00eda de tipos de gr\u00e1ficos</p> </li> <li><p>\u00bfQu\u00e9 insights de negocio obtuviste que Netflix podr\u00eda usar? \ud83d\udca1 PISTA: Piensa en estrategias de contenido, mercados objetivo, tipos de producci\u00f3n</p> </li> <li><p>\u00bfCu\u00e1l fue la visualizaci\u00f3n m\u00e1s reveladora y por qu\u00e9? \ud83d\udca1 PISTA: \u00bfQu\u00e9 patr\u00f3n no esperabas ver?</p> </li> <li><p>\u00bfC\u00f3mo mejorar\u00edas este an\u00e1lisis con m\u00e1s datos? \ud83d\udca1 PISTA: Datos de audiencia, ratings de IMDb, presupuestos, etc.</p> </li> </ol>"},{"location":"portfolio/entregas/uno/#respuestas","title":"RESPUESTAS\u00b6","text":"<ol> <li><p>El m\u00e1s claro es el de area plot ya que muestra de forma m\u00e1s directa y efectiva la variacion de los datos en el tiempo.  En segundo lugar elegir\u00eda el line plot, porque sirve para ver tendencias pero sin el efecto de \u00e1rea y el bar plot es el \u00faltimo porque no se nota la continuidad de los datos durante el tiempo y no comunica de forma directa como los otros dos.</p> </li> <li><p>Depende de lo que desees visualizar, cual tipo de grafico te sirve m\u00e1s. En otras palabras, hay algunos graficos que se ajustan m\u00e1s a cierto tipo de comunicaci\u00f3n de an\u00e1lisis y es preferible elegirlos para graficar porque resalta mejor ciertos aspectos de los datos. Un ejemplo ser\u00eda usar area plot para transmitir acumulaci\u00f3n o bar plot para comparaciones de categorias. Otro ejemplo ser\u00eda el uso de las visualizaciones tipo heatmap para identificar patrones o \u00e1reas de inter\u00e9s; as\u00ed como el uso de la psicolog\u00eda del color, donde puede cambiar la perspectiva de quien observa la visualizaci\u00f3n dependiendo de la escala de colores elegida para el heatmap.</p> </li> <li><p>Viendo los puntos donde hay un desequilibrio pens\u00e9 en opciones de mejora que lograr\u00edan revertir esa diferencia tan marcada. Algunos de los que pense fueron: Estados Unidos es con diferencia el pa\u00eds con mayor producci\u00f3n y contenido, por lo que se podr\u00eda invertir para hacer producciones en otros paises y as\u00ed abarcar culturas variadas, lo cual va a tener un engagement mayor. Las peliculas superan a los tv show en casi todas las variables, lo cual no es un problema pero esta diferencia podr\u00eda minimizarse. Quiz\u00e1 hay menos interes en series porque como muestran las graficas se producen muchas m\u00e1s peliculas comparativamente y la oferta se series no es tan variada.</p> </li> <li><p>La m\u00e1s reveladora es como se ditancia Estados Unidos del resto de los pa\u00edses en cuanto a producci\u00f3n, ya que esto determina que un porcentaje muy alto de contenido en ingles y relacionado a cultura del hemisferio norte. Tambi\u00e9n me result\u00f3 curiosa la de los t\u00edtulos duplicados.</p> </li> <li><p>Hace falta una actualizaci\u00f3n de datos, ya que se registra hasta 2020 que fue el a\u00f1o de la pandemia y esto no es beneficiario par ala aplicaci\u00f3n, debido a que la producci\u00f3n en este a\u00f1o se fren\u00f3. Tambi\u00e9n agregar\u00eda m\u00e1s visualizaciones involucrando m\u00e1s variables. Por ejemplo: n\u00famero de reproducciones, horas vistas de cada una, peliculas o series m\u00e1s vistas y en que paises e informaci\u00f3n de competencia para comparar y detectar oportunidades de mejora y posicionamiento en el mercado.</p> </li> </ol>"}]}